{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#ML-DL-Notes\" data-toc-modified-id=\"ML-DL-Notes-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>ML DL Notes</a></span></li><li><span><a href=\"#-Topics-to-learn-\" data-toc-modified-id=\"-Topics-to-learn--2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span><font color=\"red\"> Topics to learn </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#Learn-Now-(most-important)\" data-toc-modified-id=\"Learn-Now-(most-important)-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Learn Now (most important)</a></span></li><li><span><a href=\"#Learn-later\" data-toc-modified-id=\"Learn-later-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Learn later</a></span></li></ul></li><li><span><a href=\"#-Probability-and-Statistics-\" data-toc-modified-id=\"-Probability-and-Statistics--3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span><font color=\"brown\"> Probability and Statistics </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#Types-of-Probability\" data-toc-modified-id=\"Types-of-Probability-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Types of Probability</a></span></li><li><span><a href=\"#Central-Limit-Theorem-(CLT)\" data-toc-modified-id=\"Central-Limit-Theorem-(CLT)-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Central Limit Theorem (CLT)</a></span></li><li><span><a href=\"#Bernoulli-Distribution\" data-toc-modified-id=\"Bernoulli-Distribution-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Bernoulli Distribution</a></span></li><li><span><a href=\"#Binomial-Theorem\" data-toc-modified-id=\"Binomial-Theorem-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Binomial Theorem</a></span></li><li><span><a href=\"#Standard-Normal-Distribution-or-Z-distribution\" data-toc-modified-id=\"Standard-Normal-Distribution-or-Z-distribution-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Standard Normal Distribution or Z-distribution</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#-$-std(x)-=-\\frac{x---\\mu}{\\sigma}-$-\" data-toc-modified-id=\"-$-std(x)-=-\\frac{x---\\mu}{\\sigma}-$--3.5.0.1\"><span class=\"toc-item-num\">3.5.0.1&nbsp;&nbsp;</span> $ std(x) = \\frac{x - \\mu}{\\sigma} $ </a></span></li></ul></li></ul></li><li><span><a href=\"#Maximum-Likelihood-Estimation-(MLE)\" data-toc-modified-id=\"Maximum-Likelihood-Estimation-(MLE)-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>Maximum Likelihood Estimation (MLE)</a></span></li><li><span><a href=\"#Bayes-Theorem\" data-toc-modified-id=\"Bayes-Theorem-3.7\"><span class=\"toc-item-num\">3.7&nbsp;&nbsp;</span>Bayes Theorem</a></span></li><li><span><a href=\"#Bayesian-Inference\" data-toc-modified-id=\"Bayesian-Inference-3.8\"><span class=\"toc-item-num\">3.8&nbsp;&nbsp;</span>Bayesian Inference</a></span></li><li><span><a href=\"#Naive-Bayes\" data-toc-modified-id=\"Naive-Bayes-3.9\"><span class=\"toc-item-num\">3.9&nbsp;&nbsp;</span>Naive Bayes</a></span></li></ul></li><li><span><a href=\"#-ML/DL-basics-introduction-\" data-toc-modified-id=\"-ML/DL-basics-introduction--4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span><font color=\"brown\"> ML/DL basics introduction </font></a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#-Perceptron-\" data-toc-modified-id=\"-Perceptron--4.0.1\"><span class=\"toc-item-num\">4.0.1&nbsp;&nbsp;</span><font color=\"DarkMagenta\"> Perceptron </font></a></span></li><li><span><a href=\"#Parametric-Vs-Non-Parametric-algorithms\" data-toc-modified-id=\"Parametric-Vs-Non-Parametric-algorithms-4.0.2\"><span class=\"toc-item-num\">4.0.2&nbsp;&nbsp;</span>Parametric Vs Non-Parametric algorithms</a></span></li><li><span><a href=\"#-Sigmoid-Neuron-\" data-toc-modified-id=\"-Sigmoid-Neuron--4.0.3\"><span class=\"toc-item-num\">4.0.3&nbsp;&nbsp;</span><font color=\"DarkMagenta\"> Sigmoid Neuron </font></a></span></li><li><span><a href=\"#Gradient-decent\" data-toc-modified-id=\"Gradient-decent-4.0.4\"><span class=\"toc-item-num\">4.0.4&nbsp;&nbsp;</span>Gradient decent</a></span></li><li><span><a href=\"#Backpropagation\" data-toc-modified-id=\"Backpropagation-4.0.5\"><span class=\"toc-item-num\">4.0.5&nbsp;&nbsp;</span>Backpropagation</a></span></li><li><span><a href=\"#Overfitting\" data-toc-modified-id=\"Overfitting-4.0.6\"><span class=\"toc-item-num\">4.0.6&nbsp;&nbsp;</span>Overfitting</a></span></li><li><span><a href=\"#How-to-avoid-overfitting\" data-toc-modified-id=\"How-to-avoid-overfitting-4.0.7\"><span class=\"toc-item-num\">4.0.7&nbsp;&nbsp;</span>How to avoid overfitting</a></span></li><li><span><a href=\"#Vanishing-Gradients\" data-toc-modified-id=\"Vanishing-Gradients-4.0.8\"><span class=\"toc-item-num\">4.0.8&nbsp;&nbsp;</span>Vanishing Gradients</a></span></li><li><span><a href=\"#How-to-avoid-vanishing-gradients\" data-toc-modified-id=\"How-to-avoid-vanishing-gradients-4.0.9\"><span class=\"toc-item-num\">4.0.9&nbsp;&nbsp;</span>How to avoid vanishing gradients</a></span></li><li><span><a href=\"#Cross-validation\" data-toc-modified-id=\"Cross-validation-4.0.10\"><span class=\"toc-item-num\">4.0.10&nbsp;&nbsp;</span>Cross validation</a></span></li><li><span><a href=\"#Types-of-data\" data-toc-modified-id=\"Types-of-data-4.0.11\"><span class=\"toc-item-num\">4.0.11&nbsp;&nbsp;</span>Types of data</a></span></li><li><span><a href=\"#Classification-and-Regression\" data-toc-modified-id=\"Classification-and-Regression-4.0.12\"><span class=\"toc-item-num\">4.0.12&nbsp;&nbsp;</span>Classification and Regression</a></span></li></ul></li></ul></li><li><span><a href=\"#-Supervised-Learning-\" data-toc-modified-id=\"-Supervised-Learning--5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span><font color=\"brown\"> Supervised Learning </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#-Parametric-and-Classification-Algorithms-\" data-toc-modified-id=\"-Parametric-and-Classification-Algorithms--5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span><font color=\"brown\"> Parametric and Classification Algorithms </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#Support-Vector-Machine-(SVM)\" data-toc-modified-id=\"Support-Vector-Machine-(SVM)-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>Support Vector Machine (SVM)</a></span></li><li><span><a href=\"#Logistic-regression\" data-toc-modified-id=\"Logistic-regression-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;</span>Logistic regression</a></span></li></ul></li><li><span><a href=\"#-Parametric-and-Regression-Algorithms-\" data-toc-modified-id=\"-Parametric-and-Regression-Algorithms--5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span><font color=\"brown\"> Parametric and Regression Algorithms </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#Linear-regression\" data-toc-modified-id=\"Linear-regression-5.2.1\"><span class=\"toc-item-num\">5.2.1&nbsp;&nbsp;</span>Linear regression</a></span></li></ul></li><li><span><a href=\"#Non-Parametric-Algorithms\" data-toc-modified-id=\"Non-Parametric-Algorithms-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Non-Parametric Algorithms</a></span><ul class=\"toc-item\"><li><span><a href=\"#K-Nearest-Neighbor-(KNN)\" data-toc-modified-id=\"K-Nearest-Neighbor-(KNN)-5.3.1\"><span class=\"toc-item-num\">5.3.1&nbsp;&nbsp;</span>K-Nearest Neighbor (KNN)</a></span></li><li><span><a href=\"#Decision-Tree\" data-toc-modified-id=\"Decision-Tree-5.3.2\"><span class=\"toc-item-num\">5.3.2&nbsp;&nbsp;</span>Decision Tree</a></span></li><li><span><a href=\"#Random-Forest:--an-ensemble-of-decision-trees\" data-toc-modified-id=\"Random-Forest:--an-ensemble-of-decision-trees-5.3.3\"><span class=\"toc-item-num\">5.3.3&nbsp;&nbsp;</span>Random Forest:  an ensemble of decision trees</a></span></li></ul></li></ul></li><li><span><a href=\"#-Unsupervised-Learning-\" data-toc-modified-id=\"-Unsupervised-Learning--6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span><font color=\"brown\"> Unsupervised Learning </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#Clusttering\" data-toc-modified-id=\"Clusttering-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Clusttering</a></span><ul class=\"toc-item\"><li><span><a href=\"#K-means-clusttering\" data-toc-modified-id=\"K-means-clusttering-6.1.1\"><span class=\"toc-item-num\">6.1.1&nbsp;&nbsp;</span>K-means clusttering</a></span></li><li><span><a href=\"#Expectation–Maximization-(EM)-Clustering-using-Gaussian-Mixture-Models-(GMM)\" data-toc-modified-id=\"Expectation–Maximization-(EM)-Clustering-using-Gaussian-Mixture-Models-(GMM)-6.1.2\"><span class=\"toc-item-num\">6.1.2&nbsp;&nbsp;</span>Expectation–Maximization (EM) Clustering using Gaussian Mixture Models (GMM)</a></span></li><li><span><a href=\"#Hierarchical-Clusttering\" data-toc-modified-id=\"Hierarchical-Clusttering-6.1.3\"><span class=\"toc-item-num\">6.1.3&nbsp;&nbsp;</span>Hierarchical Clusttering</a></span></li></ul></li><li><span><a href=\"#Dimensionality-Reduction\" data-toc-modified-id=\"Dimensionality-Reduction-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Dimensionality Reduction</a></span><ul class=\"toc-item\"><li><span><a href=\"#Principal-Component-Analysis-(PCA)\" data-toc-modified-id=\"Principal-Component-Analysis-(PCA)-6.2.1\"><span class=\"toc-item-num\">6.2.1&nbsp;&nbsp;</span>Principal Component Analysis (PCA)</a></span></li><li><span><a href=\"#Singular-Value-Decomposition-(SVD)\" data-toc-modified-id=\"Singular-Value-Decomposition-(SVD)-6.2.2\"><span class=\"toc-item-num\">6.2.2&nbsp;&nbsp;</span>Singular Value Decomposition (SVD)</a></span></li></ul></li></ul></li><li><span><a href=\"#-Loss-Functions-\" data-toc-modified-id=\"-Loss-Functions--7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span><font color=\"brown\"> Loss Functions </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#-Classification-Loss-Functions-\" data-toc-modified-id=\"-Classification-Loss-Functions--7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span><font color=\"brown\"> Classification Loss Functions </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#Square-loss\" data-toc-modified-id=\"Square-loss-7.1.1\"><span class=\"toc-item-num\">7.1.1&nbsp;&nbsp;</span>Square loss</a></span></li><li><span><a href=\"#Hinge-loss\" data-toc-modified-id=\"Hinge-loss-7.1.2\"><span class=\"toc-item-num\">7.1.2&nbsp;&nbsp;</span>Hinge loss</a></span></li><li><span><a href=\"#Logistic-loss\" data-toc-modified-id=\"Logistic-loss-7.1.3\"><span class=\"toc-item-num\">7.1.3&nbsp;&nbsp;</span>Logistic loss</a></span></li><li><span><a href=\"#Cross-entropy-loss\" data-toc-modified-id=\"Cross-entropy-loss-7.1.4\"><span class=\"toc-item-num\">7.1.4&nbsp;&nbsp;</span>Cross entropy loss</a></span></li></ul></li><li><span><a href=\"#-Regression-Loss-Functions-\" data-toc-modified-id=\"-Regression-Loss-Functions--7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span><font color=\"brown\"> Regression Loss Functions </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#L2-Loss,-Mean-Square-Error-(MSE),-Quadratic-loss\" data-toc-modified-id=\"L2-Loss,-Mean-Square-Error-(MSE),-Quadratic-loss-7.2.1\"><span class=\"toc-item-num\">7.2.1&nbsp;&nbsp;</span>L2 Loss, Mean Square Error (MSE), Quadratic loss</a></span><ul class=\"toc-item\"><li><span><a href=\"#-$-MSE-=-\\sum\\limits_{i=1}^n--{(y_i---y_i^p)}^2-$-\" data-toc-modified-id=\"-$-MSE-=-\\sum\\limits_{i=1}^n--{(y_i---y_i^p)}^2-$--7.2.1.1\"><span class=\"toc-item-num\">7.2.1.1&nbsp;&nbsp;</span> $ MSE = \\sum\\limits_{i=1}^n  {(y_i - y_i^p)}^2 $ </a></span></li></ul></li><li><span><a href=\"#L1-Loss,-Mean-Absolute-Error-(MAE)\" data-toc-modified-id=\"L1-Loss,-Mean-Absolute-Error-(MAE)-7.2.2\"><span class=\"toc-item-num\">7.2.2&nbsp;&nbsp;</span>L1 Loss, Mean Absolute Error (MAE)</a></span><ul class=\"toc-item\"><li><span><a href=\"#-$-MAE-=-\\sum\\limits_{i=1}^n--{|y_i---y_i^p|}-$-\" data-toc-modified-id=\"-$-MAE-=-\\sum\\limits_{i=1}^n--{|y_i---y_i^p|}-$--7.2.2.1\"><span class=\"toc-item-num\">7.2.2.1&nbsp;&nbsp;</span> $ MAE = \\sum\\limits_{i=1}^n  {|y_i - y_i^p|} $ </a></span></li></ul></li><li><span><a href=\"#L1-vs-L2-Loss\" data-toc-modified-id=\"L1-vs-L2-Loss-7.2.3\"><span class=\"toc-item-num\">7.2.3&nbsp;&nbsp;</span>L1 vs L2 Loss</a></span></li><li><span><a href=\"#Huber-Loss,-Smooth-Mean-Absolute-Error\" data-toc-modified-id=\"Huber-Loss,-Smooth-Mean-Absolute-Error-7.2.4\"><span class=\"toc-item-num\">7.2.4&nbsp;&nbsp;</span>Huber Loss, Smooth Mean Absolute Error</a></span></li><li><span><a href=\"#Log-CosH-Loss\" data-toc-modified-id=\"Log-CosH-Loss-7.2.5\"><span class=\"toc-item-num\">7.2.5&nbsp;&nbsp;</span>Log-CosH Loss</a></span></li><li><span><a href=\"#Quantile-Loss\" data-toc-modified-id=\"Quantile-Loss-7.2.6\"><span class=\"toc-item-num\">7.2.6&nbsp;&nbsp;</span>Quantile Loss</a></span></li></ul></li></ul></li><li><span><a href=\"#-Regularization-Algorithms-\" data-toc-modified-id=\"-Regularization-Algorithms--8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span><font color=\"brown\"> Regularization Algorithms </font></a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#-L1-(Lasso)-and-L2-(Ridge)-as-Reguralization-\" data-toc-modified-id=\"-L1-(Lasso)-and-L2-(Ridge)-as-Reguralization--8.0.1\"><span class=\"toc-item-num\">8.0.1&nbsp;&nbsp;</span><font color=\"DarkMagenta\"> L1 (Lasso) and L2 (Ridge) as Reguralization </font></a></span></li><li><span><a href=\"#Drop-out\" data-toc-modified-id=\"Drop-out-8.0.2\"><span class=\"toc-item-num\">8.0.2&nbsp;&nbsp;</span>Drop-out</a></span></li></ul></li></ul></li><li><span><a href=\"#-Bias-Variance-Trade-Off-\" data-toc-modified-id=\"-Bias-Variance-Trade-Off--9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span><font color=\"brown\"> Bias Variance Trade-Off </font></a></span></li><li><span><a href=\"#-CNN-Layer-Theory-\" data-toc-modified-id=\"-CNN-Layer-Theory--10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span><font color=\"brown\"> CNN Layer Theory </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#Rectified-Linear-Unit-(ReLU)\" data-toc-modified-id=\"Rectified-Linear-Unit-(ReLU)-10.1\"><span class=\"toc-item-num\">10.1&nbsp;&nbsp;</span>Rectified Linear Unit (ReLU)</a></span></li></ul></li><li><span><a href=\"#Data-Pre-Processing-or-Feature-Scaling\" data-toc-modified-id=\"Data-Pre-Processing-or-Feature-Scaling-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Data Pre-Processing or Feature Scaling</a></span><ul class=\"toc-item\"><li><span><a href=\"#Standard-deviation\" data-toc-modified-id=\"Standard-deviation-11.1\"><span class=\"toc-item-num\">11.1&nbsp;&nbsp;</span>Standard deviation</a></span></li><li><span><a href=\"#Mean-Normalization\" data-toc-modified-id=\"Mean-Normalization-11.2\"><span class=\"toc-item-num\">11.2&nbsp;&nbsp;</span>Mean Normalization</a></span></li><li><span><a href=\"#Normalization-and-Standardization\" data-toc-modified-id=\"Normalization-and-Standardization-11.3\"><span class=\"toc-item-num\">11.3&nbsp;&nbsp;</span>Normalization and Standardization</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#-$-norm(x)-=-\\frac{x---min(x)}{max(x)---min(x)}-$-\" data-toc-modified-id=\"-$-norm(x)-=-\\frac{x---min(x)}{max(x)---min(x)}-$--11.3.0.1\"><span class=\"toc-item-num\">11.3.0.1&nbsp;&nbsp;</span> $ norm(x) = \\frac{x - min(x)}{max(x) - min(x)} $ </a></span></li><li><span><a href=\"#-$-std(x)-=-\\frac{x---\\mu}{\\sigma}-$-\" data-toc-modified-id=\"-$-std(x)-=-\\frac{x---\\mu}{\\sigma}-$--11.3.0.2\"><span class=\"toc-item-num\">11.3.0.2&nbsp;&nbsp;</span> $ std(x) = \\frac{x - \\mu}{\\sigma} $ </a></span></li></ul></li></ul></li><li><span><a href=\"#Covariance-and-Correlation\" data-toc-modified-id=\"Covariance-and-Correlation-11.4\"><span class=\"toc-item-num\">11.4&nbsp;&nbsp;</span>Covariance and Correlation</a></span></li><li><span><a href=\"#Whitening\" data-toc-modified-id=\"Whitening-11.5\"><span class=\"toc-item-num\">11.5&nbsp;&nbsp;</span>Whitening</a></span></li><li><span><a href=\"#Batch-Normalization\" data-toc-modified-id=\"Batch-Normalization-11.6\"><span class=\"toc-item-num\">11.6&nbsp;&nbsp;</span>Batch Normalization</a></span></li></ul></li><li><span><a href=\"#Ensemble-Algorithms\" data-toc-modified-id=\"Ensemble-Algorithms-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;</span>Ensemble Algorithms</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Voting\" data-toc-modified-id=\"Voting-12.0.1\"><span class=\"toc-item-num\">12.0.1&nbsp;&nbsp;</span>Voting</a></span></li><li><span><a href=\"#Bagging-or-Bootstrap-Aggregation\" data-toc-modified-id=\"Bagging-or-Bootstrap-Aggregation-12.0.2\"><span class=\"toc-item-num\">12.0.2&nbsp;&nbsp;</span>Bagging or Bootstrap Aggregation</a></span></li><li><span><a href=\"#Random-Forests\" data-toc-modified-id=\"Random-Forests-12.0.3\"><span class=\"toc-item-num\">12.0.3&nbsp;&nbsp;</span>Random Forests</a></span></li><li><span><a href=\"#AdaBoost\" data-toc-modified-id=\"AdaBoost-12.0.4\"><span class=\"toc-item-num\">12.0.4&nbsp;&nbsp;</span>AdaBoost</a></span></li><li><span><a href=\"#Gradient-Boosting\" data-toc-modified-id=\"Gradient-Boosting-12.0.5\"><span class=\"toc-item-num\">12.0.5&nbsp;&nbsp;</span>Gradient Boosting</a></span></li><li><span><a href=\"#XGBoost\" data-toc-modified-id=\"XGBoost-12.0.6\"><span class=\"toc-item-num\">12.0.6&nbsp;&nbsp;</span>XGBoost</a></span></li></ul></li></ul></li><li><span><a href=\"#Gradient-Descent\" data-toc-modified-id=\"Gradient-Descent-13\"><span class=\"toc-item-num\">13&nbsp;&nbsp;</span>Gradient Descent</a></span></li><li><span><a href=\"#Back-Propagation\" data-toc-modified-id=\"Back-Propagation-14\"><span class=\"toc-item-num\">14&nbsp;&nbsp;</span>Back Propagation</a></span></li><li><span><a href=\"#Deep-Learning-Training\" data-toc-modified-id=\"Deep-Learning-Training-15\"><span class=\"toc-item-num\">15&nbsp;&nbsp;</span>Deep Learning Training</a></span><ul class=\"toc-item\"><li><span><a href=\"#Weight-Initialization\" data-toc-modified-id=\"Weight-Initialization-15.1\"><span class=\"toc-item-num\">15.1&nbsp;&nbsp;</span>Weight Initialization</a></span></li></ul></li><li><span><a href=\"#Hyper-Parameter-Tuning\" data-toc-modified-id=\"Hyper-Parameter-Tuning-16\"><span class=\"toc-item-num\">16&nbsp;&nbsp;</span>Hyper Parameter Tuning</a></span></li><li><span><a href=\"#Optimizers\" data-toc-modified-id=\"Optimizers-17\"><span class=\"toc-item-num\">17&nbsp;&nbsp;</span>Optimizers</a></span></li><li><span><a href=\"#DNN-Performance-Tuning\" data-toc-modified-id=\"DNN-Performance-Tuning-18\"><span class=\"toc-item-num\">18&nbsp;&nbsp;</span>DNN Performance Tuning</a></span></li><li><span><a href=\"#Recurrent-Neural-Networks-(RNN)\" data-toc-modified-id=\"Recurrent-Neural-Networks-(RNN)-19\"><span class=\"toc-item-num\">19&nbsp;&nbsp;</span>Recurrent Neural Networks (RNN)</a></span></li><li><span><a href=\"#Reinforcement-Learning\" data-toc-modified-id=\"Reinforcement-Learning-20\"><span class=\"toc-item-num\">20&nbsp;&nbsp;</span>Reinforcement Learning</a></span></li><li><span><a href=\"#-New-DNN-Research-Concepts\" data-toc-modified-id=\"-New-DNN-Research-Concepts-21\"><span class=\"toc-item-num\">21&nbsp;&nbsp;</span><font color=\"green\"> New DNN Research Concepts</font></a></span><ul class=\"toc-item\"><li><span><a href=\"#Transparency-by-Design-(TbD-Net)\" data-toc-modified-id=\"Transparency-by-Design-(TbD-Net)-21.1\"><span class=\"toc-item-num\">21.1&nbsp;&nbsp;</span>Transparency by Design (TbD Net)</a></span></li><li><span><a href=\"#Capsule-Net\" data-toc-modified-id=\"Capsule-Net-21.2\"><span class=\"toc-item-num\">21.2&nbsp;&nbsp;</span>Capsule Net</a></span></li><li><span><a href=\"#Zero-shot-learning\" data-toc-modified-id=\"Zero-shot-learning-21.3\"><span class=\"toc-item-num\">21.3&nbsp;&nbsp;</span>Zero shot learning</a></span></li></ul></li><li><span><a href=\"#-Interview-Questions-\" data-toc-modified-id=\"-Interview-Questions--22\"><span class=\"toc-item-num\">22&nbsp;&nbsp;</span><font color=\"orange\"> Interview Questions </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#Skills-required-for-ML/DL-Engineer-Jobs\" data-toc-modified-id=\"Skills-required-for-ML/DL-Engineer-Jobs-22.1\"><span class=\"toc-item-num\">22.1&nbsp;&nbsp;</span>Skills required for ML/DL Engineer Jobs</a></span></li></ul></li><li><span><a href=\"#-Best-Resources-\" data-toc-modified-id=\"-Best-Resources--23\"><span class=\"toc-item-num\">23&nbsp;&nbsp;</span><font color=\"brown\"> Best Resources </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#Good-resources-list\" data-toc-modified-id=\"Good-resources-list-23.1\"><span class=\"toc-item-num\">23.1&nbsp;&nbsp;</span>Good resources list</a></span></li><li><span><a href=\"#Good-practical-resources\" data-toc-modified-id=\"Good-practical-resources-23.2\"><span class=\"toc-item-num\">23.2&nbsp;&nbsp;</span>Good practical resources</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# ML DL Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red> Topics to learn </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn Now (most important)\n",
    "- [** ML Cheat sheet **](https://medium.com/machine-learning-in-practice/cheat-sheet-of-machine-learning-and-python-and-math-cheat-sheets-a4afe4e791b6)\n",
    "- Probability\n",
    "    - Bayes theorem, Naive bayes\n",
    "    - Prior and posterior\n",
    "    - MAP\n",
    "    - sampling methods\n",
    "- CNN layers theory\n",
    "    - Softmax\n",
    "    - Convolution types\n",
    "        - Depthwise separable convolution\n",
    "        - Shufflenet\n",
    "        - transposed convolution\n",
    "        - subpixel convolution\n",
    "        - dilation\n",
    "        - bottleneck layer\n",
    "- Network architectures\n",
    "- Gradient descent, back propagation\n",
    "- Hyper parameters and tuning\n",
    "    - learning rate\n",
    "    - batch size\n",
    "    - momentum (addresses convergence speed and local minima)\n",
    "    - learning rate decay, weight decay\n",
    "    - grid search, random search\n",
    "    - successive halving\n",
    "- Optimizers\n",
    "    - Adam\n",
    "    - Adagard\n",
    "- DNN performance tuning\n",
    "    - [tutorial](https://medium.com/@jonathan_hui/improve-deep-learning-models-performance-network-tuning-part-6-29bf90df6d2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn later\n",
    "- log odd ratio\n",
    "- Multiclass SVM\n",
    "- Kernel PCA\n",
    "- Weight normalization\n",
    "- instance normalization, layer normalization, group normalization\n",
    "- Regularization\n",
    "    - Elastic net\n",
    "- classification loss functions\n",
    "    - cross entropy loss function\n",
    "- generative and discriminative model\n",
    "- VAE\n",
    "- GANs\n",
    "- Gaussian Mixture Model (GMM)\n",
    "- Hidden Markov Models (HMM)    \n",
    "- confusion matrix\n",
    "- Precision and recall\n",
    "- ROI pooling\n",
    "- Deep learning studio\n",
    "- Teacher student training (knowledge distilation)\n",
    "- Study later\n",
    "    - Clusttering techniques\n",
    "    - svd\n",
    "    - lda\n",
    "    - binning, xgboost\n",
    "    - deep metric learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "The raw code for this IPython notebook is by default hidden for easier reading.\n",
       "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "The raw code for this IPython notebook is by default hidden for easier reading.\n",
    "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>.''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=brown> Probability and Statistics </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Probability\n",
    "- [Tutorial](https://towardsdatascience.com/probability-concepts-explained-introduction-a7c0316de465)\n",
    "- **Marginal Probability**\n",
    "    - If A is an event, then the marginal probability is the probability of that event occurring, P(A)\n",
    "    - an example of a marginal probability would be the probability that a card drawn from a pack is red: P(red) = 0.5\n",
    "- **Joint Probability**\n",
    "    - The probability of the intersection of two or more events.\n",
    "    - If A and B are two events then the joint probability of the two events is written as P(A ∩ B)\n",
    "    - Example: the probability that a card drawn from a pack is red and has the value 4 is P(red and 4) = 2/52 = 1/26\n",
    "- **Conditional Probability**\n",
    "    - The conditional probability is the probability that some event(s) occur given that we know other events have already occurred\n",
    "    - If A and B are two events then the conditional probability of A occurring given that B has occurred is written as P(A|B)\n",
    "    - Example: the probability that a card is a four given that we have drawn a red card is P(4|red) = 2/26 = 1/13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Central Limit Theorem (CLT)\n",
    "- [CLT Simple Explanation](http://blog.minitab.com/blog/understanding-statistics/how-the-central-limit-theorem-works)\n",
    "- Central limit theorem says that if you have a sufficient number of randomly selected, independent samples (or observations), the <font color=blue> means of those samples will follow a normal distribution </font> -- even if the population you're sampling from does not!\n",
    "- As shown in below figure, the samples of rolling die distribution is almost uniform (as each number of die has same probability 1/6 follows uniform distribution)\n",
    "- ** The means of consecutive samples (5 in the example) follows gaussian distribution, with more number of samples it comes close to gaussian **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x216 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x216 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CLT visualization code\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# seed the random generator\n",
    "np.random.seed(1)\n",
    "# generate roll dies\n",
    "num_rolls = [150, 30000]\n",
    "#plt.figure(figsize=(7,7))\n",
    "for i, num_samples in enumerate(num_rolls):\n",
    "    # generate random samples\n",
    "    rolls = np.random.randint(1, 7, num_samples)\n",
    "    \n",
    "    # find means of consecutive random samples\n",
    "    #print(rolls)\n",
    "    step = 5\n",
    "    means = []\n",
    "    for j in range(0, num_samples, step):\n",
    "        means.append(np.mean(rolls[j:j+step]))\n",
    "        #print(rolls[j:j+step])    \n",
    "    #print(means)\n",
    "    \n",
    "    # plot samples histogram\n",
    "    plt.figure(figsize=(6,3))    \n",
    "    plt.subplot(1,2,1)\n",
    "    plt.hist(rolls)\n",
    "    if i == 0:\n",
    "        plt.title('samples distribution')\n",
    "    # plot samples mean histogram\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.hist(means)         \n",
    "    if i == 0:\n",
    "        plt.title('means of samples distribution')\n",
    "    plt.text(7,0,'number of samples: %d'%num_samples, ha='left', va='bottom')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bernoulli Distribution\n",
    "- The Bernoulli distribution is a model for an experiment that has only two possible outcomes. The corresponding experiment, which has only two possible outcomes is said to be a Bernoulli trial.\n",
    "- ** Bernoulli checks for specific outcome though the experiment has more outcomes.** Suppose a random experiment **has two outcomes**, namely Success and Failure.\n",
    "- example:\n",
    "    - In a single throw of a dice, the outcome \"5\" is called a success and any other outcome (not rolling a 5) is called a failure, then the successive throws of a dice will contain Bernoulli trials.The probability of success = 1/6 and the probability of failure = 5/6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binomial Theorem\n",
    "- [Binomial tutorial](https://www.analyticsvidhya.com/blog/2017/02/basic-probability-data-science-with-examples/)\n",
    "- ** binomial distribution is the discrete probability distribution of the number of success in a sequence of n independent Bernoulli trials (having only yes/no or true/false outcomes). **\n",
    "- so, the Probability for getting k successes in n Bernoulli trails is given by:\n",
    "    - P(X=k) = n<sub>C<sub>k</sub></sub> p<sup>k</sup> q<sup>(n-k)</sup>,  [here p is the probability of success and q is the probability of failure]\n",
    "- If probability of each trail is equal, then the binomial probability distribution is also  equal as shown in graph.2\n",
    "- If we increase the number of trails, there will be many combinations and bars get thinner and thinner as shown in graph.3\n",
    "- What if we play infinite number of trails, ** the bars get infinitely small and the probability distribution looks something like a continuous set of bars which are very close **, almost continuous as shown in graph.4. This now becomes <font color=blue> a probability density function </font>\n",
    "- An observation is, **probability is highest at mean value** and decrease as we move away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x216 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Binomial example visualization: find the probability of winning series in 5 games\n",
    "from scipy.stats import binom\n",
    "\n",
    "plt.figure(figsize=(13,3))\n",
    "trails = [5, 5, 20, 1000]\n",
    "win_prob = [0.75, 0.5, 0.5, 0.5]\n",
    "for i, prob in enumerate(win_prob):\n",
    "    num_games = trails[i]\n",
    "    binom_prob = binom(num_games, prob)\n",
    "\n",
    "    x = [i for i in range(0, num_games+1, 1)]\n",
    "    # plot    \n",
    "    plt.subplot(1,4,i+1)\n",
    "    plt.bar(x, binom_prob.pmf(x))  \n",
    "    plt.xlabel('number of trails')\n",
    "    if i == 0:\n",
    "        plt.ylabel('probability')\n",
    "        plt.title('UnEqual probability')\n",
    "    elif i == 1:\n",
    "        plt.title('equal probability')\n",
    "    else:\n",
    "        plt.title('equal prob. #more trails')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Normal Distribution or Z-distribution\n",
    "- [Tutorial](https://www.analyticsvidhya.com/blog/2017/02/basic-probability-data-science-with-examples/)\n",
    "- In order to make the proper comparisons among different normal distributions which are centered around their own averages, **the distributions has to standardized to same scale to have a fair/correct comparison**\n",
    "- The standard normal distribution, commonly referred to the Z-distribution, is a special case of a normal distribution with the following properties:\n",
    "    - It has a mean of zero.\n",
    "    - It has a standard deviation of one.\n",
    "- **Z-scores:**\n",
    "    - **The distance in terms of number of standard deviations, the observed value is away from the mean, is the standard score or the Z score.**\n",
    "    - A score on the standard normal distribution is called a Z-Score. It should be interpreted as the number of standard deviations a data point is above or below the mean. A positive Z-Score indicates that a point is above the average, and a negative Z-Score indicates a score below the average.\n",
    "    - Data is rescaled such that mean (μ) = 0 and standard deviation (𝛔) = 1 as,\n",
    "    <h4 align=\"center\"> $ std(x) = \\frac{x - \\mu}{\\sigma} $ </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardization visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimation (MLE)\n",
    "- [Tutorial intro](https://www.analyticsvidhya.com/blog/2018/07/introductory-guide-maximum-likelihood-estimation-case-study-r/)\n",
    "- [Tutorial calculation](https://towardsdatascience.com/probability-concepts-explained-maximum-likelihood-estimation-c7b4342fdbb1)\n",
    "- While studying stats and probability, you must have come across problems like – What is the probability of x > 100, given that x follows a normal distribution with mean 50 and standard deviation (sd) 10. In such problems, we already know the distribution (normal in this case) and its parameters (mean and sd).\n",
    "- but in real life problems these quantities are unknown and must be estimated from the data. <font color=blue> MLE is the technique which helps us in determining the parameters of the distribution that best describe the given data </font>.\n",
    "- ** MLE can be defined as a method for estimating population parameters (such as the mean and variance for Normal, rate (lambda) for Poisson, etc.) from sample data such that the probability (likelihood) of obtaining the observed data is maximized. **\n",
    "- we want to calculate is the total probability of observing all of the data, i.e. the joint probability distribution of all observed data points. To do this we would need to calculate some conditional probabilities, which can get very difficult. So it is here that we’ll make our first assumption. The **assumption is that each data point is generated independently of the others.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Theorem\n",
    "- [Bayes Intro](https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/)\n",
    "- [Bayes Example: Rich people given happy](https://www.quora.com/What-is-an-intuitive-explanation-of-Bayes-Rule)\n",
    "- [Bayesian inference](https://towardsdatascience.com/probability-concepts-explained-bayesian-inference-for-parameter-estimation-90e8930e5348)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=brown> ML/DL basics introduction </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### <font color=DarkMagenta> Perceptron </font>\n",
    "- Takes several 'binary' inputs to produce a 'binary' output by comparing the weighted sum of inputs against a threshold, \n",
    "- The threshold is treated as bias and moved to left side of equation to compare weighted sum against zero\n",
    "- If a small change in a weight or bias causes only a small change in output, it is possible for a network to learn. \n",
    "- But, this doesn't happen with perceptrons sometimes as <font color=blue>small change in weights can entirely flip the output</font> from say 1 to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametric Vs Non-Parametric algorithms\n",
    "- Parametric\n",
    "    - Parametric methods makes an assumption about the form of the function relating X and Y\n",
    "    - Linear regression is a parametric method\n",
    "- Non-Parametric\n",
    "    - non-parametric learners do not have a model structure specified a priori. \n",
    "    - We don’t speculate about the form of the function f that we are trying to learn before training the model, as we did previously with linear regression. \n",
    "    - Instead, the model structure is purely determined from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=DarkMagenta> Sigmoid Neuron </font>\n",
    "- Sigmoid neurons are similar to perceptrons (shape is a smoothed out version of a step function), <br> but modified so that **small changes in their weights and bias cause only a small change in their output**\n",
    "- instead of being just 0 or 1, these inputs can also take on any values between 0 and 1\n",
    "- output is not 0 or 1. Instead, it's σ(w⋅x+b), where σ is called the sigmoid function\n",
    "- Somewhat confusingly, and for historical reasons, such multiple layer networks are sometimes called multilayer perceptrons or MLPs, despite being made up of sigmoid neurons, not perceptrons. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient decent\n",
    "- To quantify how well we're achieving this goal we define a cost function\n",
    "- to find a set of weights and biases which make the cost as small as possible. We'll do that using an algorithm known as gradient descent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting\n",
    "- One of the problems that occur during neural network training is called overfitting. \n",
    "- The error on the training set is driven to a very small value, but when new data is presented to the network the error is large. The network has <font color=blue>memorized the training examples, but it has not learned to generalize to new situations</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to avoid overfitting\n",
    "- Go for simpler models over more complicated models. Generally, the **fewer parameters** that you have to tune the better. \n",
    "- Use **more data** to train the model. \n",
    "- Some sort of <font color= blue>regularization</font> can help penalize certain sources of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanishing Gradients\n",
    "- if a change in the parameter's value causes very small change in the network's output - the network just can't learn the parameter effectively, which is a problem.\n",
    "-  For example, **sigmoid maps the real number line onto a \"small\" range** of [0, 1]. As a result, there are large regions of the input space which are mapped to an extremely small range. In these regions of the input space, even a large change in the input will produce a small change in the output - hence the gradient is small.\n",
    "- This **becomes much worse when we stack multiple layers** of such non-linearities on top of each other. <br> For instance, first layer will map a large input region to a smaller output region, which will be mapped to an even smaller region by the second layer, which will be mapped to an even smaller region by the third layer and so on. ** As a result, even a large change in the parameters of the first layer doesn't change the output much **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to avoid vanishing gradients\n",
    "- We can avoid this problem by using activation functions which don't have this property of 'squashing' the input space into a small region. \n",
    "- A popular <font color=blue>choice is Rectified Linear Unit</font> which maps x to max(0,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation\n",
    "- **Cross validation is a method for estimating the prediction accuracy of a model.**\n",
    "- One way to evaluate a model is to see how well it predicts the data used to fit the model. But this is too optimistic -- a model tailored to a particular data set will make better predictions on that data set than on new data. \n",
    "- Another way is to hold out some data and fit the model using the rest. Then you can test your accuracy on the holdout data.  But the held out data is \"wasted\" from the point of view of building the model. If you have huge amounts of data, so holding some data out won't make the model much worse\n",
    "- Cross validation does something like this but tries to <font color=blue>make more efficient use of the data</font>: you divide the data into (say) 10 equal parts. Then **successively hold out each part and fit the model using the rest**. This gives you 10 estimates of prediction accuracy which can be combined into an overall measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of data\n",
    "- ** Categorical**: Categorical variables take on values that are names or labels. The colour of a ball (e.g., red, green, blue) or the breed of a dog (e.g., collie, shepherd, terrier) would be examples of categorical variables.\n",
    "- ** Quantitative **: Quantitative variables are numerical. They represent a measurable quantity. For example, when we speak of the population of a city, we are talking about the number of people in the city — a measurable attribute of the city. Therefore population would be a quantitative variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification and Regression\n",
    "- **So in very simple terms, classification is about predicting a label and regression is about predicting a quantity**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=brown> Supervised Learning </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=brown> Parametric and Classification Algorithms </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM)\n",
    "- [SVM Overview](https://towardsdatascience.com/support-vector-machines-a-brief-overview-37e018ae310f)\n",
    "- [SVM kernel trick example](https://medium.com/machine-learning-for-humans/supervised-learning-2-5c1c23f3560d)\n",
    "- SVM is also a <font color=blue> binary classifier </font> (classifies 2 classes) like logistic regression\n",
    "- Support vector machines attempt to pass a <font color=blue> linearly separable hyperplane through a dataset in order to classify the data into two groups </font>\n",
    "- This hyperplane is a linear separator for any dimension; it could be a line (2D), plane (3D), and hyperplane (4D+)\n",
    "- the <font color=blue> best hyperplane is the one that maximizes the margin </font>. The margin is the distance between the hyperplane and a few close points. These <font color=blue> close points are the support vectors because they control the hyperplane. </font>\n",
    "- The classes have to be linearly separable to be classified using SVM, a variant of SVM is proposed to classify the data's which are not perfectly separable, it is known as a <font color=blue> Soft Margin Classifier or a Support Vector Classifier </font>, which allows slight mis-classification. SVM classifier contains a tuning parameter in order to control how much misclassification it will allow\n",
    "- **Kernel Trick**:\n",
    "    - The non-linear lower feature space from lower dimension is transformed to higher dimension to classify non-linear, which is known as kernel trick\n",
    "    - these kernels transform our data in order to pass a linear hyperplane and thus classify our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "- The idea of <font color=blue>Logistic Regression is to find a relationship between features and probability of particular outcome.</font>\n",
    "-  Logistic regression works largely the same way linear regression works: it multiplies each input by a coefficient, sums them up, and adds a constant. <font color=blue> In logistic regression, however, the output is actually the log of the odds ratio. </font>\n",
    "- This type of a problem is referred to as **Binomial Logistic Regression**, where the response variable has two values 0 and 1 or pass and fail or true and false. **Multinomial Logistic Regression deals** with situations where the response variable can have three or more possible values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=brown> Parametric and Regression Algorithms </font>\n",
    "- **Regression is a statistical way to establish a relationship between a dependent variable and a set of independent variable(s)**\n",
    "- Regression is concerned with modeling the relationship between variables that is iteratively refined using a measure of error in the predictions made by the model.\n",
    "- Regression methods are a workhorse of statistics and have been co-opted into statistical machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression\n",
    "- While doing linear regression our objective is to **fit a line through the distribution which is nearest to most of the points**. Hence reducing the distance (error term) of data points from the fitted line. \n",
    "- It is conventional to use squares, as Regression line minimizes the sum of “Square of Residuals”. \n",
    "- That’s why the method of Linear Regression is <font color=blue> known as “Ordinary Least Square (OLS)”</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Parametric Algorithms\n",
    "- [Non-Parametric Supervised Learning Tutorial](https://medium.com/machine-learning-for-humans/supervised-learning-3-b1551b9c4930)\n",
    "- non-parametric learners do not have a model structure specified a priori. We don’t speculate about the form of the function f that we’re trying to learn before training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbor (KNN)\n",
    "- You look at the k closest data points and take the average of their values if variables are continuous (like housing prices), or the mode if they’re categorical (like cat vs. dog)\n",
    "- **Choosing k:** tuning hyperparameters with cross-validation\n",
    "    - To decide which value of k to use, you can **test different k-NN models** using different values of k with cross-validation\n",
    "    - Pick whichever yields the lowest error, on average, across all iterations\n",
    "- Higher values of k help address overfitting, but if the value of k is too high your model will be very biased and inflexible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "- Making a good decision tree is like playing a game of “20 questions”\n",
    "- There are ways to quantify information gain so that you can essentially evaluate every possible split of the training data and maximize information gain for every split\n",
    "- **Choosing splits in a decision tree**\n",
    "    - **Entropy is the amount of disorder in a set** (measured by Gini index or cross-entropy)\n",
    "    - If the values are really mixed, there’s lots of entropy; if you can cleanly split values, there’s no entropy.\n",
    "    - **For every split at a parent node, you want the child nodes to be as pure as possible (minimize entropy.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest:  an ensemble of decision trees\n",
    "- A model comprised of many models is called an **ensemble model**, and this is usually a winning strategy.\n",
    "- A single decision tree can make a lot of wrong calls because it has very black-and-white judgments. \n",
    "- A random forest is a meta-estimator that aggregates many decision trees, with some helpful modifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=brown> Unsupervised Learning </font>\n",
    "- [Unsupervised Tutorial](https://medium.com/machine-learning-for-humans/unsupervised-learning-f45587588294)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clusttering\n",
    "- [Top5 Clustterings](https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68)\n",
    "- The goal of clustering is to create groups of data points such that points in different clusters are dissimilar while points within a cluster are similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means clusttering\n",
    "- [K-means tutorial](https://towardsdatascience.com/clustering-using-k-means-algorithm-81da00f156f6)\n",
    "- k-means clustering works on unlabeled data to cluster our data points into k groups. \n",
    "- A larger k creates smaller groups with more granularity, a lower k means larger groups and less granularity.\n",
    "- The output of the algorithm would be a set of “labels” assigning each data point to one of the k groups.\n",
    "- In k-means clustering, the way these **groups are defined is by creating a centroid** for each group. The centroids are like the heart of the cluster, they “capture” the points closest to them and add them to the cluster.\n",
    "- **K-means algorithm steps**\n",
    "    1. **Define the k centroids.**\n",
    "        - Initialize these at random (there are also fancier algorithms for initializing the centroids that end up converging more effectively).\n",
    "    2. **Find the closest centroid & update cluster assignments.**\n",
    "        - Assign each data point to one of the k clusters. Each data point is assigned to the nearest centroid’s cluster. Here, the measure of “nearness” is a hyperparameter — often **Euclidean distance**.\n",
    "        - If we’re using the Euclidean distance between data points and every centroid, a straight line is drawn between two centroids, then a perpendicular bisector (boundary line) divides this line into two clusters\n",
    "    3. **Move the centroids to the center of their clusters.**\n",
    "        - The new position of each centroid is calculated as the average position of all the points in its cluster.\n",
    "    - Keep repeating steps 2 and 3 until the centroid stop moving a lot at each iteration (i.e., until the algorithm converges)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expectation–Maximization (EM) Clustering using Gaussian Mixture Models (GMM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Clusttering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "- Dimensionality reduction looks a lot like compression. This is about trying to reduce the complexity of the data while keeping as much of the relevant structure as possible.\n",
    "- reducing the dimension of the feature space is called “dimensionality reduction.” There are many ways to achieve dimensionality reduction, but most of these techniques fall into one of two classes:\n",
    "    - Feature Elimination\n",
    "    - Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)\n",
    "- [PCA Tutorial with example](https://georgemdallas.wordpress.com/2013/10/30/principal-component-analysis-4-dummies-eigenvectors-eigenvalues-and-dimension-reduction/)\n",
    "- [PCA Tutorial](https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c)\n",
    "- The principal components are the underlying structure in the data. They represent the directions in which the data has maximum variance and also the directions in which the data is most spread out.\n",
    "- Principal component analysis is a technique for feature extraction — so it combines our input variables in a specific way, then we can drop the “least important” variables while still retaining the most valuable parts of all of the variables!\n",
    "- As an added benefit, each of the “new” variables **after PCA are all independent of one another**. This is a benefit because the assumptions of a linear model require our independent variables to be independent of one another\n",
    "- **Eigenvector and Eigenvalue**\n",
    "    - eigenvector was the direction of the line drawn to find the maximum variance\n",
    "    - eigenvalue was a number that tells us how the data set is spread out on the line which is an eigenvector.\n",
    "    - **amount of eigenvectors that exist equals the number of dimensions the data set has** are perpendicular/orthogonal to each other\n",
    "    - <font color=blue> The big eigen vector (with highest variance) is the principal component </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition (SVD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=brown> Loss Functions </font>\n",
    "- All the algorithms in machine learning rely on minimizing or maximizing a function, which we call “**objective function**”. The group of functions that are minimized are called “loss functions”. \n",
    "- A loss function is a measure of how good a prediction model does in terms of being able to predict the expected outcome. \n",
    "- A most commonly used method of finding the minimum point of function is “gradient descent”.\n",
    "- Loss functions can be broadly categorized into 2 types: <font color=blue>Classification loss and Regression Loss</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=brown> Classification Loss Functions </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Square loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hinge loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic loss\n",
    "- Logistic prediction equation: Yp = Sigmoid(Wi * x + b)\n",
    "- Loss (Yp, Yl) = - (Yl logYp + (1 - Yl) log (1- Yp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=brown> Regression Loss Functions </font>\n",
    "- [** 5 regression loss funtioncs **](https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 Loss, Mean Square Error (MSE), Quadratic loss\n",
    "- Mean Square Error (MSE) is the most commonly used regression loss function. MSE is the sum of squared distances between our target variable and predicted values.\n",
    "\n",
    "<h4 align=\"center\"> $ MSE = \\sum\\limits_{i=1}^n  {(y_i - y_i^p)}^2 $ </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 Loss, Mean Absolute Error (MAE)\n",
    "- MAE is the sum of absolute differences between our target and predicted variables. \n",
    "- So it ** measures the average magnitude of errors ** in a set of predictions, without considering their directions. \n",
    "- If we consider directions also, that would be called ** Mean Bias Error (MBE) **\n",
    "\n",
    "<h4 align=\"center\"> $ MAE = \\sum\\limits_{i=1}^n  {|y_i - y_i^p|} $ </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 vs L2 Loss\n",
    "- [L1 vs L2 Comparison](http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/)\n",
    "- <font color=blue>L1 loss is more robust to outliers than L2</font>,\n",
    "    - Since MSE squares the error (y — y_predicted = e), the value of error (e) increases a lot if e > 1. \n",
    "    - If we have an outlier in our data, the value of e will be high and e² will be >> |e|. \n",
    "    - This will make the model with **MSE loss give more weight to outliers ** than a model with MAE loss.\n",
    "    - MAE loss is useful if the training data is corrupted with outliers\n",
    "- <font color=blue>L1 loss derivative is not continuous, hence inefficient to find solution, i.e. unstable</font>,\n",
    "    - which can lead to missing minima\n",
    "    - As L2 derivative is continuous, it gives more stable solution, however it not robust in case of outliers\n",
    "- <font color=blue>Issue with L1 and L2 loss functions:</font>\n",
    "    - There can be cases where neither loss function gives desirable predictions. \n",
    "    - **For example,** if 90% of observations in our data have true target value of 150 and the remaining 10% have target value between 0–30. \n",
    "    - Then a model with MAE as loss might predict 150 for all observations, ignoring 10% of outlier cases, as it will try to go towards median value. \n",
    "    - In the same case, a model using MSE would give many predictions in the range of 0 to 30 as it will get skewed towards outliers. Both results are undesirable in many business cases.\n",
    "    - **An easy fix would be to transform the target variables. Another way is to try a different loss function. This is the motivation behind our 3rd loss function, Huber loss.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huber Loss, Smooth Mean Absolute Error\n",
    "- Huber loss is less sensitive to outliers in data than the squared error loss. It’s also **differentiable at 0**. \n",
    "- It’s basically absolute error, which becomes quadratic when error is small.\n",
    "- problem with Huber loss is that we might need to train hyper parameter delta which is an iterative process\n",
    "- ** it’s twice differentiable everywhere **\n",
    "- Many ML model implementations like XGBoost use Newton’s method to find the optimum, which is why the second derivative (Hessian) is needed. For ML frameworks like XGBoost, **twice differentiable functions are more favorable**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log-CosH Loss\n",
    "- Log-cosh is another function used in regression tasks that’s **smoother than L2**. \n",
    "- Log-cosh is the logarithm of the hyperbolic cosine of the prediction error.\n",
    "- 'logcosh' works mostly like the mean squared error, but will ** not be so strongly affected by the occasional wildly incorrect prediction **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantile Loss\n",
    "- Quantile loss functions turns out to be useful when we are interested in predicting an interval instead of only point predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=brown> Regularization Algorithms </font>\n",
    "- To avoid over optimizing/over-fitting the training set to use early termination as soon as the learning stops, other method is ** to use regularization **\n",
    "- An extension made to another method (typically regression methods) that <font color=blue>penalizes models based on their complexity</font>, favoring simpler models that are also better at generalizing.\n",
    "- other regularization technique is **dropout**\n",
    "- [Regularization techniques](https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-regularization-techniques/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=DarkMagenta> L1 (Lasso) and L2 (Ridge) as Reguralization </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop-out\n",
    "- remove few connections randomly to force the network to learn redundant representation of input, so that it doesn't overfit and depend on any particular parameter, so that all learns independently\n",
    "- The dropout rate is the fraction of the features that are zeroed out; it’s usually set between 0.2 and 0.5. \n",
    "- At test time, no units are dropped out; instead, the layer’s output values are scaled down by a factor equal to the dropout rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=brown> Bias Variance Trade-Off </font>\n",
    "- [Bias Variance Trade-off Tutorial](https://www.listendata.com/2017/02/bias-variance-tradeoff.html)\n",
    "- [Bias Variance Trade-off Infograph](https://elitedatascience.com/bias-variance-tradeoff)\n",
    "- ** Bias **\n",
    "    - Bias is a measure of the prediction accuracy on training data\n",
    "    - <font color=blue> High bias means low prediction accuracy </font>, which means model may be too simple not able learn from training data known as underfitting\n",
    "    - For example, a linear regression model would have high bias when trying to model a non-linear relationship\n",
    "    - High Bias Techniques\n",
    "        - Linear Regression, Linear Discriminant Analysis and Logistic Regression\n",
    "    - Low Bias Techniques\n",
    "        - Decision Trees,  K-nearest neighbours and Gradient Boosting\n",
    "    - <font color=blue> Parametric algorithms which assume something about the distribution of the data points </font> suffer from High Bias. Whereas non-parametric algorithms which does not assume anything special about distribution have low bias.\n",
    "- ** Variance **\n",
    "    - Variance is a measure of the generalization of the network\n",
    "    - <font color=blue> High variance means less generalization </font>, complex models that fits well on training data but they cannot generalise the pattern well which results to overfitting\n",
    "    - Low Variance Techniques\n",
    "        - Linear Regression, Linear Discriminant Analysis and Logistic Regression\n",
    "    - High Variance Techniques\n",
    "        - Decision Trees,  K-nearest neighbours and SVM\n",
    "- ** Bias Variance Trade-off **\n",
    "    - It means there is a trade-off between predictive accuracy and generalization of pattern outside training data. Increasing the accuracy of the model will lead to less generalization of pattern outside training data. Increasing the bias will decrease the variance. Increasing the variance will decrease the bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=brown> CNN Layer Theory </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rectified Linear Unit (ReLU)\n",
    "- [Guide to ReLU](https://medium.com/tinymind/a-practical-guide-to-relu-b83ca804f1f7)\n",
    "- ReLU\n",
    "    - defined as y = max(0, x), doesn’t “saturate,” when x gets large. Hence, doesn’t have the vanishing gradient problem suffered by other activation functions like sigmoid or tanh\n",
    "- Dying ReLU\n",
    "    - A ReLU neuron is “dead” if it’s stuck in the negative side and always outputs 0. \n",
    "    - Because the slope of ReLU in the negative range is also 0, once a neuron gets negative, it’s unlikely for it to recover. \n",
    "    - Such neurons are not playing any role in discriminating the input and is essentially useless.\n",
    "- Leaky ReLU\n",
    "    - Leaky ReLU has a small slope for negative values, instead of altogether zero. \n",
    "    - For example, leaky ReLU may have ** y = 0.01x ** when x < 0\n",
    "- Parametric ReLU (PReLU)\n",
    "    - Parametric ReLU (PReLU) is a type of leaky ReLU that, instead of having a predetermined slope like 0.01, makes it a parameter for the neural network to figure out itself: y = ax when x < 0.\n",
    "- Exponential Linear (ELU, SELU)\n",
    "    - Similar to leaky ReLU, ELU has a small slope for negative values. Instead of a straight line, it ** uses a log curve ** y = a(ex-1)\n",
    "    - sometimes called Scaled ELU (SELU) due to the constant factor a\n",
    "    - leaky ReLU while it doesn’t have the dying ReLU problem, it saturates for large negative values, allowing them to be essentially inactive\n",
    "    - It is designed to combine the good parts of ReLU and leaky ReLU\n",
    "- Concatenated ReLU (CReLU)\n",
    "    - Concatenated ReLU has two outputs, one ReLU and one negative ReLU, concatenated together. \n",
    "    - In other words, for positive x it produces [x, 0], and for negative x it produces [0, x]. \n",
    "    - Because it has two outputs, CReLU doubles the output dimension.\n",
    "- ReLU-6\n",
    "    - ReLU capped at 6, 6 is an arbitrary choice that worked well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing or Feature Scaling\n",
    "- Standardization\n",
    "- Mean Normalization\n",
    "- Min-Max Scaling\n",
    "- Unit Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard deviation\n",
    "- [Standard deviation tutorial](https://www.mathsisfun.com/data/standard-deviation.html#WhySquare)\n",
    "- The Standard Deviation is a measure of how spread out numbers are (compared to mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization and Standardization\n",
    "- [Standardization Vs Normalization](https://medium.com/@rrfd/standardize-or-normalize-examples-in-python-e3f174b65dfc)\n",
    "- **Normalization (Min-Max Scaling):**\n",
    "    - If the training data contain some features with higher magnitude (outliers), then the results might be dominated by them instead of all the features, hence normalize the data with largest value\n",
    "    - Normalization makes training less sensitive to the scale of features, so we can better solve for coefficients\n",
    "    - Normalizing will ensure that a convergence problem does not have a massive variance, making optimization feasible\n",
    "    - Normalization modifies the data range to [0, 1] using below formula,\n",
    "    <h4 align=\"center\"> $ norm(x) = \\frac{x - min(x)}{max(x) - min(x)} $ </h4>\n",
    "- **Standardization (Z-Score Normalization)**\n",
    "    - Data is rescaled such that mean (μ) = 0 and standard deviation (𝛔) = 1 as,\n",
    "    <h4 align=\"center\"> $ std(x) = \\frac{x - \\mu}{\\sigma} $ </h4>\n",
    "    - **standardized (z-score) or studentized (t-scores) scores to increase comparability of different features in the training data**\n",
    "    - **to arrive at (transformed) data that follows a normal distribution**\n",
    "    - But if original data does not follow normal distribution, then transformed data also may not follow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance and Correlation\n",
    "- [Covariance and Correlation Tutorial](https://medium.com/@adamzerner/covariance-and-correlation-d4c64769d4f1)\n",
    "- Covariance\n",
    "    - **Covariance is a measure of how much two random variables vary together**. It’s similar to variance, but where variance tells you how a single variable varies, covariance tells you how two variables vary together\n",
    "    - positive covariance implies a direct relationship between the variables (increasing x increases y)\n",
    "    - negative covariance implies an indirect relationship between the variables (increasing x decreases y)\n",
    "    - ** If the covariance is large, so there is a strong relationship between the numbers** or if the covariance is small, so there is a weak relationship between the numbers\n",
    "- Correlation\n",
    "    - **The covariance can tell the direction/variability between two data, can not tell the how much is the relation**, since the data expressed in different units (ex: mm, cm, meters) yields varying covariance (large/small)\n",
    "    - <font color=blue>Hence the data's has to be standardized to find the strength of covariance, the standardized covariance known as Correlation which tells the strength</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whitening"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "- [Batch Normalization Tutorial](https://www.learnopencv.com/batch-normalization-in-deep-networks/)\n",
    "- [Batch Normalization Explained](https://blog.paperspace.com/busting-the-myths-about-batch-normalization/)\n",
    "- Covariate Shift\n",
    "    - covariate shift is defined as a change in the distribution of data\n",
    "    - If a DNN is trained on a data set containing a class (say red roses) and similar class with different attributes (say white roses)\n",
    "    - These two classes will be present in different regions in the feature space, this difference in distribution is called the covariate shift\n",
    "    - So while training, the mini batch should have the same distribution (of both red and white roses) sampled from the entire training data set\n",
    "    - When the mini-batches have images uniformly sampled from the entire distribution, there is negligible covariate shift\n",
    "    - However, when the mini-batches are sampled from only one of the two subsets, there is a ** significant covariate shift **. This makes the training of the rose vs non-rose classifier very slow\n",
    "    - An easy way to solve this problem to normalize the inputs (mini batch) to the neural network so that the input distribution have a zero mean and a unit variance\n",
    "- **Internal Covariate Shift**\n",
    "    - However, when the networks get deeper, say, 20 or more layers, the minor fluctuations in weights over more than 20 odd layers can produce big changes in the distribution of the input being fed to deeper layers even if the input is normalized.\n",
    "    - Just as it made intuitive sense to have a uniform distribution for the input layer, it is advantageous to have the same input distribution for each hidden unit over time while training. \n",
    "    - But in a neural network, each ** hidden unit’s input distribution changes every time there is a parameter update in the previous layer **. This is called internal covariate shift. \n",
    "    - This makes training slow and requires a very small learning rate and a good parameter initialization.\n",
    "- This problem is solved by normalizing the layer’s inputs over a mini-batch and this process is therefore called Batch Normalization.\n",
    "- However, as against above discussion, batch norm actually ends up increasing internal covariate shift as compared to a network that doesn't use batch norm. They key insight from the paper is that batch norm actually makes the loss surface smoother, which is why it works so well (as pointed by a recent research paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Algorithms\n",
    "- [Ensemble overview](https://medium.com/coinmonks/an-intro-to-ensemble-learning-in-machine-learning-5ed8792af72d)\n",
    "- [Ensemble Tutorial](https://towardsdatascience.com/ensemble-learning-in-machine-learning-getting-started-4ed85eb38e00)\n",
    "- The goal of ensemble algorithms is to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability / robustness over a single estimator\n",
    "- There are two families of ensemble methods which are usually distinguished\n",
    "    - **Averaging methods**\n",
    "        - The driving principle is to build several estimators independently and then to average their predictions. On average, the combined estimator is usually better than any of the single base estimator because its variance is reduced\n",
    "        - <font color=blue> examples: Bagging methods, Forests of randomized trees </font>\n",
    "    - **Boosting methods**\n",
    "        - Base estimators are built sequentially and one tries to reduce the bias of the combined estimator. The motivation is to combine several weak models to produce a powerful ensemble\n",
    "        - <font color=blue> examples: AdaBoost, Gradient Tree Boosting </font>\n",
    "- **The three most popular methods for combining the predictions from different models are**\n",
    "    - **Bagging or Bootstrap Aggregation**\n",
    "        - Building multiple models (typically of the same type) from different subsamples of the training dataset.\n",
    "    - **Boosting**\n",
    "        - Building multiple models (typically of the same type) each of which learns to fix the prediction errors of a prior model in the sequence of models.\n",
    "    - **Voting**\n",
    "        - Building multiple models (typically of differing types) and simple statistics (like calculating the mean) are used to combine predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting\n",
    "- You can train your model using diverse algorithms and then ensemble them to predict the final output. Say, you use a Random Forest Classifier, SVM Classifier, Linear Regression etc\n",
    "- **Hard voting** is where a model is selected from an ensemble to make the final prediction by a simple **majority** vote for accuracy\n",
    "- **Soft voting** arrives at the best result by **averaging** out the probabilities calculated by individual algorithms. Soft Voting can only be done when all your classifiers can calculate probabilities for the outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging or Bootstrap Aggregation\n",
    "- Instead of running various models on a single dataset, you can **use a single model over various random subsets of the dataset**\n",
    "- Random sampling with replacement is called Bagging, short for bootstrap aggregating. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests\n",
    "- An ensemble of Decision trees is a Random Forest. Random Forests performs Bagging internally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost\n",
    "- In simple terms, Run a Classifier and make predictions. Run another classifier to fit the previously misclassified instances and make predictions. Repeat until all/most of the training instances are fitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting\n",
    "- Similar to AdaBoost, Gradient Boosting also works with successive predictive models added to the ensemble.\n",
    "- Instead of updating the weights of the training instances like AdaBoost, Gradient Boosting fits the new model to the residual errors.\n",
    "- Put simply, Fit a model to the given Training set. Calculate the Residual Errors which become the new training instances. A new model is trained on these and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "- XGBoost is a recent, most preferred and powerful gradient boosting method. \n",
    "- Instead of making hard Yes and No Decision at the Leaf Nodes, XGBoost assigns positive and negative values to every decision made.\n",
    "- XGBoost can work with Trees as well as Linear Models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "- [Gradient Descent Tutorial](https://blog.paperspace.com/intro-to-optimization-in-deep-learning-gradient-descent/)\n",
    "- how do we go about **escaping local minima and saddle points**, while trying to converge to a global maximum. The answer is **randomness**\n",
    "- Gradient descent is a First Order Optimization Method. It only takes the first order derivatives of the loss function into account and not the higher ones. What this basically means it has no clue about the curvature of the loss function. It can tell whether the loss is declining and how fast, but cannot differentiate between whether the curve is a plane, curving upwards or curving downwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back Propagation\n",
    "- [Back propagation step by step example](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Initialization\n",
    "- Weights initialized with zeros makes network not learn anything because of symmetry problem. as all nodes will have same weights, so they dont learn different functions/relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN Performance Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=green> New DNN Research Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transparency by Design (TbD Net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capsule Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero shot learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=orange> Interview Questions </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skills required for ML/DL Engineer Jobs\n",
    "- CNN architectures & algorithms, RNNs, NLP, Reinforcement learning\n",
    "- ML basic algorithms\n",
    "- Linear algebra, Probability, Statistics\n",
    "- DL frameworks (eg: Tensorflow, Keras)\n",
    "- Pyhon, R, jupyter notebooks\n",
    "- ARM, DSP, GPU working experience\n",
    "- Experience in building ML Applications like Image Segmentation, Object Detection, etc\n",
    "- OpenCL, CUDA GPU programming\n",
    "- Publications in CVPR/NIPS/ICML/ICLR would be an added advantage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=brown> Best Resources </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good resources list\n",
    "- [Over 200 best ML, NLP, Python Tutorials - 2018 Edition](https://medium.com/machine-learning-in-practice/over-200-of-the-best-machine-learning-nlp-and-python-tutorials-2018-edition-dd8cf53cb7dc)\n",
    "- [ML, DL Tutorials](https://github.com/ujjwalkarn/Machine-Learning-Tutorials)\n",
    "- [Machine Learning Introduction- Series](https://medium.com/machine-learning-for-humans/supervised-learning-740383a2feab)\n",
    "- [ML Cheet sheet](https://medium.com/machine-learning-in-practice/cheat-sheet-of-machine-learning-and-python-and-math-cheat-sheets-a4afe4e791b6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good practical resources\n",
    "- [Deep learning with python book](https://github.com/fchollet/deep-learning-with-python-notebooks)\n",
    "- [FastAI](https://github.com/fastai/courses)\n",
    "- [Deeplearning Specialization Coursera](https://github.com/Kulbear/deep-learning-coursera)\n",
    "- [Python machine learning book 2nd edition](https://github.com/rasbt/python-machine-learning-book-2nd-edition/tree/master/code/ch01)\n",
    "- [TesnorFlow examples](https://github.com/aymericdamien/TensorFlow-Examples)\n",
    "- [Data Science Jupyter notebook](https://github.com/donnemartin/data-science-ipython-notebooks#keras-tutorials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "143px",
    "width": "223px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "241px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
