{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# ML DL Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red> Topics to learn </font>\n",
    "- [** ML Cheat sheet **](https://medium.com/machine-learning-in-practice/cheat-sheet-of-machine-learning-and-python-and-math-cheat-sheets-a4afe4e791b6)\n",
    "- classification loss functions\n",
    "- log odd ratio\n",
    "- Precision and recall\n",
    "- knn\n",
    "- Naive bayes\n",
    "- decision tree, random forest\n",
    "- svd\n",
    "- Unsupervised learning\n",
    "    - clusstering\n",
    "        - k-means\n",
    "        - heirarchical\n",
    "    - dimentionality reduction\n",
    "        - pca\n",
    "- confusion matrix\n",
    "- batch normalization\n",
    "- instance normalization\n",
    "- generative and discriminative model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=brown> ML/DL basics introduction </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=DarkMagenta> Perceptron </font>\n",
    "- Takes several 'binary' inputs to produce a 'binary' output by comparing the weighted sum of inputs against a threshold, \n",
    "- The threshold is treated as bias and moved to left side of equation to compare weighted sum against zero\n",
    "- If a small change in a weight or bias causes only a small change in output, it is possible for a network to learn. \n",
    "- But, this doesn't happen with perceptrons sometimes as <font color=blue> **small change in weights can entirely flip the output** </font> from say 1 to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametric Vs Non-Parametric algorithms\n",
    "- Parametric\n",
    "    - Parametric methods makes an assumption about the form of the function relating X and Y\n",
    "    - Linear regression is a parametric method\n",
    "- Non-Parametric\n",
    "    - non-parametric learners do not have a model structure specified a priori. \n",
    "    - We don’t speculate about the form of the function f that we’re trying to learn before training the model, as we did previously with linear regression. \n",
    "    - Instead, the model structure is purely determined from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=DarkMagenta> Sigmoid Neuron </font>\n",
    "- Sigmoid neurons are similar to perceptrons (shape is a smoothed out version of a step function), <br> but modified so that **small changes in their weights and bias cause only a small change in their output**\n",
    "- instead of being just 0 or 1, these inputs can also take on any values between 0 and 1\n",
    "- output is not 0 or 1. Instead, it's σ(w⋅x+b), where σ is called the sigmoid function\n",
    "- Somewhat confusingly, and for historical reasons, such multiple layer networks are sometimes called multilayer perceptrons or MLPs, despite being made up of sigmoid neurons, not perceptrons. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient decent\n",
    "- To quantify how well we're achieving this goal we define a cost function\n",
    "- to find a set of weights and biases which make the cost as small as possible. We'll do that using an algorithm known as gradient descent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting\n",
    "- One of the problems that occur during neural network training is called overfitting. \n",
    "- The error on the training set is driven to a very small value, but when new data is presented to the network the error is large. The network has <font color=blue> ** memorized the training examples, but it has not learned to generalize to new situations ** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to avoid overfitting\n",
    "- Go for simpler models over more complicated models. Generally, the **fewer parameters** that you have to tune the better. \n",
    "- Use **more data** to train the model. \n",
    "- Some sort of <font color= blue> **regularization** </font> can help penalize certain sources of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanishing Gradients\n",
    "- if a change in the parameter's value causes very small change in the network's output - the network just can't learn the parameter effectively, which is a problem.\n",
    "-  For example, **sigmoid maps the real number line onto a \"small\" range** of [0, 1]. As a result, there are large regions of the input space which are mapped to an extremely small range. In these regions of the input space, even a large change in the input will produce a small change in the output - hence the gradient is small.\n",
    "- This **becomes much worse when we stack multiple layers** of such non-linearities on top of each other. <br> For instance, first layer will map a large input region to a smaller output region, which will be mapped to an even smaller region by the second layer, which will be mapped to an even smaller region by the third layer and so on. ** As a result, even a large change in the parameters of the first layer doesn't change the output much **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to avoid vanishing gradients\n",
    "- We can avoid this problem by using activation functions which don't have this property of 'squashing' the input space into a small region. \n",
    "- A popular <font color=blue> **choice is Rectified Linear Unit** </font> which maps x to max(0,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation\n",
    "- **Cross validation is a method for estimating the prediction accuracy of a model.**\n",
    "- One way to evaluate a model is to see how well it predicts the data used to fit the model. But this is too optimistic -- a model tailored to a particular data set will make better predictions on that data set than on new data. \n",
    "- Another way is to hold out some data and fit the model using the rest. Then you can test your accuracy on the holdout data.  But the held out data is \"wasted\" from the point of view of building the model. If you have huge amounts of data, so holding some data out won't make the model much worse\n",
    "- Cross validation does something like this but tries to <font color=blue> **make more efficient use of the data** </font>: you divide the data into (say) 10 equal parts. Then **successively hold out each part and fit the model using the rest**. This gives you 10 estimates of prediction accuracy which can be combined into an overall measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of data\n",
    "- ** Categorical**: Categorical variables take on values that are names or labels. The colour of a ball (e.g., red, green, blue) or the breed of a dog (e.g., collie, shepherd, terrier) would be examples of categorical variables.\n",
    "- ** Quantitative **: Quantitative variables are numerical. They represent a measurable quantity. For example, when we speak of the population of a city, we are talking about the number of people in the city — a measurable attribute of the city. Therefore population would be a quantitative variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification and Regression\n",
    "- ** So in very simple terms, classification is about predicting a label and regression is about predicting a quantity **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=brown> Classification Algorithms </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM)\n",
    "- [SVM Overview](https://towardsdatascience.com/support-vector-machines-a-brief-overview-37e018ae310f)\n",
    "- Support vector machines attempt to pass a <font color=blue> linearly separable hyperplane through a dataset in order to classify the data into two groups </font>\n",
    "- This hyperplane is a linear separator for any dimension; it could be a line (2D), plane (3D), and hyperplane (4D+)\n",
    "- the <font color=blue> best hyperplane is the one that maximizes the margin </font>. The margin is the distance between the hyperplane and a few close points. These <font color=blue> close points are the support vectors because they control the hyperplane. </font>\n",
    "- The classes have to be linearly separable to be classified using SVM, a variant of SVM is proposed to classify the data's which are not perfectly separable, it is known as a <font color=blue> Soft Margin Classifier or a Support Vector Classifier </font>, which allows slight mis-classification. SVM classifier contains a tuning parameter in order to control how much misclassification it will allow\n",
    "- ** Kernel Trick **:\n",
    "    - The non-linear lower feature space from lower dimension is transformed to higher dimension to classify non-linear, which is known as kernel trick\n",
    "    - these kernels transform our data in order to pass a linear hyperplane and thus classify our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=brown> Regression Algorithms </font>\n",
    "- **Regression is a statistical way to establish a relationship between a dependent variable and a set of independent variable(s)**\n",
    "- Regression is concerned with modeling the relationship between variables that is iteratively refined using a measure of error in the predictions made by the model.\n",
    "- Regression methods are a workhorse of statistics and have been co-opted into statistical machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression\n",
    "- While doing linear regression our objective is to **fit a line through the distribution which is nearest to most of the points**. Hence reducing the distance (error term) of data points from the fitted line. \n",
    "- It is conventional to use squares, as Regression line minimizes the sum of “Square of Residuals”. \n",
    "- That’s why the method of **Linear Regression is <font color=blue> known as “Ordinary Least Square (OLS)”** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "- The idea of <font color=blue> ** Logistic Regression is to find a relationship between features and probability of particular outcome.** </font>\n",
    "-  Logistic regression works largely the same way linear regression works: it multiplies each input by a coefficient, sums them up, and adds a constant. ** <font color=blue> In logistic regression, however, the output is actually the log of the odds ratio. </font> **\n",
    "- This type of a problem is referred to as ** Binomial Logistic Regression **, where the response variable has two values 0 and 1 or pass and fail or true and false. ** Multinomial Logistic Regression deals ** with situations where the response variable can have three or more possible values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=brown> Loss Functions </font>\n",
    "- All the algorithms in machine learning rely on minimizing or maximizing a function, which we call “**objective function**”. The group of functions that are minimized are called “loss functions”. \n",
    "- A loss function is a measure of how good a prediction model does in terms of being able to predict the expected outcome. \n",
    "- A most commonly used method of finding the minimum point of function is “gradient descent”.\n",
    "- Loss functions can be broadly categorized into 2 types: <font color=blue> **Classification loss and Regression Loss ** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=brown> Classification Loss Functions </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=brown> Regression Loss Functions </font>\n",
    "- [** 5 regression loss funtioncs **](https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 Loss, Mean Square Error (MSE), Quadratic loss\n",
    "- Mean Square Error (MSE) is the most commonly used regression loss function. MSE is the sum of squared distances between our target variable and predicted values.\n",
    "\n",
    "<h4 align=\"center\"> $ MSE = \\sum\\limits_{i=1}^n  {(y_i - y_i^p)}^2 $ </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 Loss, Mean Absolute Error (MAE)\n",
    "- MAE is the sum of absolute differences between our target and predicted variables. \n",
    "- So it ** measures the average magnitude of errors ** in a set of predictions, without considering their directions. \n",
    "- If we consider directions also, that would be called ** Mean Bias Error (MBE) **\n",
    "\n",
    "<h4 align=\"center\"> $ MAE = \\sum\\limits_{i=1}^n  {|y_i - y_i^p|} $ </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 vs L2 Loss\n",
    "- [** L1 vs L2 Comparison**](http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/)\n",
    "- <font color=blue> **L1 loss is more robust to outliers than L2 ** </font>,\n",
    "    - Since MSE squares the error (y — y_predicted = e), the value of error (e) increases a lot if e > 1. \n",
    "    - If we have an outlier in our data, the value of e will be high and e² will be >> |e|. \n",
    "    - This will make the model with **MSE loss give more weight to outliers ** than a model with MAE loss.\n",
    "    - MAE loss is useful if the training data is corrupted with outliers\n",
    "- <font color=blue> **L1 loss derivative is not continuous, hence inefficient to find solution, i.e. unstable ** </font>,\n",
    "    - which can lead to missing minima\n",
    "    - As L2 derivative is continuous, it gives more stable solution, however it not robust in case of outliers\n",
    "- <font color=blue> ** Issue with L1 and L2 loss functions: ** </font>\n",
    "    - There can be cases where neither loss function gives desirable predictions. \n",
    "    - **For example,** if 90% of observations in our data have true target value of 150 and the remaining 10% have target value between 0–30. \n",
    "    - Then a model with MAE as loss might predict 150 for all observations, ignoring 10% of outlier cases, as it will try to go towards median value. \n",
    "    - In the same case, a model using MSE would give many predictions in the range of 0 to 30 as it will get skewed towards outliers. Both results are undesirable in many business cases.\n",
    "    - ** An easy fix would be to transform the target variables. Another way is to try a different loss function. This is the motivation behind our 3rd loss function, Huber loss. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huber Loss, Smooth Mean Absolute Error\n",
    "- Huber loss is less sensitive to outliers in data than the squared error loss. It’s also **differentiable at 0**. \n",
    "- It’s basically absolute error, which becomes quadratic when error is small.\n",
    "- problem with Huber loss is that we might need to train hyperparameter delta which is an iterative process\n",
    "- ** it’s twice differentiable everywhere **\n",
    "- Many ML model implementations like XGBoost use Newton’s method to find the optimum, which is why the second derivative (Hessian) is needed. For ML frameworks like XGBoost, **twice differentiable functions are more favorable**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log-CosH Loss\n",
    "- Log-cosh is another function used in regression tasks that’s **smoother than L2**. \n",
    "- Log-cosh is the logarithm of the hyperbolic cosine of the prediction error.\n",
    "- 'logcosh' works mostly like the mean squared error, but will ** not be so strongly affected by the occasional wildly incorrect prediction **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantile Loss\n",
    "- Quantile loss functions turns out to be useful when we are interested in predicting an interval instead of only point predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=brown> Regularization Algorithms </font>\n",
    "- To avoid over optimizing/over-fitting the training set to use early termination as soon as the learning stops, other method is ** to use regularization **\n",
    "- An extension made to another method (typically regression methods) that <font color=blue> ** penalizes models based on their complexity ** </font>, favoring simpler models that are also better at generalizing.\n",
    "- other regularization technique is ** dropout **\n",
    "- [Regularization techniques](https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-regularization-techniques/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=DarkMagenta> L1 (Lasso) and L2 (Ridge) as Reguralization </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop-out\n",
    "- remove few connections randomly to force the network to learn redundant representation of input, so that it doesn't overfit and depend on any particular parameter, so that all learns independently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=brown> Bias Variance Trade-Off </font>\n",
    "- [Bias Variance Trade-off Tutorial](https://www.listendata.com/2017/02/bias-variance-tradeoff.html)\n",
    "- [Bias Variance Trade-off Infograph](https://elitedatascience.com/bias-variance-tradeoff)\n",
    "- ** Bias **\n",
    "    - Bias is a measure of the prediction accuracy on training data\n",
    "    - <font color=blue> High bias means low prediction accuracy </font>, which means model may be too simple not able learn from training data known as underfitting\n",
    "    - For example, a linear regression model would have high bias when trying to model a non-linear relationship\n",
    "    - High Bias Techniques\n",
    "        - Linear Regression, Linear Discriminant Analysis and Logistic Regression\n",
    "    - Low Bias Techniques\n",
    "        - Decision Trees,  K-nearest neighbours and Gradient Boosting\n",
    "    - <font color=blue> Parametric algorithms which assume something about the distribution of the data points </font> suffer from High Bias. Whereas non-parametric algorithms which does not assume anything special about distribution have low bias.\n",
    "- ** Variance **\n",
    "    - Variance is a measure of the generalization of the network\n",
    "    - <font color=blue> High variance means less generalization </font>, complex models that fits well on training data but they cannot generalise the pattern well which results to overfitting\n",
    "    - Low Variance Techniques\n",
    "        - Linear Regression, Linear Discriminant Analysis and Logistic Regression\n",
    "    - High Variance Techniques\n",
    "        - Decision Trees,  K-nearest neighbours and SVM\n",
    "- ** Bias Variance Trade-off **\n",
    "    - It means there is a trade-off between predictive accuracy and generalization of pattern outside training data. Increasing the accuracy of the model will lead to less generalization of pattern outside training data. Increasing the bias will decrease the variance. Increasing the variance will decrease the bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"400\"\n",
       "            height=\"300\"\n",
       "            src=\"https://www.youtube.com/embed/o4QmoNfW3bI\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "\n",
       "        <iframe\n",
       "            width=\"400\"\n",
       "            height=\"300\"\n",
       "            src=\"https://www.youtube.com/embed/o4QmoNfW3bI\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('o4QmoNfW3bI')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binomial Theorem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"400\"\n",
       "            height=\"300\"\n",
       "            src=\"https://www.youtube.com/embed/6nOhUqA29EI\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "\n",
       "        <iframe\n",
       "            width=\"400\"\n",
       "            height=\"300\"\n",
       "            src=\"https://www.youtube.com/embed/6nOhUqA29EI\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YouTubeVideo('6nOhUqA29EI')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training hyper parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=green> New DNN Research Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capsule Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transparency by Design (TbD Net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=brown> Interview Questions </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skills required for ML/DL Engineer Jobs\n",
    "- \n",
    "- Strong expertise in DL frameworks (eg: Tensorflow, Keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=brown> Best Resources </font>\n",
    "- [Over 200 best ML, NLP, Python Tutorials - 2018 Edition](https://medium.com/machine-learning-in-practice/over-200-of-the-best-machine-learning-nlp-and-python-tutorials-2018-edition-dd8cf53cb7dc)\n",
    "- [ML, DL Tutorials](https://github.com/ujjwalkarn/Machine-Learning-Tutorials)\n",
    "- [Machine Learning Introduction- Series](https://medium.com/machine-learning-for-humans/supervised-learning-740383a2feab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
