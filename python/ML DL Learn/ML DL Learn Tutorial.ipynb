{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#ML-DL-Notes\" data-toc-modified-id=\"ML-DL-Notes-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>ML DL Notes</a></span></li><li><span><a href=\"#-Topics-to-learn-\" data-toc-modified-id=\"-Topics-to-learn--2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span><font color=\"red\"> Topics to learn </font></a></span></li><li><span><a href=\"#-ML/DL-basics-introduction-\" data-toc-modified-id=\"-ML/DL-basics-introduction--3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span><font color=\"brown\"> ML/DL basics introduction </font></a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#-Perceptron-\" data-toc-modified-id=\"-Perceptron--3.0.1\"><span class=\"toc-item-num\">3.0.1&nbsp;&nbsp;</span><font color=\"DarkMagenta\"> Perceptron </font></a></span></li><li><span><a href=\"#Parametric-Vs-Non-Parametric-algorithms\" data-toc-modified-id=\"Parametric-Vs-Non-Parametric-algorithms-3.0.2\"><span class=\"toc-item-num\">3.0.2&nbsp;&nbsp;</span>Parametric Vs Non-Parametric algorithms</a></span></li><li><span><a href=\"#-Sigmoid-Neuron-\" data-toc-modified-id=\"-Sigmoid-Neuron--3.0.3\"><span class=\"toc-item-num\">3.0.3&nbsp;&nbsp;</span><font color=\"DarkMagenta\"> Sigmoid Neuron </font></a></span></li><li><span><a href=\"#Gradient-decent\" data-toc-modified-id=\"Gradient-decent-3.0.4\"><span class=\"toc-item-num\">3.0.4&nbsp;&nbsp;</span>Gradient decent</a></span></li><li><span><a href=\"#Backpropagation\" data-toc-modified-id=\"Backpropagation-3.0.5\"><span class=\"toc-item-num\">3.0.5&nbsp;&nbsp;</span>Backpropagation</a></span></li><li><span><a href=\"#Overfitting\" data-toc-modified-id=\"Overfitting-3.0.6\"><span class=\"toc-item-num\">3.0.6&nbsp;&nbsp;</span>Overfitting</a></span></li><li><span><a href=\"#How-to-avoid-overfitting\" data-toc-modified-id=\"How-to-avoid-overfitting-3.0.7\"><span class=\"toc-item-num\">3.0.7&nbsp;&nbsp;</span>How to avoid overfitting</a></span></li><li><span><a href=\"#Vanishing-Gradients\" data-toc-modified-id=\"Vanishing-Gradients-3.0.8\"><span class=\"toc-item-num\">3.0.8&nbsp;&nbsp;</span>Vanishing Gradients</a></span></li><li><span><a href=\"#How-to-avoid-vanishing-gradients\" data-toc-modified-id=\"How-to-avoid-vanishing-gradients-3.0.9\"><span class=\"toc-item-num\">3.0.9&nbsp;&nbsp;</span>How to avoid vanishing gradients</a></span></li><li><span><a href=\"#Cross-validation\" data-toc-modified-id=\"Cross-validation-3.0.10\"><span class=\"toc-item-num\">3.0.10&nbsp;&nbsp;</span>Cross validation</a></span></li><li><span><a href=\"#Types-of-data\" data-toc-modified-id=\"Types-of-data-3.0.11\"><span class=\"toc-item-num\">3.0.11&nbsp;&nbsp;</span>Types of data</a></span></li><li><span><a href=\"#Classification-and-Regression\" data-toc-modified-id=\"Classification-and-Regression-3.0.12\"><span class=\"toc-item-num\">3.0.12&nbsp;&nbsp;</span>Classification and Regression</a></span></li></ul></li></ul></li><li><span><a href=\"#-Supervised-Learning-\" data-toc-modified-id=\"-Supervised-Learning--4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span><font color=\"brown\"> Supervised Learning </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#-Parametric-and-Classification-Algorithms-\" data-toc-modified-id=\"-Parametric-and-Classification-Algorithms--4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span><font color=\"brown\"> Parametric and Classification Algorithms </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#Support-Vector-Machine-(SVM)\" data-toc-modified-id=\"Support-Vector-Machine-(SVM)-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>Support Vector Machine (SVM)</a></span></li><li><span><a href=\"#Logistic-regression\" data-toc-modified-id=\"Logistic-regression-4.1.2\"><span class=\"toc-item-num\">4.1.2&nbsp;&nbsp;</span>Logistic regression</a></span></li></ul></li><li><span><a href=\"#-Parametric-and-Regression-Algorithms-\" data-toc-modified-id=\"-Parametric-and-Regression-Algorithms--4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span><font color=\"brown\"> Parametric and Regression Algorithms </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#Linear-regression\" data-toc-modified-id=\"Linear-regression-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Linear regression</a></span></li></ul></li><li><span><a href=\"#Non-Parametric-Algorithms\" data-toc-modified-id=\"Non-Parametric-Algorithms-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Non-Parametric Algorithms</a></span><ul class=\"toc-item\"><li><span><a href=\"#K-Nearest-Neighbor-(KNN)\" data-toc-modified-id=\"K-Nearest-Neighbor-(KNN)-4.3.1\"><span class=\"toc-item-num\">4.3.1&nbsp;&nbsp;</span>K-Nearest Neighbor (KNN)</a></span></li><li><span><a href=\"#Decision-Tree\" data-toc-modified-id=\"Decision-Tree-4.3.2\"><span class=\"toc-item-num\">4.3.2&nbsp;&nbsp;</span>Decision Tree</a></span></li><li><span><a href=\"#Random-Forest:--an-ensemble-of-decision-trees\" data-toc-modified-id=\"Random-Forest:--an-ensemble-of-decision-trees-4.3.3\"><span class=\"toc-item-num\">4.3.3&nbsp;&nbsp;</span>Random Forest:  an ensemble of decision trees</a></span></li></ul></li></ul></li><li><span><a href=\"#-Unsupervised-Learning-\" data-toc-modified-id=\"-Unsupervised-Learning--5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span><font color=\"brown\"> Unsupervised Learning </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#Clusttering\" data-toc-modified-id=\"Clusttering-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Clusttering</a></span><ul class=\"toc-item\"><li><span><a href=\"#K-means-clusttering\" data-toc-modified-id=\"K-means-clusttering-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>K-means clusttering</a></span></li><li><span><a href=\"#Expectation–Maximization-(EM)-Clustering-using-Gaussian-Mixture-Models-(GMM)\" data-toc-modified-id=\"Expectation–Maximization-(EM)-Clustering-using-Gaussian-Mixture-Models-(GMM)-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;</span>Expectation–Maximization (EM) Clustering using Gaussian Mixture Models (GMM)</a></span></li><li><span><a href=\"#Hierarchical-Clusttering\" data-toc-modified-id=\"Hierarchical-Clusttering-5.1.3\"><span class=\"toc-item-num\">5.1.3&nbsp;&nbsp;</span>Hierarchical Clusttering</a></span></li></ul></li><li><span><a href=\"#Dimensionality-Reduction\" data-toc-modified-id=\"Dimensionality-Reduction-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Dimensionality Reduction</a></span><ul class=\"toc-item\"><li><span><a href=\"#Principal-Component-Analysis-(PCA)\" data-toc-modified-id=\"Principal-Component-Analysis-(PCA)-5.2.1\"><span class=\"toc-item-num\">5.2.1&nbsp;&nbsp;</span>Principal Component Analysis (PCA)</a></span></li><li><span><a href=\"#Singular-Value-Decomposition-(SVD)\" data-toc-modified-id=\"Singular-Value-Decomposition-(SVD)-5.2.2\"><span class=\"toc-item-num\">5.2.2&nbsp;&nbsp;</span>Singular Value Decomposition (SVD)</a></span></li></ul></li></ul></li><li><span><a href=\"#-Loss-Functions-\" data-toc-modified-id=\"-Loss-Functions--6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span><font color=\"brown\"> Loss Functions </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#-Classification-Loss-Functions-\" data-toc-modified-id=\"-Classification-Loss-Functions--6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span><font color=\"brown\"> Classification Loss Functions </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#Square-loss\" data-toc-modified-id=\"Square-loss-6.1.1\"><span class=\"toc-item-num\">6.1.1&nbsp;&nbsp;</span>Square loss</a></span></li><li><span><a href=\"#Hinge-loss\" data-toc-modified-id=\"Hinge-loss-6.1.2\"><span class=\"toc-item-num\">6.1.2&nbsp;&nbsp;</span>Hinge loss</a></span></li><li><span><a href=\"#Logistic-loss\" data-toc-modified-id=\"Logistic-loss-6.1.3\"><span class=\"toc-item-num\">6.1.3&nbsp;&nbsp;</span>Logistic loss</a></span></li><li><span><a href=\"#Cross-entropy-loss\" data-toc-modified-id=\"Cross-entropy-loss-6.1.4\"><span class=\"toc-item-num\">6.1.4&nbsp;&nbsp;</span>Cross entropy loss</a></span></li></ul></li><li><span><a href=\"#-Regression-Loss-Functions-\" data-toc-modified-id=\"-Regression-Loss-Functions--6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span><font color=\"brown\"> Regression Loss Functions </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#L2-Loss,-Mean-Square-Error-(MSE),-Quadratic-loss\" data-toc-modified-id=\"L2-Loss,-Mean-Square-Error-(MSE),-Quadratic-loss-6.2.1\"><span class=\"toc-item-num\">6.2.1&nbsp;&nbsp;</span>L2 Loss, Mean Square Error (MSE), Quadratic loss</a></span><ul class=\"toc-item\"><li><span><a href=\"#-$-MSE-=-\\sum\\limits_{i=1}^n--{(y_i---y_i^p)}^2-$-\" data-toc-modified-id=\"-$-MSE-=-\\sum\\limits_{i=1}^n--{(y_i---y_i^p)}^2-$--6.2.1.1\"><span class=\"toc-item-num\">6.2.1.1&nbsp;&nbsp;</span> $ MSE = \\sum\\limits_{i=1}^n  {(y_i - y_i^p)}^2 $ </a></span></li></ul></li><li><span><a href=\"#L1-Loss,-Mean-Absolute-Error-(MAE)\" data-toc-modified-id=\"L1-Loss,-Mean-Absolute-Error-(MAE)-6.2.2\"><span class=\"toc-item-num\">6.2.2&nbsp;&nbsp;</span>L1 Loss, Mean Absolute Error (MAE)</a></span><ul class=\"toc-item\"><li><span><a href=\"#-$-MAE-=-\\sum\\limits_{i=1}^n--{|y_i---y_i^p|}-$-\" data-toc-modified-id=\"-$-MAE-=-\\sum\\limits_{i=1}^n--{|y_i---y_i^p|}-$--6.2.2.1\"><span class=\"toc-item-num\">6.2.2.1&nbsp;&nbsp;</span> $ MAE = \\sum\\limits_{i=1}^n  {|y_i - y_i^p|} $ </a></span></li></ul></li><li><span><a href=\"#L1-vs-L2-Loss\" data-toc-modified-id=\"L1-vs-L2-Loss-6.2.3\"><span class=\"toc-item-num\">6.2.3&nbsp;&nbsp;</span>L1 vs L2 Loss</a></span></li><li><span><a href=\"#Huber-Loss,-Smooth-Mean-Absolute-Error\" data-toc-modified-id=\"Huber-Loss,-Smooth-Mean-Absolute-Error-6.2.4\"><span class=\"toc-item-num\">6.2.4&nbsp;&nbsp;</span>Huber Loss, Smooth Mean Absolute Error</a></span></li><li><span><a href=\"#Log-CosH-Loss\" data-toc-modified-id=\"Log-CosH-Loss-6.2.5\"><span class=\"toc-item-num\">6.2.5&nbsp;&nbsp;</span>Log-CosH Loss</a></span></li><li><span><a href=\"#Quantile-Loss\" data-toc-modified-id=\"Quantile-Loss-6.2.6\"><span class=\"toc-item-num\">6.2.6&nbsp;&nbsp;</span>Quantile Loss</a></span></li></ul></li></ul></li><li><span><a href=\"#-Regularization-Algorithms-\" data-toc-modified-id=\"-Regularization-Algorithms--7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span><font color=\"brown\"> Regularization Algorithms </font></a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#-L1-(Lasso)-and-L2-(Ridge)-as-Reguralization-\" data-toc-modified-id=\"-L1-(Lasso)-and-L2-(Ridge)-as-Reguralization--7.0.1\"><span class=\"toc-item-num\">7.0.1&nbsp;&nbsp;</span><font color=\"DarkMagenta\"> L1 (Lasso) and L2 (Ridge) as Reguralization </font></a></span></li><li><span><a href=\"#Drop-out\" data-toc-modified-id=\"Drop-out-7.0.2\"><span class=\"toc-item-num\">7.0.2&nbsp;&nbsp;</span>Drop-out</a></span></li></ul></li></ul></li><li><span><a href=\"#-Bias-Variance-Trade-Off-\" data-toc-modified-id=\"-Bias-Variance-Trade-Off--8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span><font color=\"brown\"> Bias Variance Trade-Off </font></a></span></li><li><span><a href=\"#-CNN-Layer-Theory-\" data-toc-modified-id=\"-CNN-Layer-Theory--9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span><font color=\"brown\"> CNN Layer Theory </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#Rectified-Linear-Unit-(ReLU)\" data-toc-modified-id=\"Rectified-Linear-Unit-(ReLU)-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>Rectified Linear Unit (ReLU)</a></span></li></ul></li><li><span><a href=\"#Probability-and-Statistics\" data-toc-modified-id=\"Probability-and-Statistics-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Probability and Statistics</a></span><ul class=\"toc-item\"><li><span><a href=\"#Binomial-Theorem\" data-toc-modified-id=\"Binomial-Theorem-10.1\"><span class=\"toc-item-num\">10.1&nbsp;&nbsp;</span>Binomial Theorem</a></span></li></ul></li><li><span><a href=\"#Data-Pre-Processing-or-Feature-Scaling\" data-toc-modified-id=\"Data-Pre-Processing-or-Feature-Scaling-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Data Pre-Processing or Feature Scaling</a></span><ul class=\"toc-item\"><li><span><a href=\"#Standard-deviation\" data-toc-modified-id=\"Standard-deviation-11.1\"><span class=\"toc-item-num\">11.1&nbsp;&nbsp;</span>Standard deviation</a></span></li><li><span><a href=\"#Mean-Normalization\" data-toc-modified-id=\"Mean-Normalization-11.2\"><span class=\"toc-item-num\">11.2&nbsp;&nbsp;</span>Mean Normalization</a></span></li><li><span><a href=\"#Normalization-and-Standardization\" data-toc-modified-id=\"Normalization-and-Standardization-11.3\"><span class=\"toc-item-num\">11.3&nbsp;&nbsp;</span>Normalization and Standardization</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#-$-norm(x)-=-\\frac{x---min(x)}{max(x)---min(x)}-$-\" data-toc-modified-id=\"-$-norm(x)-=-\\frac{x---min(x)}{max(x)---min(x)}-$--11.3.0.1\"><span class=\"toc-item-num\">11.3.0.1&nbsp;&nbsp;</span> $ norm(x) = \\frac{x - min(x)}{max(x) - min(x)} $ </a></span></li><li><span><a href=\"#-$-std(x)-=-\\frac{x---\\mu}{\\sigma}-$-\" data-toc-modified-id=\"-$-std(x)-=-\\frac{x---\\mu}{\\sigma}-$--11.3.0.2\"><span class=\"toc-item-num\">11.3.0.2&nbsp;&nbsp;</span> $ std(x) = \\frac{x - \\mu}{\\sigma} $ </a></span></li></ul></li></ul></li><li><span><a href=\"#Covariance-and-Correlation\" data-toc-modified-id=\"Covariance-and-Correlation-11.4\"><span class=\"toc-item-num\">11.4&nbsp;&nbsp;</span>Covariance and Correlation</a></span></li><li><span><a href=\"#Whitening\" data-toc-modified-id=\"Whitening-11.5\"><span class=\"toc-item-num\">11.5&nbsp;&nbsp;</span>Whitening</a></span></li><li><span><a href=\"#Batch-Normalization\" data-toc-modified-id=\"Batch-Normalization-11.6\"><span class=\"toc-item-num\">11.6&nbsp;&nbsp;</span>Batch Normalization</a></span></li></ul></li><li><span><a href=\"#Ensemble-Algorithms\" data-toc-modified-id=\"Ensemble-Algorithms-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;</span>Ensemble Algorithms</a></span></li><li><span><a href=\"#Gradient-Descent\" data-toc-modified-id=\"Gradient-Descent-13\"><span class=\"toc-item-num\">13&nbsp;&nbsp;</span>Gradient Descent</a></span></li><li><span><a href=\"#Back-Propagation\" data-toc-modified-id=\"Back-Propagation-14\"><span class=\"toc-item-num\">14&nbsp;&nbsp;</span>Back Propagation</a></span></li><li><span><a href=\"#Deep-Learning-Training\" data-toc-modified-id=\"Deep-Learning-Training-15\"><span class=\"toc-item-num\">15&nbsp;&nbsp;</span>Deep Learning Training</a></span></li><li><span><a href=\"#Recurrent-Neural-Networks-(RNN)\" data-toc-modified-id=\"Recurrent-Neural-Networks-(RNN)-16\"><span class=\"toc-item-num\">16&nbsp;&nbsp;</span>Recurrent Neural Networks (RNN)</a></span></li><li><span><a href=\"#Reinforcement-Learning\" data-toc-modified-id=\"Reinforcement-Learning-17\"><span class=\"toc-item-num\">17&nbsp;&nbsp;</span>Reinforcement Learning</a></span></li><li><span><a href=\"#-New-DNN-Research-Concepts\" data-toc-modified-id=\"-New-DNN-Research-Concepts-18\"><span class=\"toc-item-num\">18&nbsp;&nbsp;</span><font color=\"green\"> New DNN Research Concepts</font></a></span><ul class=\"toc-item\"><li><span><a href=\"#Transparency-by-Design-(TbD-Net)\" data-toc-modified-id=\"Transparency-by-Design-(TbD-Net)-18.1\"><span class=\"toc-item-num\">18.1&nbsp;&nbsp;</span>Transparency by Design (TbD Net)</a></span></li><li><span><a href=\"#Capsule-Net\" data-toc-modified-id=\"Capsule-Net-18.2\"><span class=\"toc-item-num\">18.2&nbsp;&nbsp;</span>Capsule Net</a></span></li><li><span><a href=\"#Zero-shot-learning\" data-toc-modified-id=\"Zero-shot-learning-18.3\"><span class=\"toc-item-num\">18.3&nbsp;&nbsp;</span>Zero shot learning</a></span></li></ul></li><li><span><a href=\"#-Interview-Questions-\" data-toc-modified-id=\"-Interview-Questions--19\"><span class=\"toc-item-num\">19&nbsp;&nbsp;</span><font color=\"orange\"> Interview Questions </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#Skills-required-for-ML/DL-Engineer-Jobs\" data-toc-modified-id=\"Skills-required-for-ML/DL-Engineer-Jobs-19.1\"><span class=\"toc-item-num\">19.1&nbsp;&nbsp;</span>Skills required for ML/DL Engineer Jobs</a></span></li></ul></li><li><span><a href=\"#-Best-Resources-\" data-toc-modified-id=\"-Best-Resources--20\"><span class=\"toc-item-num\">20&nbsp;&nbsp;</span><font color=\"brown\"> Best Resources </font></a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# ML DL Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red> Topics to learn </font>\n",
    "- [** ML Cheat sheet **](https://medium.com/machine-learning-in-practice/cheat-sheet-of-machine-learning-and-python-and-math-cheat-sheets-a4afe4e791b6)\n",
    "- Unsupervised learning\n",
    "    - dimentionality reduction\n",
    "        - pca\n",
    "        - lda\n",
    "- Ensemble\n",
    "    - Bagging\n",
    "    - Binning\n",
    "    - Boosting\n",
    "        - Gradient Boost Model (GBM)\n",
    "        - XGBoost (Extreme Gradient Boost)\n",
    "- CNN layers theory\n",
    "    - Softmax\n",
    "    - Convolution types\n",
    "        - Depthwise separable convolution\n",
    "        - Shufflenet\n",
    "        - transposed convolution\n",
    "        - subpixel convolution\n",
    "- log odd ratio\n",
    "- svd\n",
    "- Multiclass SVM\n",
    "- Kernel PCA\n",
    "- Weight normalization\n",
    "- instance normalization, layer normalization, group normalization\n",
    "- Regularization\n",
    "    - Elastic net\n",
    "- Gaussian Mixture Model (GMM)\n",
    "- Hidden Markov Models (HMM)\n",
    "- Probability\n",
    "    - Joint, marginal, conditional\n",
    "    - Bayes theorem, Naive bayes\n",
    "    - Random variables\n",
    "    - Benoulli, binomial distribution\n",
    "    - standard normal distribution\n",
    "    - Maximum likelihood estimation\n",
    "    - Prior and posterior\n",
    "    - MAP\n",
    "    - sampling methods\n",
    "- Network architecture\n",
    "- Gradient descent, back propagation\n",
    "- confusion matrix\n",
    "- Precision and recall\n",
    "- classification loss functions\n",
    "    - cross entropy loss function\n",
    "- generative and discriminative model\n",
    "- VAE\n",
    "- GANs\n",
    "- Deep learning studio\n",
    "- Study later\n",
    "    - Clusttering techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=brown> ML/DL basics introduction </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### <font color=DarkMagenta> Perceptron </font>\n",
    "- Takes several 'binary' inputs to produce a 'binary' output by comparing the weighted sum of inputs against a threshold, \n",
    "- The threshold is treated as bias and moved to left side of equation to compare weighted sum against zero\n",
    "- If a small change in a weight or bias causes only a small change in output, it is possible for a network to learn. \n",
    "- But, this doesn't happen with perceptrons sometimes as <font color=blue>small change in weights can entirely flip the output</font> from say 1 to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametric Vs Non-Parametric algorithms\n",
    "- Parametric\n",
    "    - Parametric methods makes an assumption about the form of the function relating X and Y\n",
    "    - Linear regression is a parametric method\n",
    "- Non-Parametric\n",
    "    - non-parametric learners do not have a model structure specified a priori. \n",
    "    - We don’t speculate about the form of the function f that we are trying to learn before training the model, as we did previously with linear regression. \n",
    "    - Instead, the model structure is purely determined from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=DarkMagenta> Sigmoid Neuron </font>\n",
    "- Sigmoid neurons are similar to perceptrons (shape is a smoothed out version of a step function), <br> but modified so that **small changes in their weights and bias cause only a small change in their output**\n",
    "- instead of being just 0 or 1, these inputs can also take on any values between 0 and 1\n",
    "- output is not 0 or 1. Instead, it's σ(w⋅x+b), where σ is called the sigmoid function\n",
    "- Somewhat confusingly, and for historical reasons, such multiple layer networks are sometimes called multilayer perceptrons or MLPs, despite being made up of sigmoid neurons, not perceptrons. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient decent\n",
    "- To quantify how well we're achieving this goal we define a cost function\n",
    "- to find a set of weights and biases which make the cost as small as possible. We'll do that using an algorithm known as gradient descent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting\n",
    "- One of the problems that occur during neural network training is called overfitting. \n",
    "- The error on the training set is driven to a very small value, but when new data is presented to the network the error is large. The network has <font color=blue>memorized the training examples, but it has not learned to generalize to new situations</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to avoid overfitting\n",
    "- Go for simpler models over more complicated models. Generally, the **fewer parameters** that you have to tune the better. \n",
    "- Use **more data** to train the model. \n",
    "- Some sort of <font color= blue>regularization</font> can help penalize certain sources of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanishing Gradients\n",
    "- if a change in the parameter's value causes very small change in the network's output - the network just can't learn the parameter effectively, which is a problem.\n",
    "-  For example, **sigmoid maps the real number line onto a \"small\" range** of [0, 1]. As a result, there are large regions of the input space which are mapped to an extremely small range. In these regions of the input space, even a large change in the input will produce a small change in the output - hence the gradient is small.\n",
    "- This **becomes much worse when we stack multiple layers** of such non-linearities on top of each other. <br> For instance, first layer will map a large input region to a smaller output region, which will be mapped to an even smaller region by the second layer, which will be mapped to an even smaller region by the third layer and so on. ** As a result, even a large change in the parameters of the first layer doesn't change the output much **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to avoid vanishing gradients\n",
    "- We can avoid this problem by using activation functions which don't have this property of 'squashing' the input space into a small region. \n",
    "- A popular <font color=blue>choice is Rectified Linear Unit</font> which maps x to max(0,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation\n",
    "- **Cross validation is a method for estimating the prediction accuracy of a model.**\n",
    "- One way to evaluate a model is to see how well it predicts the data used to fit the model. But this is too optimistic -- a model tailored to a particular data set will make better predictions on that data set than on new data. \n",
    "- Another way is to hold out some data and fit the model using the rest. Then you can test your accuracy on the holdout data.  But the held out data is \"wasted\" from the point of view of building the model. If you have huge amounts of data, so holding some data out won't make the model much worse\n",
    "- Cross validation does something like this but tries to <font color=blue>make more efficient use of the data</font>: you divide the data into (say) 10 equal parts. Then **successively hold out each part and fit the model using the rest**. This gives you 10 estimates of prediction accuracy which can be combined into an overall measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of data\n",
    "- ** Categorical**: Categorical variables take on values that are names or labels. The colour of a ball (e.g., red, green, blue) or the breed of a dog (e.g., collie, shepherd, terrier) would be examples of categorical variables.\n",
    "- ** Quantitative **: Quantitative variables are numerical. They represent a measurable quantity. For example, when we speak of the population of a city, we are talking about the number of people in the city — a measurable attribute of the city. Therefore population would be a quantitative variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification and Regression\n",
    "- **So in very simple terms, classification is about predicting a label and regression is about predicting a quantity**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=brown> Supervised Learning </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=brown> Parametric and Classification Algorithms </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM)\n",
    "- [SVM Overview](https://towardsdatascience.com/support-vector-machines-a-brief-overview-37e018ae310f)\n",
    "- [SVM kernel trick example](https://medium.com/machine-learning-for-humans/supervised-learning-2-5c1c23f3560d)\n",
    "- SVM is also a <font color=blue> binary classifier </font> (classifies 2 classes) like logistic regression\n",
    "- Support vector machines attempt to pass a <font color=blue> linearly separable hyperplane through a dataset in order to classify the data into two groups </font>\n",
    "- This hyperplane is a linear separator for any dimension; it could be a line (2D), plane (3D), and hyperplane (4D+)\n",
    "- the <font color=blue> best hyperplane is the one that maximizes the margin </font>. The margin is the distance between the hyperplane and a few close points. These <font color=blue> close points are the support vectors because they control the hyperplane. </font>\n",
    "- The classes have to be linearly separable to be classified using SVM, a variant of SVM is proposed to classify the data's which are not perfectly separable, it is known as a <font color=blue> Soft Margin Classifier or a Support Vector Classifier </font>, which allows slight mis-classification. SVM classifier contains a tuning parameter in order to control how much misclassification it will allow\n",
    "- **Kernel Trick**:\n",
    "    - The non-linear lower feature space from lower dimension is transformed to higher dimension to classify non-linear, which is known as kernel trick\n",
    "    - these kernels transform our data in order to pass a linear hyperplane and thus classify our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "- The idea of <font color=blue>Logistic Regression is to find a relationship between features and probability of particular outcome.</font>\n",
    "-  Logistic regression works largely the same way linear regression works: it multiplies each input by a coefficient, sums them up, and adds a constant. <font color=blue> In logistic regression, however, the output is actually the log of the odds ratio. </font>\n",
    "- This type of a problem is referred to as **Binomial Logistic Regression**, where the response variable has two values 0 and 1 or pass and fail or true and false. **Multinomial Logistic Regression deals** with situations where the response variable can have three or more possible values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=brown> Parametric and Regression Algorithms </font>\n",
    "- **Regression is a statistical way to establish a relationship between a dependent variable and a set of independent variable(s)**\n",
    "- Regression is concerned with modeling the relationship between variables that is iteratively refined using a measure of error in the predictions made by the model.\n",
    "- Regression methods are a workhorse of statistics and have been co-opted into statistical machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression\n",
    "- While doing linear regression our objective is to **fit a line through the distribution which is nearest to most of the points**. Hence reducing the distance (error term) of data points from the fitted line. \n",
    "- It is conventional to use squares, as Regression line minimizes the sum of “Square of Residuals”. \n",
    "- That’s why the method of Linear Regression is <font color=blue> known as “Ordinary Least Square (OLS)”</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Parametric Algorithms\n",
    "- [Non-Parametric Supervised Learning Tutorial](https://medium.com/machine-learning-for-humans/supervised-learning-3-b1551b9c4930)\n",
    "- non-parametric learners do not have a model structure specified a priori. We don’t speculate about the form of the function f that we’re trying to learn before training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbor (KNN)\n",
    "- You look at the k closest data points and take the average of their values if variables are continuous (like housing prices), or the mode if they’re categorical (like cat vs. dog)\n",
    "- **Choosing k:** tuning hyperparameters with cross-validation\n",
    "    - To decide which value of k to use, you can **test different k-NN models** using different values of k with cross-validation\n",
    "    - Pick whichever yields the lowest error, on average, across all iterations\n",
    "- Higher values of k help address overfitting, but if the value of k is too high your model will be very biased and inflexible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "- Making a good decision tree is like playing a game of “20 questions”\n",
    "- There are ways to quantify information gain so that you can essentially evaluate every possible split of the training data and maximize information gain for every split\n",
    "- **Choosing splits in a decision tree**\n",
    "    - **Entropy is the amount of disorder in a set** (measured by Gini index or cross-entropy)\n",
    "    - If the values are really mixed, there’s lots of entropy; if you can cleanly split values, there’s no entropy.\n",
    "    - **For every split at a parent node, you want the child nodes to be as pure as possible (minimize entropy.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest:  an ensemble of decision trees\n",
    "- A model comprised of many models is called an **ensemble model**, and this is usually a winning strategy.\n",
    "- A single decision tree can make a lot of wrong calls because it has very black-and-white judgments. \n",
    "- A random forest is a meta-estimator that aggregates many decision trees, with some helpful modifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=brown> Unsupervised Learning </font>\n",
    "- [Unsupervised Tutorial](https://medium.com/machine-learning-for-humans/unsupervised-learning-f45587588294)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clusttering\n",
    "- [Top5 Clustterings](https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68)\n",
    "- The goal of clustering is to create groups of data points such that points in different clusters are dissimilar while points within a cluster are similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means clusttering\n",
    "- [K-means tutorial](https://towardsdatascience.com/clustering-using-k-means-algorithm-81da00f156f6)\n",
    "- k-means clustering works on unlabeled data to cluster our data points into k groups. \n",
    "- A larger k creates smaller groups with more granularity, a lower k means larger groups and less granularity.\n",
    "- The output of the algorithm would be a set of “labels” assigning each data point to one of the k groups.\n",
    "- In k-means clustering, the way these **groups are defined is by creating a centroid** for each group. The centroids are like the heart of the cluster, they “capture” the points closest to them and add them to the cluster.\n",
    "- **K-means algorithm steps**\n",
    "    1. **Define the k centroids.**\n",
    "        - Initialize these at random (there are also fancier algorithms for initializing the centroids that end up converging more effectively).\n",
    "    2. **Find the closest centroid & update cluster assignments.**\n",
    "        - Assign each data point to one of the k clusters. Each data point is assigned to the nearest centroid’s cluster. Here, the measure of “nearness” is a hyperparameter — often **Euclidean distance**.\n",
    "        - If we’re using the Euclidean distance between data points and every centroid, a straight line is drawn between two centroids, then a perpendicular bisector (boundary line) divides this line into two clusters\n",
    "    3. **Move the centroids to the center of their clusters.**\n",
    "        - The new position of each centroid is calculated as the average position of all the points in its cluster.\n",
    "    - Keep repeating steps 2 and 3 until the centroid stop moving a lot at each iteration (i.e., until the algorithm converges)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expectation–Maximization (EM) Clustering using Gaussian Mixture Models (GMM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Clusttering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "- Dimensionality reduction looks a lot like compression. This is about trying to reduce the complexity of the data while keeping as much of the relevant structure as possible.\n",
    "- reducing the dimension of the feature space is called “dimensionality reduction.” There are many ways to achieve dimensionality reduction, but most of these techniques fall into one of two classes:\n",
    "    - Feature Elimination\n",
    "    - Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)\n",
    "- [PCA Tutorial with example](https://georgemdallas.wordpress.com/2013/10/30/principal-component-analysis-4-dummies-eigenvectors-eigenvalues-and-dimension-reduction/)\n",
    "- [PCA Tutorial](https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c)\n",
    "- The principal components are the underlying structure in the data. They represent the directions in which the data has maximum variance and also the directions in which the data is most spread out.\n",
    "- Principal component analysis is a technique for feature extraction — so it combines our input variables in a specific way, then we can drop the “least important” variables while still retaining the most valuable parts of all of the variables!\n",
    "- As an added benefit, each of the “new” variables **after PCA are all independent of one another**. This is a benefit because the assumptions of a linear model require our independent variables to be independent of one another\n",
    "- **Eigenvector and Eigenvalue**\n",
    "    - eigenvector was the direction of the line drawn to find the maximum variance\n",
    "    - eigenvalue was a number that tells us how the data set is spread out on the line which is an eigenvector.\n",
    "    - **amount of eigenvectors that exist equals the number of dimensions the data set has** are perpendicular/orthogonal to each other\n",
    "    - <font color=blue> The big eigen vector (with highest variance) is the principal component </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition (SVD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=brown> Loss Functions </font>\n",
    "- All the algorithms in machine learning rely on minimizing or maximizing a function, which we call “**objective function**”. The group of functions that are minimized are called “loss functions”. \n",
    "- A loss function is a measure of how good a prediction model does in terms of being able to predict the expected outcome. \n",
    "- A most commonly used method of finding the minimum point of function is “gradient descent”.\n",
    "- Loss functions can be broadly categorized into 2 types: <font color=blue>Classification loss and Regression Loss</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=brown> Classification Loss Functions </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Square loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hinge loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=brown> Regression Loss Functions </font>\n",
    "- [** 5 regression loss funtioncs **](https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 Loss, Mean Square Error (MSE), Quadratic loss\n",
    "- Mean Square Error (MSE) is the most commonly used regression loss function. MSE is the sum of squared distances between our target variable and predicted values.\n",
    "\n",
    "<h4 align=\"center\"> $ MSE = \\sum\\limits_{i=1}^n  {(y_i - y_i^p)}^2 $ </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 Loss, Mean Absolute Error (MAE)\n",
    "- MAE is the sum of absolute differences between our target and predicted variables. \n",
    "- So it ** measures the average magnitude of errors ** in a set of predictions, without considering their directions. \n",
    "- If we consider directions also, that would be called ** Mean Bias Error (MBE) **\n",
    "\n",
    "<h4 align=\"center\"> $ MAE = \\sum\\limits_{i=1}^n  {|y_i - y_i^p|} $ </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 vs L2 Loss\n",
    "- [L1 vs L2 Comparison](http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/)\n",
    "- <font color=blue>L1 loss is more robust to outliers than L2</font>,\n",
    "    - Since MSE squares the error (y — y_predicted = e), the value of error (e) increases a lot if e > 1. \n",
    "    - If we have an outlier in our data, the value of e will be high and e² will be >> |e|. \n",
    "    - This will make the model with **MSE loss give more weight to outliers ** than a model with MAE loss.\n",
    "    - MAE loss is useful if the training data is corrupted with outliers\n",
    "- <font color=blue>L1 loss derivative is not continuous, hence inefficient to find solution, i.e. unstable</font>,\n",
    "    - which can lead to missing minima\n",
    "    - As L2 derivative is continuous, it gives more stable solution, however it not robust in case of outliers\n",
    "- <font color=blue>Issue with L1 and L2 loss functions:</font>\n",
    "    - There can be cases where neither loss function gives desirable predictions. \n",
    "    - **For example,** if 90% of observations in our data have true target value of 150 and the remaining 10% have target value between 0–30. \n",
    "    - Then a model with MAE as loss might predict 150 for all observations, ignoring 10% of outlier cases, as it will try to go towards median value. \n",
    "    - In the same case, a model using MSE would give many predictions in the range of 0 to 30 as it will get skewed towards outliers. Both results are undesirable in many business cases.\n",
    "    - **An easy fix would be to transform the target variables. Another way is to try a different loss function. This is the motivation behind our 3rd loss function, Huber loss.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huber Loss, Smooth Mean Absolute Error\n",
    "- Huber loss is less sensitive to outliers in data than the squared error loss. It’s also **differentiable at 0**. \n",
    "- It’s basically absolute error, which becomes quadratic when error is small.\n",
    "- problem with Huber loss is that we might need to train hyper parameter delta which is an iterative process\n",
    "- ** it’s twice differentiable everywhere **\n",
    "- Many ML model implementations like XGBoost use Newton’s method to find the optimum, which is why the second derivative (Hessian) is needed. For ML frameworks like XGBoost, **twice differentiable functions are more favorable**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log-CosH Loss\n",
    "- Log-cosh is another function used in regression tasks that’s **smoother than L2**. \n",
    "- Log-cosh is the logarithm of the hyperbolic cosine of the prediction error.\n",
    "- 'logcosh' works mostly like the mean squared error, but will ** not be so strongly affected by the occasional wildly incorrect prediction **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantile Loss\n",
    "- Quantile loss functions turns out to be useful when we are interested in predicting an interval instead of only point predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=brown> Regularization Algorithms </font>\n",
    "- To avoid over optimizing/over-fitting the training set to use early termination as soon as the learning stops, other method is ** to use regularization **\n",
    "- An extension made to another method (typically regression methods) that <font color=blue>penalizes models based on their complexity</font>, favoring simpler models that are also better at generalizing.\n",
    "- other regularization technique is **dropout**\n",
    "- [Regularization techniques](https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-regularization-techniques/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=DarkMagenta> L1 (Lasso) and L2 (Ridge) as Reguralization </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop-out\n",
    "- remove few connections randomly to force the network to learn redundant representation of input, so that it doesn't overfit and depend on any particular parameter, so that all learns independently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=brown> Bias Variance Trade-Off </font>\n",
    "- [Bias Variance Trade-off Tutorial](https://www.listendata.com/2017/02/bias-variance-tradeoff.html)\n",
    "- [Bias Variance Trade-off Infograph](https://elitedatascience.com/bias-variance-tradeoff)\n",
    "- ** Bias **\n",
    "    - Bias is a measure of the prediction accuracy on training data\n",
    "    - <font color=blue> High bias means low prediction accuracy </font>, which means model may be too simple not able learn from training data known as underfitting\n",
    "    - For example, a linear regression model would have high bias when trying to model a non-linear relationship\n",
    "    - High Bias Techniques\n",
    "        - Linear Regression, Linear Discriminant Analysis and Logistic Regression\n",
    "    - Low Bias Techniques\n",
    "        - Decision Trees,  K-nearest neighbours and Gradient Boosting\n",
    "    - <font color=blue> Parametric algorithms which assume something about the distribution of the data points </font> suffer from High Bias. Whereas non-parametric algorithms which does not assume anything special about distribution have low bias.\n",
    "- ** Variance **\n",
    "    - Variance is a measure of the generalization of the network\n",
    "    - <font color=blue> High variance means less generalization </font>, complex models that fits well on training data but they cannot generalise the pattern well which results to overfitting\n",
    "    - Low Variance Techniques\n",
    "        - Linear Regression, Linear Discriminant Analysis and Logistic Regression\n",
    "    - High Variance Techniques\n",
    "        - Decision Trees,  K-nearest neighbours and SVM\n",
    "- ** Bias Variance Trade-off **\n",
    "    - It means there is a trade-off between predictive accuracy and generalization of pattern outside training data. Increasing the accuracy of the model will lead to less generalization of pattern outside training data. Increasing the bias will decrease the variance. Increasing the variance will decrease the bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=brown> CNN Layer Theory </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rectified Linear Unit (ReLU)\n",
    "- [Guide to ReLU](https://medium.com/tinymind/a-practical-guide-to-relu-b83ca804f1f7)\n",
    "- ReLU\n",
    "    - defined as y = max(0, x), doesn’t “saturate,” when x gets large. Hence, doesn’t have the vanishing gradient problem suffered by other activation functions like sigmoid or tanh\n",
    "- Dying ReLU\n",
    "    - A ReLU neuron is “dead” if it’s stuck in the negative side and always outputs 0. \n",
    "    - Because the slope of ReLU in the negative range is also 0, once a neuron gets negative, it’s unlikely for it to recover. \n",
    "    - Such neurons are not playing any role in discriminating the input and is essentially useless.\n",
    "- Leaky ReLU\n",
    "    - Leaky ReLU has a small slope for negative values, instead of altogether zero. \n",
    "    - For example, leaky ReLU may have ** y = 0.01x ** when x < 0\n",
    "- Parametric ReLU (PReLU)\n",
    "    - Parametric ReLU (PReLU) is a type of leaky ReLU that, instead of having a predetermined slope like 0.01, makes it a parameter for the neural network to figure out itself: y = ax when x < 0.\n",
    "- Exponential Linear (ELU, SELU)\n",
    "    - Similar to leaky ReLU, ELU has a small slope for negative values. Instead of a straight line, it ** uses a log curve ** y = a(ex-1)\n",
    "    - sometimes called Scaled ELU (SELU) due to the constant factor a\n",
    "    - leaky ReLU while it doesn’t have the dying ReLU problem, it saturates for large negative values, allowing them to be essentially inactive\n",
    "    - It is designed to combine the good parts of ReLU and leaky ReLU\n",
    "- Concatenated ReLU (CReLU)\n",
    "    - Concatenated ReLU has two outputs, one ReLU and one negative ReLU, concatenated together. \n",
    "    - In other words, for positive x it produces [x, 0], and for negative x it produces [0, x]. \n",
    "    - Because it has two outputs, CReLU doubles the output dimension.\n",
    "- ReLU-6\n",
    "    - ReLU capped at 6, 6 is an arbitrary choice that worked well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability and Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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\n",
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"400\"\n",
       "            height=\"300\"\n",
       "            src=\"https://www.youtube.com/embed/o4QmoNfW3bI\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x184ad4db9e8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('o4QmoNfW3bI')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binomial Theorem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/jpeg": "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\n",
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"400\"\n",
       "            height=\"300\"\n",
       "            src=\"https://www.youtube.com/embed/6nOhUqA29EI\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x184ad5aca58>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YouTubeVideo('6nOhUqA29EI')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing or Feature Scaling\n",
    "- Standardization\n",
    "- Mean Normalization\n",
    "- Min-Max Scaling\n",
    "- Unit Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard deviation\n",
    "- [Standard deviation tutorial](https://www.mathsisfun.com/data/standard-deviation.html#WhySquare)\n",
    "- The Standard Deviation is a measure of how spread out numbers are (compared to mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization and Standardization\n",
    "- [Standardization Vs Normalization](https://medium.com/@rrfd/standardize-or-normalize-examples-in-python-e3f174b65dfc)\n",
    "- **Normalization (Min-Max Scaling):**\n",
    "    - If the training data contain some features with higher magnitude (outliers), then the results might be dominated by them instead of all the features, hence normalize the data with largest value\n",
    "    - Normalization makes training less sensitive to the scale of features, so we can better solve for coefficients\n",
    "    - Normalizing will ensure that a convergence problem does not have a massive variance, making optimization feasible\n",
    "    - Normalization modifies the data range to [0, 1] using below formula,\n",
    "    <h4 align=\"center\"> $ norm(x) = \\frac{x - min(x)}{max(x) - min(x)} $ </h4>\n",
    "- **Standardization (Z-Score Normalization)**\n",
    "    - Data is rescaled such that mean (μ) = 0 and standard deviation (𝛔) = 1 as,\n",
    "    <h4 align=\"center\"> $ std(x) = \\frac{x - \\mu}{\\sigma} $ </h4>\n",
    "    - **standardized (z-score) or studentized (t-scores) scores to increase comparability of different features in the training data**\n",
    "    - **to arrive at (transformed) data that follows a normal distribution**\n",
    "    - But if original data does not follow normal distribution, then transformed data also may not follow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance and Correlation\n",
    "- [Covariance and Correlation Tutorial](https://medium.com/@adamzerner/covariance-and-correlation-d4c64769d4f1)\n",
    "- Covariance\n",
    "    - **Covariance is a measure of how much two random variables vary together**. It’s similar to variance, but where variance tells you how a single variable varies, covariance tells you how two variables vary together\n",
    "    - positive covariance implies a direct relationship between the variables (increasing x increases y)\n",
    "    - negative covariance implies an indirect relationship between the variables (increasing x decreases y)\n",
    "    - ** If the covariance is large, so there is a strong relationship between the numbers** or if the covariance is small, so there is a weak relationship between the numbers\n",
    "- Correlation\n",
    "    - **The covariance can tell the direction/variability between two data, can not tell the how much is the relation**, since the data expressed in different units (ex: mm, cm, meters) yields varying covariance (large/small)\n",
    "    - <font color=blue>Hence the data's has to be standardized to find the strength of covariance, the standardized covariance known as Correlation which tells the strength</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whitening"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "- [Batch Normalization Tutorial](https://www.learnopencv.com/batch-normalization-in-deep-networks/)\n",
    "- [Batch Normalization Explained](https://blog.paperspace.com/busting-the-myths-about-batch-normalization/)\n",
    "- Covariate Shift\n",
    "    - covariate shift is defined as a change in the distribution of data\n",
    "    - If a DNN is trained on a data set containing a class (say red roses) and similar class with different attributes (say white roses)\n",
    "    - These two classes will be present in different regions in the feature space, this difference in distribution is called the covariate shift\n",
    "    - So while training, the mini batch should have the same distribution (of both red and white roses) sampled from the entire training data set\n",
    "    - When the mini-batches have images uniformly sampled from the entire distribution, there is negligible covariate shift\n",
    "    - However, when the mini-batches are sampled from only one of the two subsets, there is a ** significant covariate shift **. This makes the training of the rose vs non-rose classifier very slow\n",
    "    - An easy way to solve this problem to normalize the inputs (mini batch) to the neural network so that the input distribution have a zero mean and a unit variance\n",
    "- **Internal Covariate Shift**\n",
    "    - However, when the networks get deeper, say, 20 or more layers, the minor fluctuations in weights over more than 20 odd layers can produce big changes in the distribution of the input being fed to deeper layers even if the input is normalized.\n",
    "    - Just as it made intuitive sense to have a uniform distribution for the input layer, it is advantageous to have the same input distribution for each hidden unit over time while training. \n",
    "    - But in a neural network, each ** hidden unit’s input distribution changes every time there is a parameter update in the previous layer **. This is called internal covariate shift. \n",
    "    - This makes training slow and requires a very small learning rate and a good parameter initialization.\n",
    "- This problem is solved by normalizing the layer’s inputs over a mini-batch and this process is therefore called Batch Normalization.\n",
    "- However, as against above discussion, batch norm actually ends up increasing internal covariate shift as compared to a network that doesn't use batch norm. They key insight from the paper is that batch norm actually makes the loss surface smoother, which is why it works so well (as pointed by a recent research paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "- [Gradient Descent Tutorial](https://blog.paperspace.com/intro-to-optimization-in-deep-learning-gradient-descent/)\n",
    "- how do we go about **escaping local minima and saddle points**, while trying to converge to a global maximum. The answer is **randomness**\n",
    "- Gradient descent is a First Order Optimization Method. It only takes the first order derivatives of the loss function into account and not the higher ones. What this basically means it has no clue about the curvature of the loss function. It can tell whether the loss is declining and how fast, but cannot differentiate between whether the curve is a plane, curving upwards or curving downwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back Propagation\n",
    "- [Back propagation step by step example](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=green> New DNN Research Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transparency by Design (TbD Net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capsule Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero shot learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=orange> Interview Questions </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skills required for ML/DL Engineer Jobs\n",
    "- CNN architectures & algorithms, RNNs, NLP, Reinforcement learning\n",
    "- ML basic algorithms\n",
    "- Linear algebra, Probability, Statistics\n",
    "- DL frameworks (eg: Tensorflow, Keras)\n",
    "- Pyhon, R, jupyter notebooks\n",
    "- ARM, DSP, GPU working experience\n",
    "- Experience in building ML Applications like Image Segmentation, Object Detection, etc\n",
    "- OpenCL, CUDA GPU programming\n",
    "- Publications in CVPR/NIPS/ICML/ICLR would be an added advantage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=brown> Best Resources </font>\n",
    "- [Over 200 best ML, NLP, Python Tutorials - 2018 Edition](https://medium.com/machine-learning-in-practice/over-200-of-the-best-machine-learning-nlp-and-python-tutorials-2018-edition-dd8cf53cb7dc)\n",
    "- [ML, DL Tutorials](https://github.com/ujjwalkarn/Machine-Learning-Tutorials)\n",
    "- [Machine Learning Introduction- Series](https://medium.com/machine-learning-for-humans/supervised-learning-740383a2feab)\n",
    "- [ML Cheet sheet](https://medium.com/machine-learning-in-practice/cheat-sheet-of-machine-learning-and-python-and-math-cheat-sheets-a4afe4e791b6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "143px",
    "width": "223px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "241px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
