{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#ML-DL-Notes\" data-toc-modified-id=\"ML-DL-Notes-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>ML DL Notes</a></span></li><li><span><a href=\"#-Topics-to-learn-\" data-toc-modified-id=\"-Topics-to-learn--2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span><font color=\"red\"> Topics to learn </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#-Learn-Now-(most-important)-\" data-toc-modified-id=\"-Learn-Now-(most-important)--2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span><font color=\"green\"> Learn Now (most important) </font></a></span></li><li><span><a href=\"#-Learn-later-\" data-toc-modified-id=\"-Learn-later--2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span><font color=\"red\"> Learn later </font></a></span></li></ul></li><li><span><a href=\"#-Probability-and-Statistics-\" data-toc-modified-id=\"-Probability-and-Statistics--3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span><font color=\"DarkOrange\"> Probability and Statistics </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#Probability-distributions\" data-toc-modified-id=\"Probability-distributions-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Probability distributions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Discrete-distributions,-Probability-Mass-Function-(PMF)\" data-toc-modified-id=\"Discrete-distributions,-Probability-Mass-Function-(PMF)-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Discrete distributions, Probability Mass Function (PMF)</a></span></li><li><span><a href=\"#Continuous-distributions,-Probability-Density-Function-(PDF)\" data-toc-modified-id=\"Continuous-distributions,-Probability-Density-Function-(PDF)-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;</span>Continuous distributions, Probability Density Function (PDF)</a></span></li><li><span><a href=\"#-$-\\int_{\\Omega}-f(x)-dx-=-1-$-\" data-toc-modified-id=\"-$-\\int_{\\Omega}-f(x)-dx-=-1-$--3.1.3\"><span class=\"toc-item-num\">3.1.3&nbsp;&nbsp;</span> $ \\int_{\\Omega} f(x) dx = 1 $ </a></span></li><li><span><a href=\"#-$-\\int_{0}^{1}-f(x;-\\mu,-\\sigma)-dx-=-P-(0-<-X-<-1)-$-\" data-toc-modified-id=\"-$-\\int_{0}^{1}-f(x;-\\mu,-\\sigma)-dx-=-P-(0-<-X-<-1)-$--3.1.4\"><span class=\"toc-item-num\">3.1.4&nbsp;&nbsp;</span> $ \\int_{0}^{1} f(x; \\mu, \\sigma) dx = P (0 &lt; X &lt; 1) $ </a></span></li></ul></li><li><span><a href=\"#Types-of-Probability\" data-toc-modified-id=\"Types-of-Probability-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Types of Probability</a></span></li><li><span><a href=\"#Central-Limit-Theorem-(CLT)\" data-toc-modified-id=\"Central-Limit-Theorem-(CLT)-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Central Limit Theorem (CLT)</a></span></li><li><span><a href=\"#Bernoulli-Distribution\" data-toc-modified-id=\"Bernoulli-Distribution-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Bernoulli Distribution</a></span></li><li><span><a href=\"#Binomial-Theorem\" data-toc-modified-id=\"Binomial-Theorem-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Binomial Theorem</a></span></li><li><span><a href=\"#Standard-Normal-Distribution-or-Z-distribution\" data-toc-modified-id=\"Standard-Normal-Distribution-or-Z-distribution-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>Standard Normal Distribution or Z-distribution</a></span><ul class=\"toc-item\"><li><span><a href=\"#-$-std(x)-=-\\frac{x---\\mu}{\\sigma}-$-\" data-toc-modified-id=\"-$-std(x)-=-\\frac{x---\\mu}{\\sigma}-$--3.6.1\"><span class=\"toc-item-num\">3.6.1&nbsp;&nbsp;</span> $ std(x) = \\frac{x - \\mu}{\\sigma} $ </a></span></li></ul></li><li><span><a href=\"#Maximum-Likelihood-Estimation-(MLE)\" data-toc-modified-id=\"Maximum-Likelihood-Estimation-(MLE)-3.7\"><span class=\"toc-item-num\">3.7&nbsp;&nbsp;</span>Maximum Likelihood Estimation (MLE)</a></span></li><li><span><a href=\"#Log-likelihood\" data-toc-modified-id=\"Log-likelihood-3.8\"><span class=\"toc-item-num\">3.8&nbsp;&nbsp;</span>Log likelihood</a></span></li><li><span><a href=\"#Conditional-Probability\" data-toc-modified-id=\"Conditional-Probability-3.9\"><span class=\"toc-item-num\">3.9&nbsp;&nbsp;</span>Conditional Probability</a></span></li><li><span><a href=\"#Bayes-Theorem\" data-toc-modified-id=\"Bayes-Theorem-3.10\"><span class=\"toc-item-num\">3.10&nbsp;&nbsp;</span>Bayes Theorem</a></span><ul class=\"toc-item\"><li><span><a href=\"#-$-Posterior-=-\\frac-{Likelihood-*-Prior}{Evidence}-$-\" data-toc-modified-id=\"-$-Posterior-=-\\frac-{Likelihood-*-Prior}{Evidence}-$--3.10.1\"><span class=\"toc-item-num\">3.10.1&nbsp;&nbsp;</span> $ Posterior = \\frac {Likelihood * Prior}{Evidence} $ </a></span></li></ul></li><li><span><a href=\"#Bayesian-Inference\" data-toc-modified-id=\"Bayesian-Inference-3.11\"><span class=\"toc-item-num\">3.11&nbsp;&nbsp;</span>Bayesian Inference</a></span></li><li><span><a href=\"#Naive-Bayes\" data-toc-modified-id=\"Naive-Bayes-3.12\"><span class=\"toc-item-num\">3.12&nbsp;&nbsp;</span>Naive Bayes</a></span></li></ul></li><li><span><a href=\"#-ML/DL-basics-introduction-\" data-toc-modified-id=\"-ML/DL-basics-introduction--4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span><font color=\"DarkOrange\"> ML/DL basics introduction </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#Perceptron\" data-toc-modified-id=\"Perceptron-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Perceptron</a></span></li><li><span><a href=\"#Parametric-Vs-Non-Parametric-algorithms\" data-toc-modified-id=\"Parametric-Vs-Non-Parametric-algorithms-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Parametric Vs Non-Parametric algorithms</a></span></li><li><span><a href=\"#Sigmoid-Neuron\" data-toc-modified-id=\"Sigmoid-Neuron-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Sigmoid Neuron</a></span></li><li><span><a href=\"#Gradient-decent\" data-toc-modified-id=\"Gradient-decent-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Gradient decent</a></span></li><li><span><a href=\"#Backpropagation\" data-toc-modified-id=\"Backpropagation-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Backpropagation</a></span></li><li><span><a href=\"#Overfitting\" data-toc-modified-id=\"Overfitting-4.6\"><span class=\"toc-item-num\">4.6&nbsp;&nbsp;</span>Overfitting</a></span><ul class=\"toc-item\"><li><span><a href=\"#How-to-avoid-overfitting\" data-toc-modified-id=\"How-to-avoid-overfitting-4.6.1\"><span class=\"toc-item-num\">4.6.1&nbsp;&nbsp;</span>How to avoid overfitting</a></span></li></ul></li><li><span><a href=\"#Vanishing-Gradients\" data-toc-modified-id=\"Vanishing-Gradients-4.7\"><span class=\"toc-item-num\">4.7&nbsp;&nbsp;</span>Vanishing Gradients</a></span><ul class=\"toc-item\"><li><span><a href=\"#How-to-avoid-vanishing-gradients\" data-toc-modified-id=\"How-to-avoid-vanishing-gradients-4.7.1\"><span class=\"toc-item-num\">4.7.1&nbsp;&nbsp;</span>How to avoid vanishing gradients</a></span></li></ul></li><li><span><a href=\"#Cross-validation\" data-toc-modified-id=\"Cross-validation-4.8\"><span class=\"toc-item-num\">4.8&nbsp;&nbsp;</span>Cross validation</a></span></li><li><span><a href=\"#Types-of-data\" data-toc-modified-id=\"Types-of-data-4.9\"><span class=\"toc-item-num\">4.9&nbsp;&nbsp;</span>Types of data</a></span></li><li><span><a href=\"#Classification-and-Regression\" data-toc-modified-id=\"Classification-and-Regression-4.10\"><span class=\"toc-item-num\">4.10&nbsp;&nbsp;</span>Classification and Regression</a></span></li></ul></li><li><span><a href=\"#-Supervised-Learning-\" data-toc-modified-id=\"-Supervised-Learning--5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span><font color=\"DarkOrange\"> Supervised Learning </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#-Parametric-and-Classification-Algorithms-\" data-toc-modified-id=\"-Parametric-and-Classification-Algorithms--5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span><font color=\"DarkMagenta\"> Parametric and Classification Algorithms </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#Support-Vector-Machine-(SVM)\" data-toc-modified-id=\"Support-Vector-Machine-(SVM)-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>Support Vector Machine (SVM)</a></span></li><li><span><a href=\"#Logistic-regression\" data-toc-modified-id=\"Logistic-regression-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;</span>Logistic regression</a></span></li></ul></li><li><span><a href=\"#-Parametric-and-Regression-Algorithms-\" data-toc-modified-id=\"-Parametric-and-Regression-Algorithms--5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span><font color=\"DarkMagenta\"> Parametric and Regression Algorithms </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#Linear-regression\" data-toc-modified-id=\"Linear-regression-5.2.1\"><span class=\"toc-item-num\">5.2.1&nbsp;&nbsp;</span>Linear regression</a></span></li></ul></li><li><span><a href=\"#-Non-Parametric-Algorithms-\" data-toc-modified-id=\"-Non-Parametric-Algorithms--5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span><font color=\"DarkMagenta\"> Non-Parametric Algorithms </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#K-Nearest-Neighbor-(KNN)\" data-toc-modified-id=\"K-Nearest-Neighbor-(KNN)-5.3.1\"><span class=\"toc-item-num\">5.3.1&nbsp;&nbsp;</span>K-Nearest Neighbor (KNN)</a></span></li><li><span><a href=\"#Decision-Tree\" data-toc-modified-id=\"Decision-Tree-5.3.2\"><span class=\"toc-item-num\">5.3.2&nbsp;&nbsp;</span>Decision Tree</a></span></li><li><span><a href=\"#Random-Forest:--an-ensemble-of-decision-trees\" data-toc-modified-id=\"Random-Forest:--an-ensemble-of-decision-trees-5.3.3\"><span class=\"toc-item-num\">5.3.3&nbsp;&nbsp;</span>Random Forest:  an ensemble of decision trees</a></span></li></ul></li></ul></li><li><span><a href=\"#-Unsupervised-Learning-\" data-toc-modified-id=\"-Unsupervised-Learning--6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span><font color=\"DarkOrange\"> Unsupervised Learning </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#-Clusttering-\" data-toc-modified-id=\"-Clusttering--6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span><font color=\"DarkMagenta\"> Clusttering </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#K-means-clusttering\" data-toc-modified-id=\"K-means-clusttering-6.1.1\"><span class=\"toc-item-num\">6.1.1&nbsp;&nbsp;</span>K-means clusttering</a></span></li><li><span><a href=\"#Expectation–Maximization-(EM)-Clustering-using-Gaussian-Mixture-Models-(GMM)\" data-toc-modified-id=\"Expectation–Maximization-(EM)-Clustering-using-Gaussian-Mixture-Models-(GMM)-6.1.2\"><span class=\"toc-item-num\">6.1.2&nbsp;&nbsp;</span>Expectation–Maximization (EM) Clustering using Gaussian Mixture Models (GMM)</a></span></li><li><span><a href=\"#Hierarchical-Clusttering\" data-toc-modified-id=\"Hierarchical-Clusttering-6.1.3\"><span class=\"toc-item-num\">6.1.3&nbsp;&nbsp;</span>Hierarchical Clusttering</a></span></li></ul></li><li><span><a href=\"#-Dimensionality-Reduction-\" data-toc-modified-id=\"-Dimensionality-Reduction--6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span><font color=\"DarkMagenta\"> Dimensionality Reduction </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#Principal-Component-Analysis-(PCA)\" data-toc-modified-id=\"Principal-Component-Analysis-(PCA)-6.2.1\"><span class=\"toc-item-num\">6.2.1&nbsp;&nbsp;</span>Principal Component Analysis (PCA)</a></span></li><li><span><a href=\"#Singular-Value-Decomposition-(SVD)\" data-toc-modified-id=\"Singular-Value-Decomposition-(SVD)-6.2.2\"><span class=\"toc-item-num\">6.2.2&nbsp;&nbsp;</span>Singular Value Decomposition (SVD)</a></span></li></ul></li></ul></li><li><span><a href=\"#-Loss-Functions-\" data-toc-modified-id=\"-Loss-Functions--7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span><font color=\"DarkOrange\"> Loss Functions </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#-Classification-Loss-Functions-\" data-toc-modified-id=\"-Classification-Loss-Functions--7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span><font color=\"DarkMagenta\"> Classification Loss Functions </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#Square-loss\" data-toc-modified-id=\"Square-loss-7.1.1\"><span class=\"toc-item-num\">7.1.1&nbsp;&nbsp;</span>Square loss</a></span></li><li><span><a href=\"#Hinge-loss\" data-toc-modified-id=\"Hinge-loss-7.1.2\"><span class=\"toc-item-num\">7.1.2&nbsp;&nbsp;</span>Hinge loss</a></span></li><li><span><a href=\"#Cross-entropy-loss\" data-toc-modified-id=\"Cross-entropy-loss-7.1.3\"><span class=\"toc-item-num\">7.1.3&nbsp;&nbsp;</span>Cross entropy loss</a></span><ul class=\"toc-item\"><li><span><a href=\"#-$-H(y)-=-\\sum\\limits_{i}--{(y_i-\\log(\\frac{1}{y_i}))}-=--\\sum\\limits_{i}--{(y_i-\\log({y_i}))}-$-\" data-toc-modified-id=\"-$-H(y)-=-\\sum\\limits_{i}--{(y_i-\\log(\\frac{1}{y_i}))}-=--\\sum\\limits_{i}--{(y_i-\\log({y_i}))}-$--7.1.3.1\"><span class=\"toc-item-num\">7.1.3.1&nbsp;&nbsp;</span> $ H(y) = \\sum\\limits_{i}  {(y_i \\log(\\frac{1}{y_i}))} = -\\sum\\limits_{i}  {(y_i \\log({y_i}))} $ </a></span></li><li><span><a href=\"#-$-\\sum\\limits_{i}--{(p_i-\\log(\\frac{1}{p_i}))}-$-\" data-toc-modified-id=\"-$-\\sum\\limits_{i}--{(p_i-\\log(\\frac{1}{p_i}))}-$--7.1.3.2\"><span class=\"toc-item-num\">7.1.3.2&nbsp;&nbsp;</span> $ \\sum\\limits_{i}  {(p_i \\log(\\frac{1}{p_i}))} $ </a></span></li><li><span><a href=\"#-$-\\hat{y}^y-*-(1---\\hat{y})^{(1---y)}-$-\" data-toc-modified-id=\"-$-\\hat{y}^y-*-(1---\\hat{y})^{(1---y)}-$--7.1.3.3\"><span class=\"toc-item-num\">7.1.3.3&nbsp;&nbsp;</span> $ \\hat{y}^y * (1 - \\hat{y})^{(1 - y)} $ </a></span></li><li><span><a href=\"#-$-y-*-\\log{\\hat{y}}-+-(1---\\hat{y})-*-\\log(1---\\hat{y})-$-\" data-toc-modified-id=\"-$-y-*-\\log{\\hat{y}}-+-(1---\\hat{y})-*-\\log(1---\\hat{y})-$--7.1.3.4\"><span class=\"toc-item-num\">7.1.3.4&nbsp;&nbsp;</span> $ y * \\log{\\hat{y}} + (1 - \\hat{y}) * \\log(1 - \\hat{y}) $ </a></span></li><li><span><a href=\"#-$--y-*-\\log{\\hat{y}}---(1---\\hat{y})-*-\\log(1---\\hat{y})-$-\" data-toc-modified-id=\"-$--y-*-\\log{\\hat{y}}---(1---\\hat{y})-*-\\log(1---\\hat{y})-$--7.1.3.5\"><span class=\"toc-item-num\">7.1.3.5&nbsp;&nbsp;</span> $ -y * \\log{\\hat{y}} - (1 - \\hat{y}) * \\log(1 - \\hat{y}) $ </a></span></li></ul></li><li><span><a href=\"#Logistic-loss\" data-toc-modified-id=\"Logistic-loss-7.1.4\"><span class=\"toc-item-num\">7.1.4&nbsp;&nbsp;</span>Logistic loss</a></span><ul class=\"toc-item\"><li><span><a href=\"#-$Loss-(y,-\\hat{y})-=---y-*-\\log{\\hat{y}}---(1---\\hat{y})-*-\\log(1---\\hat{y})-$-\" data-toc-modified-id=\"-$Loss-(y,-\\hat{y})-=---y-*-\\log{\\hat{y}}---(1---\\hat{y})-*-\\log(1---\\hat{y})-$--7.1.4.1\"><span class=\"toc-item-num\">7.1.4.1&nbsp;&nbsp;</span> $Loss (y, \\hat{y}) =  -y * \\log{\\hat{y}} - (1 - \\hat{y}) * \\log(1 - \\hat{y}) $ </a></span></li></ul></li><li><span><a href=\"#KL-Divergence\" data-toc-modified-id=\"KL-Divergence-7.1.5\"><span class=\"toc-item-num\">7.1.5&nbsp;&nbsp;</span>KL Divergence</a></span></li></ul></li><li><span><a href=\"#-Regression-Loss-Functions-\" data-toc-modified-id=\"-Regression-Loss-Functions--7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span><font color=\"DarkMagenta\"> Regression Loss Functions </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#L2-Loss,-Mean-Square-Error-(MSE),-Quadratic-loss\" data-toc-modified-id=\"L2-Loss,-Mean-Square-Error-(MSE),-Quadratic-loss-7.2.1\"><span class=\"toc-item-num\">7.2.1&nbsp;&nbsp;</span>L2 Loss, Mean Square Error (MSE), Quadratic loss</a></span><ul class=\"toc-item\"><li><span><a href=\"#-$-MSE-=-\\sum\\limits_{i=1}^n--{(y_i---y_i^p)}^2-$-\" data-toc-modified-id=\"-$-MSE-=-\\sum\\limits_{i=1}^n--{(y_i---y_i^p)}^2-$--7.2.1.1\"><span class=\"toc-item-num\">7.2.1.1&nbsp;&nbsp;</span> $ MSE = \\sum\\limits_{i=1}^n  {(y_i - y_i^p)}^2 $ </a></span></li><li><span><a href=\"#-$-PSNR-=-10-log-\\frac{L^2}{MSE}-$-\" data-toc-modified-id=\"-$-PSNR-=-10-log-\\frac{L^2}{MSE}-$--7.2.1.2\"><span class=\"toc-item-num\">7.2.1.2&nbsp;&nbsp;</span> $ PSNR = 10 log \\frac{L^2}{MSE} $ </a></span></li></ul></li><li><span><a href=\"#L1-Loss,-Mean-Absolute-Error-(MAE)\" data-toc-modified-id=\"L1-Loss,-Mean-Absolute-Error-(MAE)-7.2.2\"><span class=\"toc-item-num\">7.2.2&nbsp;&nbsp;</span>L1 Loss, Mean Absolute Error (MAE)</a></span><ul class=\"toc-item\"><li><span><a href=\"#-$-MAE-=-\\sum\\limits_{i=1}^n--{|y_i---y_i^p|}-$-\" data-toc-modified-id=\"-$-MAE-=-\\sum\\limits_{i=1}^n--{|y_i---y_i^p|}-$--7.2.2.1\"><span class=\"toc-item-num\">7.2.2.1&nbsp;&nbsp;</span> $ MAE = \\sum\\limits_{i=1}^n  {|y_i - y_i^p|} $ </a></span></li></ul></li><li><span><a href=\"#L1-vs-L2-Loss\" data-toc-modified-id=\"L1-vs-L2-Loss-7.2.3\"><span class=\"toc-item-num\">7.2.3&nbsp;&nbsp;</span>L1 vs L2 Loss</a></span></li><li><span><a href=\"#Huber-Loss,-Smooth-Mean-Absolute-Error\" data-toc-modified-id=\"Huber-Loss,-Smooth-Mean-Absolute-Error-7.2.4\"><span class=\"toc-item-num\">7.2.4&nbsp;&nbsp;</span>Huber Loss, Smooth Mean Absolute Error</a></span></li><li><span><a href=\"#Log-CosH-Loss\" data-toc-modified-id=\"Log-CosH-Loss-7.2.5\"><span class=\"toc-item-num\">7.2.5&nbsp;&nbsp;</span>Log-CosH Loss</a></span></li><li><span><a href=\"#Quantile-Loss\" data-toc-modified-id=\"Quantile-Loss-7.2.6\"><span class=\"toc-item-num\">7.2.6&nbsp;&nbsp;</span>Quantile Loss</a></span></li></ul></li><li><span><a href=\"#SSIM,-MS-SSIM-loss-functions\" data-toc-modified-id=\"SSIM,-MS-SSIM-loss-functions-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>SSIM, MS-SSIM loss functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#-$-mean:-\\mu_I-=-\\frac{1}{N}-\\sum\\limits_{i=1}^N--{I(i)}-$-\" data-toc-modified-id=\"-$-mean:-\\mu_I-=-\\frac{1}{N}-\\sum\\limits_{i=1}^N--{I(i)}-$--7.3.1\"><span class=\"toc-item-num\">7.3.1&nbsp;&nbsp;</span> $ mean: \\mu_I = \\frac{1}{N} \\sum\\limits_{i=1}^N  {I(i)} $ </a></span></li><li><span><a href=\"#-$-contrast:-\\sigma_I-=-\\sqrt{\\frac{1}{N-1}-\\sum\\limits_{i=1}^N--({I(i)---\\mu_I})^2}-$-\" data-toc-modified-id=\"-$-contrast:-\\sigma_I-=-\\sqrt{\\frac{1}{N-1}-\\sum\\limits_{i=1}^N--({I(i)---\\mu_I})^2}-$--7.3.2\"><span class=\"toc-item-num\">7.3.2&nbsp;&nbsp;</span> $ contrast: \\sigma_I = \\sqrt{\\frac{1}{N-1} \\sum\\limits_{i=1}^N  ({I(i) - \\mu_I})^2} $ </a></span></li></ul></li><li><span><a href=\"#Combination-of-loss-functions-for-perceptual-quality\" data-toc-modified-id=\"Combination-of-loss-functions-for-perceptual-quality-7.4\"><span class=\"toc-item-num\">7.4&nbsp;&nbsp;</span>Combination of loss functions for perceptual quality</a></span></li><li><span><a href=\"#Texture-loss\" data-toc-modified-id=\"Texture-loss-7.5\"><span class=\"toc-item-num\">7.5&nbsp;&nbsp;</span>Texture loss</a></span></li><li><span><a href=\"#Total-Variation-Loss\" data-toc-modified-id=\"Total-Variation-Loss-7.6\"><span class=\"toc-item-num\">7.6&nbsp;&nbsp;</span>Total Variation Loss</a></span></li><li><span><a href=\"#Prior-based-loss\" data-toc-modified-id=\"Prior-based-loss-7.7\"><span class=\"toc-item-num\">7.7&nbsp;&nbsp;</span>Prior based loss</a></span></li><li><span><a href=\"#Triplet-loss-(ex:-face-recognition)\" data-toc-modified-id=\"Triplet-loss-(ex:-face-recognition)-7.8\"><span class=\"toc-item-num\">7.8&nbsp;&nbsp;</span>Triplet loss (ex: face recognition)</a></span></li></ul></li><li><span><a href=\"#-Regularization-Algorithms-\" data-toc-modified-id=\"-Regularization-Algorithms--8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span><font color=\"DarkOrange\"> Regularization Algorithms </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#L1-(Lasso)-and-L2-(Ridge)-as-Reguralization\" data-toc-modified-id=\"L1-(Lasso)-and-L2-(Ridge)-as-Reguralization-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>L1 (Lasso) and L2 (Ridge) as Reguralization</a></span></li><li><span><a href=\"#L2-or-Frobenius-Regularization-or-Weight-decay\" data-toc-modified-id=\"L2-or-Frobenius-Regularization-or-Weight-decay-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>L2 or Frobenius Regularization or Weight decay</a></span><ul class=\"toc-item\"><li><span><a href=\"#-$-Loss-(E)-=-\\frac{1}{m}\\sum\\limits_{i=1}^n--{(y_i---y_i^p)}^2-+-\\frac{\\lambda}{2m}-\\sum\\limits_{l=1}^L-{||W_l||^2}-$-\" data-toc-modified-id=\"-$-Loss-(E)-=-\\frac{1}{m}\\sum\\limits_{i=1}^n--{(y_i---y_i^p)}^2-+-\\frac{\\lambda}{2m}-\\sum\\limits_{l=1}^L-{||W_l||^2}-$--8.2.1\"><span class=\"toc-item-num\">8.2.1&nbsp;&nbsp;</span> $ Loss (E) = \\frac{1}{m}\\sum\\limits_{i=1}^n  {(y_i - y_i^p)}^2 + \\frac{\\lambda}{2m} \\sum\\limits_{l=1}^L {||W_l||^2} $ </a></span></li><li><span><a href=\"#-$-dW-=-dNormal-+-\\frac-{\\lambda}{m}-W_l-$-\" data-toc-modified-id=\"-$-dW-=-dNormal-+-\\frac-{\\lambda}{m}-W_l-$--8.2.2\"><span class=\"toc-item-num\">8.2.2&nbsp;&nbsp;</span> $ dW = dNormal + \\frac {\\lambda}{m} W_l $ </a></span></li><li><span><a href=\"#-$-W_l-=-W_l---\\sigma-dW-$-\" data-toc-modified-id=\"-$-W_l-=-W_l---\\sigma-dW-$--8.2.3\"><span class=\"toc-item-num\">8.2.3&nbsp;&nbsp;</span> $ W_l = W_l - \\sigma dW $ </a></span></li><li><span><a href=\"#-$-(1---\\frac{\\sigma-\\lambda}-{m})-W_l---\\sigma-dNormal-$-\" data-toc-modified-id=\"-$-(1---\\frac{\\sigma-\\lambda}-{m})-W_l---\\sigma-dNormal-$--8.2.4\"><span class=\"toc-item-num\">8.2.4&nbsp;&nbsp;</span> $ (1 - \\frac{\\sigma \\lambda} {m}) W_l - \\sigma dNormal $ </a></span></li></ul></li><li><span><a href=\"#Drop-out\" data-toc-modified-id=\"Drop-out-8.3\"><span class=\"toc-item-num\">8.3&nbsp;&nbsp;</span>Drop-out</a></span></li><li><span><a href=\"#Early-Stopping\" data-toc-modified-id=\"Early-Stopping-8.4\"><span class=\"toc-item-num\">8.4&nbsp;&nbsp;</span>Early Stopping</a></span></li><li><span><a href=\"#Data-Augmentation-to-avoid-overfitting\" data-toc-modified-id=\"Data-Augmentation-to-avoid-overfitting-8.5\"><span class=\"toc-item-num\">8.5&nbsp;&nbsp;</span>Data Augmentation to avoid overfitting</a></span></li></ul></li><li><span><a href=\"#-CNN-Layer-Theory-\" data-toc-modified-id=\"-CNN-Layer-Theory--9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span><font color=\"DarkOrange\"> CNN Layer Theory </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#Rectified-Linear-Unit-(ReLU)\" data-toc-modified-id=\"Rectified-Linear-Unit-(ReLU)-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>Rectified Linear Unit (ReLU)</a></span></li><li><span><a href=\"#Softmax-regression-for-multi-class-classification\" data-toc-modified-id=\"Softmax-regression-for-multi-class-classification-9.2\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;</span>Softmax regression for multi-class classification</a></span><ul class=\"toc-item\"><li><span><a href=\"#-$-Softmax_{prob}-=-\\frac-{e^Z}{\\sum{e^Z}}-$-\" data-toc-modified-id=\"-$-Softmax_{prob}-=-\\frac-{e^Z}{\\sum{e^Z}}-$--9.2.1\"><span class=\"toc-item-num\">9.2.1&nbsp;&nbsp;</span> $ Softmax_{prob} = \\frac {e^Z}{\\sum{e^Z}} $ </a></span></li></ul></li></ul></li><li><span><a href=\"#-Data-Pre-Processing-or-Feature-Scaling-\" data-toc-modified-id=\"-Data-Pre-Processing-or-Feature-Scaling--10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span><font color=\"DarkOrange\"> Data Pre-Processing or Feature Scaling </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#Standard-deviation\" data-toc-modified-id=\"Standard-deviation-10.1\"><span class=\"toc-item-num\">10.1&nbsp;&nbsp;</span>Standard deviation</a></span></li><li><span><a href=\"#Mean-Normalization\" data-toc-modified-id=\"Mean-Normalization-10.2\"><span class=\"toc-item-num\">10.2&nbsp;&nbsp;</span>Mean Normalization</a></span></li><li><span><a href=\"#Normalization-and-Standardization\" data-toc-modified-id=\"Normalization-and-Standardization-10.3\"><span class=\"toc-item-num\">10.3&nbsp;&nbsp;</span>Normalization and Standardization</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#-$-norm(x)-=-\\frac{x---min(x)}{max(x)---min(x)}-$-\" data-toc-modified-id=\"-$-norm(x)-=-\\frac{x---min(x)}{max(x)---min(x)}-$--10.3.0.1\"><span class=\"toc-item-num\">10.3.0.1&nbsp;&nbsp;</span> $ norm(x) = \\frac{x - min(x)}{max(x) - min(x)} $ </a></span></li><li><span><a href=\"#-$-std(x)-=-\\frac{x---\\mu}{\\sigma}-$-\" data-toc-modified-id=\"-$-std(x)-=-\\frac{x---\\mu}{\\sigma}-$--10.3.0.2\"><span class=\"toc-item-num\">10.3.0.2&nbsp;&nbsp;</span> $ std(x) = \\frac{x - \\mu}{\\sigma} $ </a></span></li></ul></li></ul></li><li><span><a href=\"#Covariance-and-Correlation\" data-toc-modified-id=\"Covariance-and-Correlation-10.4\"><span class=\"toc-item-num\">10.4&nbsp;&nbsp;</span>Covariance and Correlation</a></span></li><li><span><a href=\"#Whitening\" data-toc-modified-id=\"Whitening-10.5\"><span class=\"toc-item-num\">10.5&nbsp;&nbsp;</span>Whitening</a></span></li><li><span><a href=\"#Batch-Normalization\" data-toc-modified-id=\"Batch-Normalization-10.6\"><span class=\"toc-item-num\">10.6&nbsp;&nbsp;</span>Batch Normalization</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#-$-Z_{norm}-=-\\frac{Z---\\mu}{\\sqrt{\\sigma^2+\\epsilon}}-$-\" data-toc-modified-id=\"-$-Z_{norm}-=-\\frac{Z---\\mu}{\\sqrt{\\sigma^2+\\epsilon}}-$--10.6.0.1\"><span class=\"toc-item-num\">10.6.0.1&nbsp;&nbsp;</span> $ Z_{norm} = \\frac{Z - \\mu}{\\sqrt{\\sigma^2+\\epsilon}} $ </a></span></li><li><span><a href=\"#-$-Z_{N}-=-\\gamma-*-Z_{norm}-+-\\beta-$-\" data-toc-modified-id=\"-$-Z_{N}-=-\\gamma-*-Z_{norm}-+-\\beta-$--10.6.0.2\"><span class=\"toc-item-num\">10.6.0.2&nbsp;&nbsp;</span> $ Z_{N} = \\gamma * Z_{norm} + \\beta $ </a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#-Back-Propagation-\" data-toc-modified-id=\"-Back-Propagation--11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span><font color=\"DarkOrange\"> Back Propagation </font></a></span></li><li><span><a href=\"#-Gradient-Descent-\" data-toc-modified-id=\"-Gradient-Descent--12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;</span><font color=\"DarkOrange\"> Gradient Descent </font></a></span></li><li><span><a href=\"#-Deep-Learning-Training-\" data-toc-modified-id=\"-Deep-Learning-Training--13\"><span class=\"toc-item-num\">13&nbsp;&nbsp;</span><font color=\"DarkOrange\"> Deep Learning Training </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#-Bias-Variance-Trade-Off-\" data-toc-modified-id=\"-Bias-Variance-Trade-Off--13.1\"><span class=\"toc-item-num\">13.1&nbsp;&nbsp;</span><font color=\"DarkMagenta\"> Bias Variance Trade-Off </font></a></span></li><li><span><a href=\"#Weight-Initialization\" data-toc-modified-id=\"Weight-Initialization-13.2\"><span class=\"toc-item-num\">13.2&nbsp;&nbsp;</span>Weight Initialization</a></span><ul class=\"toc-item\"><li><span><a href=\"#He-initialization\" data-toc-modified-id=\"He-initialization-13.2.1\"><span class=\"toc-item-num\">13.2.1&nbsp;&nbsp;</span>He initialization</a></span></li></ul></li><li><span><a href=\"#Gradient-checking\" data-toc-modified-id=\"Gradient-checking-13.3\"><span class=\"toc-item-num\">13.3&nbsp;&nbsp;</span>Gradient checking</a></span></li><li><span><a href=\"#Gradient-Clipping\" data-toc-modified-id=\"Gradient-Clipping-13.4\"><span class=\"toc-item-num\">13.4&nbsp;&nbsp;</span>Gradient Clipping</a></span></li></ul></li><li><span><a href=\"#-Hyper-Parameter-Tuning-\" data-toc-modified-id=\"-Hyper-Parameter-Tuning--14\"><span class=\"toc-item-num\">14&nbsp;&nbsp;</span><font color=\"DarkOrange\"> Hyper Parameter Tuning </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#Mini-Batch-Gradient-Descent\" data-toc-modified-id=\"Mini-Batch-Gradient-Descent-14.1\"><span class=\"toc-item-num\">14.1&nbsp;&nbsp;</span>Mini-Batch Gradient Descent</a></span></li><li><span><a href=\"#Exponentially-Weighted-Moving-Average\" data-toc-modified-id=\"Exponentially-Weighted-Moving-Average-14.2\"><span class=\"toc-item-num\">14.2&nbsp;&nbsp;</span>Exponentially Weighted Moving Average</a></span><ul class=\"toc-item\"><li><span><a href=\"#-$-Val_t-=-\\beta-*-Val_{t-1}-+-(1-\\beta)-*-currentsample-$-\" data-toc-modified-id=\"-$-Val_t-=-\\beta-*-Val_{t-1}-+-(1-\\beta)-*-currentsample-$--14.2.1\"><span class=\"toc-item-num\">14.2.1&nbsp;&nbsp;</span> $ Val_t = \\beta * Val_{t-1} + (1-\\beta) * currentsample $ </a></span></li><li><span><a href=\"#Bias-correction-in-exponential-weighted-average\" data-toc-modified-id=\"Bias-correction-in-exponential-weighted-average-14.2.2\"><span class=\"toc-item-num\">14.2.2&nbsp;&nbsp;</span>Bias correction in exponential weighted average</a></span></li></ul></li><li><span><a href=\"#Gradient-descent-with-momentum\" data-toc-modified-id=\"Gradient-descent-with-momentum-14.3\"><span class=\"toc-item-num\">14.3&nbsp;&nbsp;</span>Gradient descent with momentum</a></span><ul class=\"toc-item\"><li><span><a href=\"#-$-V_{dw}-=-\\beta-*-V_{dw}-+-(1-\\beta)-*-dw-$-\" data-toc-modified-id=\"-$-V_{dw}-=-\\beta-*-V_{dw}-+-(1-\\beta)-*-dw-$--14.3.1\"><span class=\"toc-item-num\">14.3.1&nbsp;&nbsp;</span> $ V_{dw} = \\beta * V_{dw} + (1-\\beta) * dw $ </a></span></li><li><span><a href=\"#-$-W-=-W---\\alpha-*-V_{dw}-$-\" data-toc-modified-id=\"-$-W-=-W---\\alpha-*-V_{dw}-$--14.3.2\"><span class=\"toc-item-num\">14.3.2&nbsp;&nbsp;</span> $ W = W - \\alpha * V_{dw} $ </a></span></li></ul></li><li><span><a href=\"#RMSprop-(Root-Mean-Square-prop)\" data-toc-modified-id=\"RMSprop-(Root-Mean-Square-prop)-14.4\"><span class=\"toc-item-num\">14.4&nbsp;&nbsp;</span>RMSprop (Root Mean Square prop)</a></span><ul class=\"toc-item\"><li><span><a href=\"#-$-S_{dw}-=-\\beta-*-S_{dw}-+-(1-\\beta)-*-dw-*-dw-$-\" data-toc-modified-id=\"-$-S_{dw}-=-\\beta-*-S_{dw}-+-(1-\\beta)-*-dw-*-dw-$--14.4.1\"><span class=\"toc-item-num\">14.4.1&nbsp;&nbsp;</span> $ S_{dw} = \\beta * S_{dw} + (1-\\beta) * dw * dw $ </a></span></li><li><span><a href=\"#-$-W-=-W---\\alpha-*-\\frac-{dw}{\\sqrt{1--S_{dw}}}-$-\" data-toc-modified-id=\"-$-W-=-W---\\alpha-*-\\frac-{dw}{\\sqrt{1--S_{dw}}}-$--14.4.2\"><span class=\"toc-item-num\">14.4.2&nbsp;&nbsp;</span> $ W = W - \\alpha * \\frac {dw}{\\sqrt{1- S_{dw}}} $ </a></span></li></ul></li><li><span><a href=\"#Adam-Optimizer\" data-toc-modified-id=\"Adam-Optimizer-14.5\"><span class=\"toc-item-num\">14.5&nbsp;&nbsp;</span>Adam Optimizer</a></span><ul class=\"toc-item\"><li><span><a href=\"#-$-V_{dw}-=-\\beta_1-*-V_{dw}-+-(1-\\beta_1)-*-dw-$-\" data-toc-modified-id=\"-$-V_{dw}-=-\\beta_1-*-V_{dw}-+-(1-\\beta_1)-*-dw-$--14.5.1\"><span class=\"toc-item-num\">14.5.1&nbsp;&nbsp;</span> $ V_{dw} = \\beta_1 * V_{dw} + (1-\\beta_1) * dw $ </a></span></li><li><span><a href=\"#-$-S_{dw}-=-\\beta_2-*-S_{dw}-+-(1-\\beta_2)-*-dw-*-dw-$-\" data-toc-modified-id=\"-$-S_{dw}-=-\\beta_2-*-S_{dw}-+-(1-\\beta_2)-*-dw-*-dw-$--14.5.2\"><span class=\"toc-item-num\">14.5.2&nbsp;&nbsp;</span> $ S_{dw} = \\beta_2 * S_{dw} + (1-\\beta_2) * dw * dw $ </a></span></li><li><span><a href=\"#-$-W-=-W---\\alpha-*-\\frac-{V_{dw}}{\\sqrt{1--S_{dw}}}-$-\" data-toc-modified-id=\"-$-W-=-W---\\alpha-*-\\frac-{V_{dw}}{\\sqrt{1--S_{dw}}}-$--14.5.3\"><span class=\"toc-item-num\">14.5.3&nbsp;&nbsp;</span> $ W = W - \\alpha * \\frac {V_{dw}}{\\sqrt{1- S_{dw}}} $ </a></span></li></ul></li><li><span><a href=\"#Learning-rate-decay\" data-toc-modified-id=\"Learning-rate-decay-14.6\"><span class=\"toc-item-num\">14.6&nbsp;&nbsp;</span>Learning rate decay</a></span></li><li><span><a href=\"#Cyclical-learning-rate\" data-toc-modified-id=\"Cyclical-learning-rate-14.7\"><span class=\"toc-item-num\">14.7&nbsp;&nbsp;</span>Cyclical learning rate</a></span></li><li><span><a href=\"##layers-and-#hidden-units-hyper-parameters\" data-toc-modified-id=\"#layers-and-#hidden-units-hyper-parameters-14.8\"><span class=\"toc-item-num\">14.8&nbsp;&nbsp;</span>#layers and #hidden units hyper parameters</a></span></li><li><span><a href=\"#Train-dev-set-for-different-distribution-of-train,-dev-sets\" data-toc-modified-id=\"Train-dev-set-for-different-distribution-of-train,-dev-sets-14.9\"><span class=\"toc-item-num\">14.9&nbsp;&nbsp;</span>Train-dev set for different distribution of train, dev sets</a></span></li></ul></li><li><span><a href=\"#-Hyper-Parameter-Search-\" data-toc-modified-id=\"-Hyper-Parameter-Search--15\"><span class=\"toc-item-num\">15&nbsp;&nbsp;</span><font color=\"DarkOrange\"> Hyper Parameter Search </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#Grid-Search\" data-toc-modified-id=\"Grid-Search-15.1\"><span class=\"toc-item-num\">15.1&nbsp;&nbsp;</span>Grid Search</a></span></li><li><span><a href=\"#Random-Search-for-hyper-parameter-tuning\" data-toc-modified-id=\"Random-Search-for-hyper-parameter-tuning-15.2\"><span class=\"toc-item-num\">15.2&nbsp;&nbsp;</span>Random Search for hyper parameter tuning</a></span></li><li><span><a href=\"#Coarse-to-fine-sampling-of-hyper-param-search-space\" data-toc-modified-id=\"Coarse-to-fine-sampling-of-hyper-param-search-space-15.3\"><span class=\"toc-item-num\">15.3&nbsp;&nbsp;</span>Coarse to fine sampling of hyper param search space</a></span></li><li><span><a href=\"#Bayesian-Search\" data-toc-modified-id=\"Bayesian-Search-15.4\"><span class=\"toc-item-num\">15.4&nbsp;&nbsp;</span>Bayesian Search</a></span><ul class=\"toc-item\"><li><span><a href=\"#Sequential-Model-Based-Optimization-(SMBO)\" data-toc-modified-id=\"Sequential-Model-Based-Optimization-(SMBO)-15.4.1\"><span class=\"toc-item-num\">15.4.1&nbsp;&nbsp;</span>Sequential Model Based Optimization (SMBO)</a></span></li><li><span><a href=\"#Gaussian-Process\" data-toc-modified-id=\"Gaussian-Process-15.4.2\"><span class=\"toc-item-num\">15.4.2&nbsp;&nbsp;</span>Gaussian Process</a></span></li><li><span><a href=\"#Tree-Parzen-Estimators-(TPE)\" data-toc-modified-id=\"Tree-Parzen-Estimators-(TPE)-15.4.3\"><span class=\"toc-item-num\">15.4.3&nbsp;&nbsp;</span>Tree Parzen Estimators (TPE)</a></span></li><li><span><a href=\"#Bayesian-Search-Implementation\" data-toc-modified-id=\"Bayesian-Search-Implementation-15.4.4\"><span class=\"toc-item-num\">15.4.4&nbsp;&nbsp;</span>Bayesian Search Implementation</a></span></li></ul></li></ul></li><li><span><a href=\"#-Ensemble-Algorithms-\" data-toc-modified-id=\"-Ensemble-Algorithms--16\"><span class=\"toc-item-num\">16&nbsp;&nbsp;</span><font color=\"DarkOrange\"> Ensemble Algorithms </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#Voting\" data-toc-modified-id=\"Voting-16.1\"><span class=\"toc-item-num\">16.1&nbsp;&nbsp;</span>Voting</a></span></li><li><span><a href=\"#Bagging-or-Bootstrap-Aggregation\" data-toc-modified-id=\"Bagging-or-Bootstrap-Aggregation-16.2\"><span class=\"toc-item-num\">16.2&nbsp;&nbsp;</span>Bagging or Bootstrap Aggregation</a></span></li><li><span><a href=\"#Random-Forests\" data-toc-modified-id=\"Random-Forests-16.3\"><span class=\"toc-item-num\">16.3&nbsp;&nbsp;</span>Random Forests</a></span></li><li><span><a href=\"#AdaBoost\" data-toc-modified-id=\"AdaBoost-16.4\"><span class=\"toc-item-num\">16.4&nbsp;&nbsp;</span>AdaBoost</a></span></li><li><span><a href=\"#Gradient-Boosting\" data-toc-modified-id=\"Gradient-Boosting-16.5\"><span class=\"toc-item-num\">16.5&nbsp;&nbsp;</span>Gradient Boosting</a></span></li><li><span><a href=\"#XGBoost\" data-toc-modified-id=\"XGBoost-16.6\"><span class=\"toc-item-num\">16.6&nbsp;&nbsp;</span>XGBoost</a></span></li></ul></li><li><span><a href=\"#-Recurrent-Neural-Networks-(RNN)-\" data-toc-modified-id=\"-Recurrent-Neural-Networks-(RNN)--17\"><span class=\"toc-item-num\">17&nbsp;&nbsp;</span><font color=\"DarkOrange\"> Recurrent Neural Networks (RNN) </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#Why-RNNs?\" data-toc-modified-id=\"Why-RNNs?-17.1\"><span class=\"toc-item-num\">17.1&nbsp;&nbsp;</span>Why RNNs?</a></span></li><li><span><a href=\"#Simple-RNN\" data-toc-modified-id=\"Simple-RNN-17.2\"><span class=\"toc-item-num\">17.2&nbsp;&nbsp;</span>Simple RNN</a></span><ul class=\"toc-item\"><li><span><a href=\"#Simple-RNN-equations\" data-toc-modified-id=\"Simple-RNN-equations-17.2.1\"><span class=\"toc-item-num\">17.2.1&nbsp;&nbsp;</span>Simple RNN equations</a></span></li><li><span><a href=\"#Simplified-RNN-equations-notation\" data-toc-modified-id=\"Simplified-RNN-equations-notation-17.2.2\"><span class=\"toc-item-num\">17.2.2&nbsp;&nbsp;</span>Simplified RNN equations notation</a></span></li><li><span><a href=\"#Simple-RNN-representation\" data-toc-modified-id=\"Simple-RNN-representation-17.2.3\"><span class=\"toc-item-num\">17.2.3&nbsp;&nbsp;</span>Simple RNN representation</a></span></li></ul></li><li><span><a href=\"#BPTT-(Back-Prop-Through-Time)\" data-toc-modified-id=\"BPTT-(Back-Prop-Through-Time)-17.3\"><span class=\"toc-item-num\">17.3&nbsp;&nbsp;</span>BPTT (Back Prop Through Time)</a></span></li><li><span><a href=\"#Name-Entity-Recognition-(NER)\" data-toc-modified-id=\"Name-Entity-Recognition-(NER)-17.4\"><span class=\"toc-item-num\">17.4&nbsp;&nbsp;</span>Name Entity Recognition (NER)</a></span></li><li><span><a href=\"#Language-Modelling\" data-toc-modified-id=\"Language-Modelling-17.5\"><span class=\"toc-item-num\">17.5&nbsp;&nbsp;</span>Language Modelling</a></span><ul class=\"toc-item\"><li><span><a href=\"#Tokenization\" data-toc-modified-id=\"Tokenization-17.5.1\"><span class=\"toc-item-num\">17.5.1&nbsp;&nbsp;</span>Tokenization</a></span></li><li><span><a href=\"#Word-vs-Character-RNN\" data-toc-modified-id=\"Word-vs-Character-RNN-17.5.2\"><span class=\"toc-item-num\">17.5.2&nbsp;&nbsp;</span>Word vs Character RNN</a></span></li></ul></li><li><span><a href=\"#GRU-(Gated-Recurrent-Unit)\" data-toc-modified-id=\"GRU-(Gated-Recurrent-Unit)-17.6\"><span class=\"toc-item-num\">17.6&nbsp;&nbsp;</span>GRU (Gated Recurrent Unit)</a></span></li><li><span><a href=\"#LSTM-(Long-Short-Term-Memory)\" data-toc-modified-id=\"LSTM-(Long-Short-Term-Memory)-17.7\"><span class=\"toc-item-num\">17.7&nbsp;&nbsp;</span>LSTM (Long Short Term Memory)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Peephole-connection\" data-toc-modified-id=\"Peephole-connection-17.7.1\"><span class=\"toc-item-num\">17.7.1&nbsp;&nbsp;</span>Peephole connection</a></span></li></ul></li><li><span><a href=\"#BRNN-(Bi-directional-RNN)\" data-toc-modified-id=\"BRNN-(Bi-directional-RNN)-17.8\"><span class=\"toc-item-num\">17.8&nbsp;&nbsp;</span>BRNN (Bi-directional RNN)</a></span></li><li><span><a href=\"#Deep-RNN\" data-toc-modified-id=\"Deep-RNN-17.9\"><span class=\"toc-item-num\">17.9&nbsp;&nbsp;</span>Deep RNN</a></span></li></ul></li><li><span><a href=\"#-Word-Embedding-\" data-toc-modified-id=\"-Word-Embedding--18\"><span class=\"toc-item-num\">18&nbsp;&nbsp;</span><font color=\"DarkOrange\"> Word Embedding </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#Featurized-representation\" data-toc-modified-id=\"Featurized-representation-18.1\"><span class=\"toc-item-num\">18.1&nbsp;&nbsp;</span>Featurized representation</a></span></li><li><span><a href=\"#Cosine-Similarity\" data-toc-modified-id=\"Cosine-Similarity-18.2\"><span class=\"toc-item-num\">18.2&nbsp;&nbsp;</span>Cosine Similarity</a></span></li><li><span><a href=\"#Word2Vec\" data-toc-modified-id=\"Word2Vec-18.3\"><span class=\"toc-item-num\">18.3&nbsp;&nbsp;</span>Word2Vec</a></span><ul class=\"toc-item\"><li><span><a href=\"#Skip-gram\" data-toc-modified-id=\"Skip-gram-18.3.1\"><span class=\"toc-item-num\">18.3.1&nbsp;&nbsp;</span>Skip gram</a></span></li></ul></li><li><span><a href=\"#Negative-Sampling\" data-toc-modified-id=\"Negative-Sampling-18.4\"><span class=\"toc-item-num\">18.4&nbsp;&nbsp;</span>Negative Sampling</a></span></li></ul></li><li><span><a href=\"#-Sequence-Models-\" data-toc-modified-id=\"-Sequence-Models--19\"><span class=\"toc-item-num\">19&nbsp;&nbsp;</span><font color=\"DarkOrange\"> Sequence Models </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#Beam-Search\" data-toc-modified-id=\"Beam-Search-19.1\"><span class=\"toc-item-num\">19.1&nbsp;&nbsp;</span>Beam Search</a></span></li></ul></li><li><span><a href=\"#-Generative-Models-\" data-toc-modified-id=\"-Generative-Models--20\"><span class=\"toc-item-num\">20&nbsp;&nbsp;</span><font color=\"DarkOrange\"> Generative Models </font></a></span></li><li><span><a href=\"#-GAN-(Generative-Adversarial-Network)-\" data-toc-modified-id=\"-GAN-(Generative-Adversarial-Network)--21\"><span class=\"toc-item-num\">21&nbsp;&nbsp;</span><font color=\"DarkOrange\"> GAN (Generative Adversarial Network) </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#Generator-and-Descriminator\" data-toc-modified-id=\"Generator-and-Descriminator-21.1\"><span class=\"toc-item-num\">21.1&nbsp;&nbsp;</span>Generator and Descriminator</a></span></li><li><span><a href=\"#Generative-models-sample-target-probability-distribution\" data-toc-modified-id=\"Generative-models-sample-target-probability-distribution-21.2\"><span class=\"toc-item-num\">21.2&nbsp;&nbsp;</span>Generative models sample target probability distribution</a></span></li><li><span><a href=\"#GAN-training\" data-toc-modified-id=\"GAN-training-21.3\"><span class=\"toc-item-num\">21.3&nbsp;&nbsp;</span>GAN training</a></span><ul class=\"toc-item\"><li><span><a href=\"#Simple-GAN-training\" data-toc-modified-id=\"Simple-GAN-training-21.3.1\"><span class=\"toc-item-num\">21.3.1&nbsp;&nbsp;</span>Simple GAN training</a></span></li></ul></li><li><span><a href=\"#How-to-measure-GAN-performance?\" data-toc-modified-id=\"How-to-measure-GAN-performance?-21.4\"><span class=\"toc-item-num\">21.4&nbsp;&nbsp;</span>How to measure GAN performance?</a></span></li><li><span><a href=\"#Deep-Convolution-GAN-(DCGAN)\" data-toc-modified-id=\"Deep-Convolution-GAN-(DCGAN)-21.5\"><span class=\"toc-item-num\">21.5&nbsp;&nbsp;</span>Deep Convolution GAN (DCGAN)</a></span></li><li><span><a href=\"#InfoGAN\" data-toc-modified-id=\"InfoGAN-21.6\"><span class=\"toc-item-num\">21.6&nbsp;&nbsp;</span>InfoGAN</a></span></li><li><span><a href=\"#CGAN-(Conditional-Generative-Adversarial-Networks)\" data-toc-modified-id=\"CGAN-(Conditional-Generative-Adversarial-Networks)-21.7\"><span class=\"toc-item-num\">21.7&nbsp;&nbsp;</span>CGAN (Conditional Generative Adversarial Networks)</a></span></li><li><span><a href=\"#SRGAN-(Super-Resolution-GAN-)\" data-toc-modified-id=\"SRGAN-(Super-Resolution-GAN-)-21.8\"><span class=\"toc-item-num\">21.8&nbsp;&nbsp;</span>SRGAN (Super Resolution GAN )</a></span></li><li><span><a href=\"#ESRGAN\" data-toc-modified-id=\"ESRGAN-21.9\"><span class=\"toc-item-num\">21.9&nbsp;&nbsp;</span>ESRGAN</a></span></li><li><span><a href=\"#CycleGAN\" data-toc-modified-id=\"CycleGAN-21.10\"><span class=\"toc-item-num\">21.10&nbsp;&nbsp;</span>CycleGAN</a></span></li></ul></li><li><span><a href=\"#-VAE-(Variational-Auto-Encoders)-\" data-toc-modified-id=\"-VAE-(Variational-Auto-Encoders)--22\"><span class=\"toc-item-num\">22&nbsp;&nbsp;</span><font color=\"DarkOrange\"> VAE (Variational Auto Encoders) </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#Auto-Encoder\" data-toc-modified-id=\"Auto-Encoder-22.1\"><span class=\"toc-item-num\">22.1&nbsp;&nbsp;</span>Auto Encoder</a></span></li><li><span><a href=\"#VAE\" data-toc-modified-id=\"VAE-22.2\"><span class=\"toc-item-num\">22.2&nbsp;&nbsp;</span>VAE</a></span><ul class=\"toc-item\"><li><span><a href=\"#KL-divergence-loss\" data-toc-modified-id=\"KL-divergence-loss-22.2.1\"><span class=\"toc-item-num\">22.2.1&nbsp;&nbsp;</span>KL divergence loss</a></span></li></ul></li></ul></li><li><span><a href=\"#-Reinforcement-Learning-\" data-toc-modified-id=\"-Reinforcement-Learning--23\"><span class=\"toc-item-num\">23&nbsp;&nbsp;</span><font color=\"DarkOrange\"> Reinforcement Learning </font></a></span></li><li><span><a href=\"#-SOTA-CNN-Networks/Concepts-\" data-toc-modified-id=\"-SOTA-CNN-Networks/Concepts--24\"><span class=\"toc-item-num\">24&nbsp;&nbsp;</span><font color=\"DarkOrange\"> SOTA CNN Networks/Concepts </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#Classification-Networks\" data-toc-modified-id=\"Classification-Networks-24.1\"><span class=\"toc-item-num\">24.1&nbsp;&nbsp;</span>Classification Networks</a></span><ul class=\"toc-item\"><li><span><a href=\"#-DenseNet-\" data-toc-modified-id=\"-DenseNet--24.1.1\"><span class=\"toc-item-num\">24.1.1&nbsp;&nbsp;</span><font color=\"DarkMagenta\"> DenseNet </font></a></span></li><li><span><a href=\"#Inception\" data-toc-modified-id=\"Inception-24.1.2\"><span class=\"toc-item-num\">24.1.2&nbsp;&nbsp;</span>Inception</a></span></li><li><span><a href=\"#ResNet\" data-toc-modified-id=\"ResNet-24.1.3\"><span class=\"toc-item-num\">24.1.3&nbsp;&nbsp;</span>ResNet</a></span></li><li><span><a href=\"#ResNext\" data-toc-modified-id=\"ResNext-24.1.4\"><span class=\"toc-item-num\">24.1.4&nbsp;&nbsp;</span>ResNext</a></span></li><li><span><a href=\"#Highway-Net\" data-toc-modified-id=\"Highway-Net-24.1.5\"><span class=\"toc-item-num\">24.1.5&nbsp;&nbsp;</span>Highway Net</a></span></li><li><span><a href=\"#Pre-Activation-ResNet\" data-toc-modified-id=\"Pre-Activation-ResNet-24.1.6\"><span class=\"toc-item-num\">24.1.6&nbsp;&nbsp;</span>Pre-Activation ResNet</a></span></li><li><span><a href=\"#WRNs-(Wide-Residual-Networks)\" data-toc-modified-id=\"WRNs-(Wide-Residual-Networks)-24.1.7\"><span class=\"toc-item-num\">24.1.7&nbsp;&nbsp;</span>WRNs (Wide Residual Networks)</a></span></li><li><span><a href=\"#MobileNet\" data-toc-modified-id=\"MobileNet-24.1.8\"><span class=\"toc-item-num\">24.1.8&nbsp;&nbsp;</span>MobileNet</a></span></li><li><span><a href=\"#Xception\" data-toc-modified-id=\"Xception-24.1.9\"><span class=\"toc-item-num\">24.1.9&nbsp;&nbsp;</span>Xception</a></span></li><li><span><a href=\"#Spatial-Transformer-Network-(STN)\" data-toc-modified-id=\"Spatial-Transformer-Network-(STN)-24.1.10\"><span class=\"toc-item-num\">24.1.10&nbsp;&nbsp;</span>Spatial Transformer Network (STN)</a></span></li><li><span><a href=\"#Deformable-Convolutional-Networks-(DCN)\" data-toc-modified-id=\"Deformable-Convolutional-Networks-(DCN)-24.1.11\"><span class=\"toc-item-num\">24.1.11&nbsp;&nbsp;</span>Deformable Convolutional Networks (DCN)</a></span></li></ul></li></ul></li><li><span><a href=\"#-SOTA-Super-Resolution-Networks-\" data-toc-modified-id=\"-SOTA-Super-Resolution-Networks--25\"><span class=\"toc-item-num\">25&nbsp;&nbsp;</span><font color=\"DarkOrange\"> SOTA Super Resolution Networks </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#-Survey-of-SR-Techniques-\" data-toc-modified-id=\"-Survey-of-SR-Techniques--25.1\"><span class=\"toc-item-num\">25.1&nbsp;&nbsp;</span><font color=\"DarkMagenta\"> Survey of SR Techniques </font></a></span></li><li><span><a href=\"#SR-using-Residual-Channel-Attention-Networks-(RCAN)\" data-toc-modified-id=\"SR-using-Residual-Channel-Attention-Networks-(RCAN)-25.2\"><span class=\"toc-item-num\">25.2&nbsp;&nbsp;</span>SR using Residual Channel Attention Networks (RCAN)</a></span></li></ul></li><li><span><a href=\"#-New-DNN-Research-Concepts/Papers-\" data-toc-modified-id=\"-New-DNN-Research-Concepts/Papers--26\"><span class=\"toc-item-num\">26&nbsp;&nbsp;</span><font color=\"green\"> New DNN Research Concepts/Papers </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#-Attention-Mechanism-(Channel,-Spatial)-\" data-toc-modified-id=\"-Attention-Mechanism-(Channel,-Spatial)--26.1\"><span class=\"toc-item-num\">26.1&nbsp;&nbsp;</span><font color=\"blue\"> Attention Mechanism (Channel, Spatial) </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#CBAM:-Convolutional-Block-Attention-Module\" data-toc-modified-id=\"CBAM:-Convolutional-Block-Attention-Module-26.1.1\"><span class=\"toc-item-num\">26.1.1&nbsp;&nbsp;</span>CBAM: Convolutional Block Attention Module</a></span></li></ul></li><li><span><a href=\"#Recursive-Learning\" data-toc-modified-id=\"Recursive-Learning-26.2\"><span class=\"toc-item-num\">26.2&nbsp;&nbsp;</span>Recursive Learning</a></span><ul class=\"toc-item\"><li><span><a href=\"#DRCN\" data-toc-modified-id=\"DRCN-26.2.1\"><span class=\"toc-item-num\">26.2.1&nbsp;&nbsp;</span>DRCN</a></span></li><li><span><a href=\"#DRRN\" data-toc-modified-id=\"DRRN-26.2.2\"><span class=\"toc-item-num\">26.2.2&nbsp;&nbsp;</span>DRRN</a></span></li></ul></li><li><span><a href=\"#Transparency-by-Design-(TbD-Net)\" data-toc-modified-id=\"Transparency-by-Design-(TbD-Net)-26.3\"><span class=\"toc-item-num\">26.3&nbsp;&nbsp;</span>Transparency by Design (TbD Net)</a></span></li><li><span><a href=\"#Capsule-Net\" data-toc-modified-id=\"Capsule-Net-26.4\"><span class=\"toc-item-num\">26.4&nbsp;&nbsp;</span>Capsule Net</a></span></li><li><span><a href=\"#Zero-shot-learning\" data-toc-modified-id=\"Zero-shot-learning-26.5\"><span class=\"toc-item-num\">26.5&nbsp;&nbsp;</span>Zero shot learning</a></span></li><li><span><a href=\"#Federated-Learning\" data-toc-modified-id=\"Federated-Learning-26.6\"><span class=\"toc-item-num\">26.6&nbsp;&nbsp;</span>Federated Learning</a></span></li></ul></li><li><span><a href=\"#-Interview-Questions/Skills/JD-\" data-toc-modified-id=\"-Interview-Questions/Skills/JD--27\"><span class=\"toc-item-num\">27&nbsp;&nbsp;</span><font color=\"green\"> Interview Questions/Skills/JD </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#-Skills-required-for-ML/DL-Engineer-Jobs-\" data-toc-modified-id=\"-Skills-required-for-ML/DL-Engineer-Jobs--27.1\"><span class=\"toc-item-num\">27.1&nbsp;&nbsp;</span><font color=\"blue\"> Skills required for ML/DL Engineer Jobs </font></a></span></li></ul></li><li><span><a href=\"#-Best-Resources-\" data-toc-modified-id=\"-Best-Resources--28\"><span class=\"toc-item-num\">28&nbsp;&nbsp;</span><font color=\"green\"> Best Resources </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#Good-resources-list\" data-toc-modified-id=\"Good-resources-list-28.1\"><span class=\"toc-item-num\">28.1&nbsp;&nbsp;</span>Good resources list</a></span></li><li><span><a href=\"#Good-practical-resources\" data-toc-modified-id=\"Good-practical-resources-28.2\"><span class=\"toc-item-num\">28.2&nbsp;&nbsp;</span>Good practical resources</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# ML DL Notes\n",
    "- <font color=red> ** Use images/diagrams for clarity, update below sections with images** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red> Topics to learn </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=green> Learn Now (most important) </font>\n",
    "- [** ML Cheat sheet **](https://medium.com/machine-learning-in-practice/cheat-sheet-of-machine-learning-and-python-and-math-cheat-sheets-a4afe4e791b6)\n",
    "- Probability\n",
    "    - log likelihood\n",
    "    - Naive bayes\n",
    "    - MAP\n",
    "    - sampling methods\n",
    "- CNN layers theory\n",
    "    - Convolution types\n",
    "        - Depthwise separable convolution\n",
    "        - Shufflenet\n",
    "        - transposed convolution\n",
    "        - subpixel convolution\n",
    "        - dilation\n",
    "        - bottleneck layer\n",
    "        - global pooling\n",
    "        - total variation\n",
    "- Gradient descent, back propagation\n",
    "- SOTA CNNs\n",
    "    - network in network \n",
    "    - mobilenet\n",
    "    - squeeze net\n",
    "    - Highway network\n",
    "    - SR-GAN, ESRGAN\n",
    "- Hyper parameters and tuning\n",
    "    - Bayesian search: Gaussian process, TPE\n",
    "- Optimizers\n",
    "    - Adagard\n",
    "- DNN performance tuning\n",
    "    - [tutorial](https://medium.com/@jonathan_hui/improve-deep-learning-models-performance-network-tuning-part-6-29bf90df6d2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red> Learn later </font>\n",
    "- log odd ratio\n",
    "- Multiclass SVM\n",
    "- Kernel PCA\n",
    "- Weight normalization\n",
    "- Random theory for ML\n",
    "- instance normalization, layer normalization, group normalization\n",
    "- Regularization\n",
    "    - Elastic net\n",
    "- classification loss functions\n",
    "    - cross entropy loss function\n",
    "- generative and discriminative model\n",
    "- Gaussian Mixture Model (GMM)\n",
    "- Hidden Markov Models (HMM)    \n",
    "- confusion matrix\n",
    "- Precision and recall\n",
    "- ROI pooling\n",
    "- Deep learning studio\n",
    "- Teacher student training (knowledge distilation)\n",
    "- Curiculam learning\n",
    "- Multitask learning\n",
    "- Conditional Random Fields (CRF)\n",
    "- Study later\n",
    "    - Clusttering techniques\n",
    "    - svd\n",
    "    - lda\n",
    "    - binning, xgboost\n",
    "    - deep metric learning\n",
    "    - Deep Belief Network (DBN)\n",
    "- Hierarchical Clusttering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "The raw code for this IPython notebook is by default hidden for easier reading.\n",
       "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "The raw code for this IPython notebook is by default hidden for easier reading.\n",
    "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>.''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=DarkOrange> Probability and Statistics </font>\n",
    "- [Probability basics](https://towardsdatascience.com/probability-fundamentals-of-machine-learning-part-1-a156b4703e69)\n",
    "- Probability theory is a mathematical framework for quantifying our uncertainty about the world. It allows us to reason effectively in situations where being certain is impossible.\n",
    "- The **frequentist’s approach is that estimations come from experiments and experiments only**. If we want to estimate how likely a six sided die is to roll a 4, we should roll the die a number of times and observe the frequency in which a 4 appears. **This method works well when we have a large amount of data, but with fewer examples we can’t be confident in our estimates. **\n",
    "- <font color=blue> The popular estimation approach is Bayesian, A Bayesian treatment of probability allows us to combine our prior beliefs with our observations. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability distributions\n",
    "- [Probability distributions explained](https://towardsdatascience.com/probability-concepts-explained-probability-distributions-introduction-part-3-4a5db81858dc)\n",
    "- [PDF explained](https://mathinsight.org/probability_density_function_idea)\n",
    "- The set of all possible outcomes is called the sample space. <font color=blue> A random variable x, is a variable which randomly takes on values from a sample space. \n",
    "- A probability distribution is a list of all of the possible outcomes of a random variable along with their corresponding probability values. </font>\n",
    "- To describe the likelihood of each possible value of a random variable x, we specify a probability distribution.\n",
    "- Probability distributions are described differently depending on if the random variable is discrete or continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete distributions, Probability Mass Function (PMF)\n",
    "- Discrete random variables are described with a probability mass function (PMF).\n",
    "- <font color=blue> A PMF maps each value in the variable’s sample space to a probability. A PMF just returns the probability of the outcome x. An example, the probability of rolling a 3 is f(3) = 1/6. That’s it. </font>\n",
    "- One such PMF is the uniform distribution over n possible outcomes: P(X=x) = 1/n. This reads as “The probability of X taking on the value x is 1 divided by the number of possible values”. It’s called the uniform distribution because each outcome is equally likely\n",
    "- A loaded die is modeled by a categorical distribution, where each outcome is assigned a different probability.\n",
    "- Another common discrete distribution is the Bernoulli. A Bernoulli distribution specifies the probability for a random variable which can take on one of two values (1/0, heads/tails, true/false, rain/no rain, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous distributions, Probability Density Function (PDF)\n",
    "- Continuous random variables are described by probability density functions (PDF)\n",
    "- We generally indicate the PDF for a random variable X as f(X). <font color=blue> PDFs map an infinite sample space to relative likelihood values. \n",
    "- Continuous distributions takes infinitely possible values between a range, hence unlike PMF a PDF of a specific value is zero, hence a PDF is used to get a probability within a range, this the very important point of PDF. </font>\n",
    "- A normal distribution example to understand PDF, the Gaussian/normal distribution is parameterized by two values: the mean μ (mu) and variance σ² (sigma squared).\n",
    "- The functional form of PDF is, The left hand side of the equation says “The PDF of x given μ and σ² ”.\n",
    "<img src=\"images/pdf_normal.PNG\" width=\"400\">\n",
    "- Let’s plot this equation given μ = 0 and σ² = 4:\n",
    "<img src=\"images/pdf_normal_plot.PNG\" width=\"400\">\n",
    "- ** So what do I mean by relative likelihoods? Unlike discrete distributions, the value of the PDF at X = x is not the actual probability of x. ** Since there are infinitely many values that X could take on, the probability of x taking on any specific value is actually 0\n",
    "- Recall that the total probability for every possible value needs to sum to 1. How do we sum over an infinite number of values? The answer comes from calculus in the form of the integral. We can rewrite our axiom in terms of the PDF using the integral:\n",
    " <h3 align=\"center\"> $ \\int_{\\Omega} f(x) dx = 1 $ </h3>\n",
    "- we can find the probability that the outcome is between 0 and 1 by finding the area shown in the image below,\n",
    "<img src=\"images/pdf_normal_plot1.PNG\" width='300'>\n",
    "- Mathematically we would write this as,\n",
    " <h3 align=\"center\"> $ \\int_{0}^{1} f(x; \\mu, \\sigma) dx = P (0 < X < 1) $ </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Probability\n",
    "- [Tutorial](https://towardsdatascience.com/probability-concepts-explained-introduction-a7c0316de465)\n",
    "- **Marginal Probability**\n",
    "    - If A is an event, then the marginal probability is the probability of that event occurring, P(A)\n",
    "    - an example of a marginal probability is the probability that a card drawn from a pack is red: P(red) = 0.5\n",
    "- **Joint Probability**\n",
    "    - The probability of the intersection of two or more events.\n",
    "    - If A and B are two events then the joint probability of the two events is written as **P(A ∩ B)**\n",
    "    - Ex: the probability that a card drawn from a pack is red and has the value 4 is P(red and 4) = 2/52 = 1/26\n",
    "- **Conditional Probability**\n",
    "    - The conditional probability is the probability that some event(s) occur given that we know other events have already occurred\n",
    "    - If A and B are two events then the conditional probability of A occurring given that B has occurred is written as P(A|B)\n",
    "    - Ex: the probability that a card is a four given that we have drawn a red card is P(4|red) = 2/26 = 1/13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Central Limit Theorem (CLT)\n",
    "- [CLT Simple Explanation](http://blog.minitab.com/blog/understanding-statistics/how-the-central-limit-theorem-works)\n",
    "- Central limit theorem says that <font color=blue> if you have a sufficient number of randomly selected, independent samples (or observations), the means of those samples will follow a normal distribution </font> -- even if the population you're sampling from does not!\n",
    "- As shown in below figure, the samples of rolling die distribution is almost uniform (as each number of die has same probability 1/6 follows uniform distribution)\n",
    "- ** The means of consecutive samples (5 in the example) follows gaussian distribution, with more number of samples it comes close to gaussian **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bernoulli Distribution\n",
    "- The Bernoulli distribution is a model for an experiment that has only two possible outcomes. The corresponding experiment, which has only two possible outcomes is said to be a Bernoulli trial.\n",
    "- ** Bernoulli checks for specific outcome though the experiment has more outcomes.** Suppose a random experiment **has two outcomes**, namely Success and Failure.\n",
    "- example:\n",
    "    - In a single throw of a dice, the outcome \"5\" is called a success and any other outcome (not rolling a 5) is called a failure, then the successive throws of a dice will contain Bernoulli trials.The probability of success = 1/6 and the probability of failure = 5/6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binomial Theorem\n",
    "- [Binomial tutorial](https://www.analyticsvidhya.com/blog/2017/02/basic-probability-data-science-with-examples/)\n",
    "- ** binomial distribution is the discrete probability distribution of the number of success in a sequence of n independent Bernoulli trials (having only yes/no or true/false outcomes). see above tutorial with good example**\n",
    "- so, the Probability for getting k successes in n Bernoulli trails is given by:\n",
    "    - P(X=k) = n<sub>C<sub>k</sub></sub> p<sup>k</sup> q<sup>(n-k)</sup>,  [here p is the probability of success and q is the probability of failure]\n",
    "- If probability of each trail is equal, then the binomial probability distribution is also  equal as shown in graph.2\n",
    "- If we increase the number of trails, there will be many combinations and bars get thinner and thinner as shown in graph.3\n",
    "- What if we play infinite number of trails, ** the bars get infinitely small and the probability distribution looks something like a continuous set of bars which are very close **, almost continuous as shown in graph.4. This now becomes <font color=blue> a probability density function </font>\n",
    "- An observation is, **probability is highest at mean value** and decrease as we move away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Binomial example visualization: find the probability of winning series in 5 games\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import binom\n",
    "\n",
    "plt.figure(figsize=(13,3))\n",
    "trails = [5, 5, 20, 1000]\n",
    "win_prob = [0.75, 0.5, 0.5, 0.5]\n",
    "for i, prob in enumerate(win_prob):\n",
    "    num_games = trails[i]\n",
    "    binom_prob = binom(num_games, prob)\n",
    "\n",
    "    x = [i for i in range(0, num_games+1, 1)]\n",
    "    # plot    \n",
    "    plt.subplot(1,4,i+1)\n",
    "    plt.bar(x, binom_prob.pmf(x))  \n",
    "    plt.xlabel('number of trails')\n",
    "    if i == 0:\n",
    "        plt.ylabel('probability')\n",
    "        plt.title('UnEqual probability')\n",
    "    elif i == 1:\n",
    "        plt.title('equal probability')\n",
    "    else:\n",
    "        plt.title('equal prob. #more trails')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Normal Distribution or Z-distribution\n",
    "- [Z-distribution Introduction](https://study.com/academy/lesson/standard-normal-distribution-definition-example.html#)\n",
    "- [Z-distribution example](https://www.quora.com/Why-do-we-convert-normal-distribution-into-standard-normal-distribution)\n",
    "- In order <font color=blue> to make the proper comparisons among different normal distributions which are centered around their own averages, the distributions has to standardized to same scale to have a fair/correct comparison </font>\n",
    "- The standard normal distribution, commonly referred to the Z-distribution, is a special case of a normal distribution with the <font color=blue> following properties:\n",
    "    - It has a mean of zero.\n",
    "    - It has a standard deviation of one. \n",
    "- ** Z-scores: ** </font>\n",
    "    - **The distance in terms of number of standard deviations, the observed value is away from the mean, is the standard score or the Z score.**\n",
    "    - A score on the standard normal distribution is called a Z-Score. It should be interpreted as the number of standard deviations a data point is above or below the mean. A positive Z-Score indicates that a point is above the average, and a negative Z-Score indicates a score below the average.\n",
    "    - Data is rescaled such that mean (μ) = 0 and standard deviation (𝛔) = 1 as,\n",
    "    <h3 align=\"center\"> $ std(x) = \\frac{x - \\mu}{\\sigma} $ </h3>\n",
    "- ** Example **\n",
    "  - Happy gets 65 marks in Maths exam and Ekta gets 80 marks in English exam. Now, if we are asked to tell who performed better with respect to others ?\n",
    "  - But we can't compare these marks without knowing how others performed, that is to say they belong to different distributions\n",
    "  - We have below further information,\n",
    "    - Maths marks follow Normal distribution with mean 60 and sd 4\n",
    "    - English marks also follow Normal distribution with mean 79 and sd 2.\n",
    "  - Let us standardize the scores to same distribution (mean 0, stddev: 1),\n",
    "    - Z-score Happy (maths)  = (65-60)/4 = 1.25\n",
    "    - Z-score Ekta (english) = (80-79)/2 = 0.5\n",
    "  - So Z-score of Happy is higher than Ekta, that means which is above average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimation (MLE)\n",
    "- [Tutorial youtube](https://www.youtube.com/watch?v=XepXtl9YKwc)\n",
    "- While studying stats and probability, you must have come across problems like, What is the probability of x > 100, given that x follows a normal distribution with mean 50 and standard deviation (sd) 10. In such problems, we already know the distribution (normal in this case) and its parameters (mean and sd).\n",
    "- But in real life problems these quantities are unknown and must be estimated from the data. <font color=blue> MLE is a technique used for estimating the parameters of the distribution that best describe the given data </font>.\n",
    "- ** MLE can be defined as a method for estimating population parameters (such as the mean and variance for Normal, rate (lambda) for Poisson, etc.) from sample data such that the probability (likelihood) of obtaining the observed data is maximized. **\n",
    "- The goal of MLE is to maximize the likelihood function:\n",
    "- ** Example **\n",
    "- To find a normal distribution parameters (mean, std-dev) for given input data\n",
    "\n",
    "<img src=\"images/max_likelihood_mean.PNG\" width=\"300\" align=\"left\">\n",
    "<img src=\"images/max_likelihood_stddev.PNG\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Probability\n",
    "- [Tutorial](https://medium.com/@mithunmanohar/machine-learning-101-what-the-is-a-conditional-probability-f0f9a9ec6cda)\n",
    "- The conditional probability of an event A is the probability of an event ( A ), given that another event ( B ) has already occurred.\n",
    "<img src=\"images/conditional_probability.PNG\" width=\"300\">\n",
    "- Independent events are events that do not affect the outcome of each other. For independent activities the probability does not change p(A|B)=p(A)\n",
    "- For dependent activities, p(A|B) = p(A ^ B)/p(B)\n",
    "\n",
    "---\n",
    "- Set A represents one set of events and Set B represents another. We wish to calculate the probability of A given B has already happened. Lets represent the happening of event B by shading it with red.\n",
    "<img src=\"images/conditional_probability_venn.PNG\" width=\"300\">\n",
    "- <font color=blue> Now since B has happened, the part which now matters for A is the part shaded in blue which is interestingly A^B, so condition probability can be written as, </font>\n",
    "<img src=\"images/conditional_probability_equ.PNG\" width=\"400\">\n",
    "\n",
    "- ** Example: **\n",
    "- Probability of drawing red card with value 4?\n",
    "- There are two events, p(red) = 26, p(4) = 4\n",
    "- If B already occured, then sample space reduce to p(B) and p(red^4)=2\n",
    "- so p(A|B) = 2/26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Theorem\n",
    "- [Bayes Intro](https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/)\n",
    "- [Bayes Intution, Example](https://betterexplained.com/articles/an-intuitive-and-short-explanation-of-bayes-theorem/)\n",
    "- [Bayes Example](https://medium.com/bright-minds-analytica/bayes-theorem-explained-66f572b875f6)\n",
    "- [Bayes good example](http://www.lichun.cc/blog/2013/07/understand-bayes-theorem-prior-likelihood-posterior-evidence/)\n",
    "- [Bayes Example: Rich people given happy](https://www.quora.com/What-is-an-intuitive-explanation-of-Bayes-Rule)\n",
    "- [Bayesian inference](https://towardsdatascience.com/probability-concepts-explained-bayesian-inference-for-parameter-estimation-90e8930e5348)\n",
    "\n",
    "- <font color=blue> Bayesian statistics is a mathematical procedure that applies probabilities to statistical problems. It provides people the tools to update their beliefs in the evidence of new data. </font>\n",
    "- Bayes’ Theorem is an important mathematical tool for calculating the conditional probability of an event using the probabilities of other related events. \n",
    "\n",
    "<h3 align=\"center\"> $ Posterior = \\frac {Likelihood * Prior}{Evidence} $ </h3>\n",
    "<img src=\"images/bayes_equ.PNG\" width=\"500\">\n",
    "\n",
    "---\n",
    "- ** Example-1 **\n",
    "    - a cancer testing scenario:\n",
    "        - 1% of women have breast cancer (and therefore 99% do not).\n",
    "        - 80% of mammograms detect breast cancer when it is there (and therefore 20% miss it).\n",
    "        - 9.6% of mammograms detect breast cancer when it’s not there (and therefore 90.4% correctly return a negative result).\n",
    "        - Put in a table, the probabilities look like this:        \n",
    "<img src=\"images/bayes_ex1_table1.PNG\" width=\"500\">\n",
    "        - How do we read it?\n",
    "            - 1% of people have cancer\n",
    "            - If you already have cancer, you are in the first column. There’s an 80% chance you will test positive. There’s a 20% chance you will test negative.\n",
    "            - If you don’t have cancer, you are in the second column. There’s a 9.6% chance you will test positive, and a 90.4% chance you will test negative.\n",
    "        - Now **suppose you get a positive test result. What are the chances you have cancer?** 80%? 99%? 1%?\n",
    "            - Hear's how we think about it,\n",
    "            - Ok, we got a positive result. It means we’re somewhere in the top row of our table. Let’s not assume anything — it could be a true positive or a false positive.\n",
    "<font color=blue> The chances of a true positive = chance you have cancer * chance test caught it = 1% * 80% = .008\n",
    "            - The chances of a false positive = chance you don’t have cancer * chance test caught it anyway = 99% * 9.6% = 0.09504 </font>\n",
    "<img src=\"images/bayes_ex1_table2.PNG\" width=\"500\">\n",
    "        - <font color=blue> what’s the chance we really have cancer if we get a positive result. The chance of an event is the number of ways it could happen given all possible outcomes: $Probability = \\frac{desired \\; event}{all \\; possibilities} $ </font>\n",
    "            - The chance of getting a real, positive result is .008. The chance of getting any type of positive result is the chance of a true positive plus the chance of a false positive (.008 + 0.09504 = .10304).\n",
    "            - So, our chance of cancer is .008/.10304 = 0.0776, or about 7.8%.\n",
    "        - <font color=blue> ** We can turn the process above into an equation, understand and map the terms **, which is Bayes’ Theorem. </font>\n",
    "        <img src=\"images/bayes_ex1_table3.PNG\" width=\"500\">\n",
    "        - And here’s the decoder key to read it:\n",
    "            - <font color=blue> Pr(H|E) = Chance of having cancer (H) given a positive test (E). This is what we want to know: How likely is it to have cancer with a positive result? In our case it was 7.8%.\n",
    "            - Pr(E|H) = Chance of a positive test (E) given that you had cancer (H). This is the chance of a true positive, 80% in our case.\n",
    "            - Pr(H) = Chance of having cancer (1%).\n",
    "            - Pr(not H) = Chance of not having cancer (99%).\n",
    "            - Pr(E|not H) = Chance of a positive test (E) given that you didn’t have cancer (not H). This is a false positive, 9.6% in our case. </font>\n",
    "        - It all comes down to the chance of a true positive divided by the chance of any positive. We can simplify the equation to:\n",
    "            <img src=\"images/bayes_ex1_table4.PNG\" width=\"400\">\n",
    "            - Pr(E) tells us the chance of getting any positive result, whether a true positive in the cancer population (1%) or a false positive in the non-cancer population (99%). \n",
    "\n",
    "---\n",
    "- ** Example-2 **\n",
    "    - Suppose we have 100 movies and 50 books.\n",
    "        - There are 3 different movie types: Action, Sci-fi, Romance,\n",
    "        - 2 different book types: Sci-fi, Romance\n",
    "        - 20 of those 100 movies are Action.\n",
    "        - 30 are Sci-fi\n",
    "        - 50 are Romance.\n",
    "        - 15 of those 50 books are Sci-fi\n",
    "        - 35 are Romance\n",
    "    - The probability that it's a movie is 100/150, 50/150 for book.\n",
    "    - The probability that it's a Sci-fi type is 45/150, 20/150 for Action and 85/150 for Romance.\n",
    "    - If we already know it's a movie, then the probability that it's an action movie is 20/100, 30/100 for Sci-fi and 50/100 for Romance.\n",
    "    - If we already know it's a book, then that probability that it's an Sci-fi book is 15/50, 35/50 for Romance.\n",
    "    - Right now, we want **to know that given an object which has type Sci-fi, what the probability is if it’s a movie?**\n",
    "        - <font color=blue> We can calculate it based on above data, total sci-fi count is 45 and out of that movies are 30, so prob (movie | given sci-fi)= $\\frac{30}{45}$. Same thing is computed by Bayes. </font>\n",
    "        - Using Bayes theorem, we know that the formula is:\n",
    "        $ P(movie \\;|\\; Sci-fi) = \\frac{P(Sci-fi \\;|\\; Movie) * P(Movie) }{ P(Sci-fi)} $\n",
    "            - Here, P(movie|Sci-fi) is called Posterior,\n",
    "            - P(Sci-fi|Movie) is Likelihood,\n",
    "            - P(movie) is Prior,\n",
    "            - P(Sci-fi) is Evidence.\n",
    "            - <font color=blue> $ P(movie \\;|\\; Sci-fi) = \\frac{\\frac{30}{100}*\\frac{100}{150}}{\\frac{45}{150}} = \\frac{30}{45} $ which same as above </font>\n",
    "- ** Prior: ** Before we observe it’s a Sci-fi type, the object is completely unknown to us. Our goal is to find out the possibility that it’s a movie, we actually have the data prior(or before) our observation, which is the possibility that it’s a movie if it’s a completely unknown object: P(movie).\n",
    "- ** Posterior: ** After we observed it’s a Sci-fi type, we know something about the object. Because it’s post(or after) the observation, we call it posterior: P(movie|Sci-fi).\n",
    "- ** Evidence: ** Because we’ve already known it’s a Sci-fi type, what has happened is happened. We witness it’s appearance, so to us, it’s an evidence, and the chance we get this evidence is P(Sci-fi).\n",
    "- ** Likelihood: ** The dictionary meaning of this word is chance or probability that one thing will happen.**Here it means when it’s a movie, what the chance will be if it is also a Sci-fi type.** This term is very important in Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=DarkOrange> ML/DL basics introduction </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron\n",
    "- Takes several 'binary' inputs to produce a 'binary' output by comparing the weighted sum of inputs against a threshold, \n",
    "- The threshold is treated as bias and moved to left side of equation to compare weighted sum against zero\n",
    "- If a small change in a weight or bias causes only a small change in output, it is possible for a network to learn. \n",
    "- But, this doesn't happen with perceptrons sometimes as <font color=blue>small change in weights can entirely flip the output</font> from say 1 to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametric Vs Non-Parametric algorithms\n",
    "- Parametric\n",
    "    - Parametric methods makes an assumption about the form of the function relating X and Y\n",
    "    - Linear regression is a parametric method\n",
    "- Non-Parametric\n",
    "    - non-parametric learners do not have a model structure specified a priori. \n",
    "    - We don’t speculate about the form of the function f that we are trying to learn before training the model, as we did previously with linear regression. \n",
    "    - Instead, the model structure is purely determined from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Neuron\n",
    "- Sigmoid neurons are similar to perceptrons (shape is a smoothed out version of a step function), <br> but modified so that **small changes in their weights and bias cause only a small change in their output**\n",
    "- instead of being just 0 or 1, these inputs can also take on any values between 0 and 1\n",
    "- output is not 0 or 1. Instead, it's σ(w⋅x+b), where σ is called the sigmoid function\n",
    "- Somewhat confusingly, and for historical reasons, such multiple layer networks are sometimes called multilayer perceptrons or MLPs, despite being made up of sigmoid neurons, not perceptrons. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient decent\n",
    "- To quantify how well we're achieving this goal we define a cost function\n",
    "- to find a set of weights and biases which make the cost as small as possible. We'll do that using an algorithm known as gradient descent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "- One of the problems that occur during neural network training is called overfitting. \n",
    "- The error on the training set is driven to a very small value, but when new data is presented to the network the error is large. The network has <font color=blue>memorized the training examples, but it has not learned to generalize to new situations</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to avoid overfitting\n",
    "- Go for simpler models over more complicated models. Generally, the **fewer parameters** that you have to tune the better. \n",
    "- Use **more data** to train the model. \n",
    "- Some sort of <font color= blue>regularization</font> can help penalize certain sources of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing Gradients\n",
    "- if a change in the parameter's value causes very small change in the network's output - the network just can't learn the parameter effectively, which is a problem.\n",
    "-  For example, **sigmoid maps the real number line onto a \"small\" range** of [0, 1]. As a result, there are large regions of the input space which are mapped to an extremely small range. In these regions of the input space, even a large change in the input will produce a small change in the output - hence the gradient is small.\n",
    "- This **becomes much worse when we stack multiple layers** of such non-linearities on top of each other. <br> For instance, first layer will map a large input region to a smaller output region, which will be mapped to an even smaller region by the second layer, which will be mapped to an even smaller region by the third layer and so on. ** As a result, even a large change in the parameters of the first layer doesn't change the output much **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to avoid vanishing gradients\n",
    "- We can avoid this problem by using activation functions which don't have this property of 'squashing' the input space into a small region. \n",
    "- A popular <font color=blue>choice is Rectified Linear Unit</font> which maps x to max(0,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "- **Cross validation is a method for estimating the prediction accuracy of a model.**\n",
    "- One way to evaluate a model is to see how well it predicts the data used to fit the model. But this is too optimistic -- a model tailored to a particular data set will make better predictions on that data set than on new data. \n",
    "- Another way is to hold out some data and fit the model using the rest. Then you can test your accuracy on the holdout data.  But the held out data is \"wasted\" from the point of view of building the model. If you have huge amounts of data, so holding some data out won't make the model much worse\n",
    "- Cross validation does something like this but tries to <font color=blue>make more efficient use of the data</font>: you divide the data into (say) 10 equal parts. Then **successively hold out each part and fit the model using the rest**. This gives you 10 estimates of prediction accuracy which can be combined into an overall measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of data\n",
    "- ** Categorical**: Categorical variables take on values that are names or labels. The colour of a ball (e.g., red, green, blue) or the breed of a dog (e.g., collie, shepherd, terrier) would be examples of categorical variables.\n",
    "- ** Quantitative **: Quantitative variables are numerical. They represent a measurable quantity. For example, when we speak of the population of a city, we are talking about the number of people in the city — a measurable attribute of the city. Therefore population would be a quantitative variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification and Regression\n",
    "- So in very simple terms, classification is about predicting a label and regression is about predicting a quantity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=DarkOrange> Supervised Learning </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=DarkMagenta> Parametric and Classification Algorithms </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM)\n",
    "- [SVM Overview](https://towardsdatascience.com/support-vector-machines-a-brief-overview-37e018ae310f)\n",
    "- [SVM kernel trick example](https://medium.com/machine-learning-for-humans/supervised-learning-2-5c1c23f3560d)\n",
    "- SVM is also a <font color=blue> binary classifier </font> (classifies 2 classes) like logistic regression\n",
    "- Support vector machines attempt to pass a <font color=blue> linearly separable hyperplane through a dataset in order to classify the data into two groups </font>\n",
    "- This hyperplane is a linear separator for any dimension; it could be a line (2D), plane (3D), and hyperplane (4D+)\n",
    "- the <font color=blue> best hyperplane is the one that maximizes the margin </font>. The margin is the distance between the hyperplane and a few close points. These <font color=blue> close points are the support vectors because they control the hyperplane. </font>\n",
    "- The classes have to be linearly separable to be classified using SVM, a variant of SVM is proposed to classify the data's which are not perfectly separable, it is known as a <font color=blue> Soft Margin Classifier or a Support Vector Classifier </font>, which allows slight mis-classification. SVM classifier contains a tuning parameter in order to control how much misclassification it will allow\n",
    "- **Kernel Trick**:\n",
    "    - The non-linear lower feature space from lower dimension is transformed to higher dimension to classify non-linear, which is known as kernel trick\n",
    "    - these kernels transform our data in order to pass a linear hyperplane and thus classify our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "- The idea of <font color=blue>Logistic Regression is to find a relationship between features and probability of particular outcome.</font>\n",
    "-  Logistic regression works largely the same way linear regression works: it multiplies each input by a coefficient, sums them up, and adds a constant. <font color=blue> In logistic regression, however, the output is actually the log of the odds ratio. </font>\n",
    "- This type of a problem is referred to as **Binomial Logistic Regression**, where the response variable has two values 0 and 1 or pass and fail or true and false. **Multinomial Logistic Regression deals** with situations where the response variable can have three or more possible values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=DarkMagenta> Parametric and Regression Algorithms </font>\n",
    "- **Regression is a statistical way to establish a relationship between a dependent variable and a set of independent variable(s)**\n",
    "- Regression is concerned with modeling the relationship between variables that is iteratively refined using a measure of error in the predictions made by the model.\n",
    "- Regression methods are a workhorse of statistics and have been co-opted into statistical machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression\n",
    "- While doing linear regression our objective is to **fit a line through the distribution which is nearest to most of the points**. Hence reducing the distance (error term) of data points from the fitted line. \n",
    "- It is conventional to use squares, as Regression line minimizes the sum of “Square of Residuals”. \n",
    "- That’s why the method of Linear Regression is <font color=blue> known as “Ordinary Least Square (OLS)”</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=DarkMagenta> Non-Parametric Algorithms </font>\n",
    "- [Non-Parametric Supervised Learning Tutorial](https://medium.com/machine-learning-for-humans/supervised-learning-3-b1551b9c4930)\n",
    "- non-parametric learners do not have a model structure specified a priori. We don’t speculate about the form of the function f that we’re trying to learn before training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbor (KNN)\n",
    "- You look at the k closest data points and take the average of their values if variables are continuous (like housing prices), or the mode if they’re categorical (like cat vs. dog)\n",
    "- **Choosing k:** tuning hyperparameters with cross-validation\n",
    "    - To decide which value of k to use, you can **test different k-NN models** using different values of k with cross-validation\n",
    "    - Pick whichever yields the lowest error, on average, across all iterations\n",
    "- Higher values of k help address overfitting, but if the value of k is too high your model will be very biased and inflexible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "- Making a good decision tree is like playing a game of “20 questions”\n",
    "- There are ways to quantify information gain so that you can essentially evaluate every possible split of the training data and maximize information gain for every split\n",
    "- **Choosing splits in a decision tree**\n",
    "    - **Entropy is the amount of disorder in a set** (measured by Gini index or cross-entropy)\n",
    "    - If the values are really mixed, there’s lots of entropy; if you can cleanly split values, there’s no entropy.\n",
    "    - **For every split at a parent node, you want the child nodes to be as pure as possible (minimize entropy.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest:  an ensemble of decision trees\n",
    "- A model comprised of many models is called an **ensemble model**, and this is usually a winning strategy.\n",
    "- A single decision tree can make a lot of wrong calls because it has very black-and-white judgments. \n",
    "- A random forest is a meta-estimator that aggregates many decision trees, with some helpful modifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=DarkOrange> Unsupervised Learning </font>\n",
    "- [Unsupervised Tutorial](https://medium.com/machine-learning-for-humans/unsupervised-learning-f45587588294)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=DarkMagenta> Clusttering </font>\n",
    "- [Top5 Clustterings](https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68)\n",
    "- The goal of clustering is to create groups of data points such that points in different clusters are dissimilar while points within a cluster are similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means clusttering\n",
    "- [K-means tutorial](https://towardsdatascience.com/clustering-using-k-means-algorithm-81da00f156f6)\n",
    "- k-means clustering works on unlabeled data to cluster our data points into k groups. \n",
    "- A larger k creates smaller groups with more granularity, a lower k means larger groups and less granularity.\n",
    "- The output of the algorithm would be a set of “labels” assigning each data point to one of the k groups.\n",
    "- In k-means clustering, the way these **groups are defined is by creating a centroid** for each group. The centroids are like the heart of the cluster, they “capture” the points closest to them and add them to the cluster.\n",
    "- **K-means algorithm steps**\n",
    "    1. **Define the k centroids.**\n",
    "        - Initialize these at random (there are also fancier algorithms for initializing the centroids that end up converging more effectively).\n",
    "    2. **Find the closest centroid & update cluster assignments.**\n",
    "        - Assign each data point to one of the k clusters. Each data point is assigned to the nearest centroid’s cluster. Here, the measure of “nearness” is a hyperparameter — often **Euclidean distance**.\n",
    "        - If we’re using the Euclidean distance between data points and every centroid, a straight line is drawn between two centroids, then a perpendicular bisector (boundary line) divides this line into two clusters\n",
    "    3. **Move the centroids to the center of their clusters.**\n",
    "        - The new position of each centroid is calculated as the average position of all the points in its cluster.\n",
    "    - Keep repeating steps 2 and 3 until the centroid stop moving a lot at each iteration (i.e., until the algorithm converges)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expectation–Maximization (EM) Clustering using Gaussian Mixture Models (GMM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Clusttering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=DarkMagenta> Dimensionality Reduction </font>\n",
    "- Dimensionality reduction looks a lot like compression. This is about trying to reduce the complexity of the data while keeping as much of the relevant structure as possible.\n",
    "- reducing the dimension of the feature space is called “dimensionality reduction.” There are many ways to achieve dimensionality reduction, but most of these techniques fall into one of two classes:\n",
    "    - Feature Elimination\n",
    "    - Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)\n",
    "- [PCA Tutorial with example](https://georgemdallas.wordpress.com/2013/10/30/principal-component-analysis-4-dummies-eigenvectors-eigenvalues-and-dimension-reduction/)\n",
    "- [PCA Tutorial](https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c)\n",
    "- The principal components are the underlying structure in the data. They represent the directions in which the data has maximum variance and also the directions in which the data is most spread out.\n",
    "- Principal component analysis is a technique for feature extraction — so it combines our input variables in a specific way, then we can drop the “least important” variables while still retaining the most valuable parts of all of the variables!\n",
    "- As an added benefit, each of the “new” variables **after PCA are all independent of one another**. This is a benefit because the assumptions of a linear model require our independent variables to be independent of one another\n",
    "- **Eigenvector and Eigenvalue**\n",
    "    - eigenvector was the direction of the line drawn to find the maximum variance\n",
    "    - eigenvalue was a number that tells us how the data set is spread out on the line which is an eigenvector.\n",
    "    - **amount of eigenvectors that exist equals the number of dimensions the data set has** are perpendicular/orthogonal to each other\n",
    "    - <font color=blue> The big eigen vector (with highest variance) is the principal component </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition (SVD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=DarkOrange> Loss Functions </font>\n",
    "- All the algorithms in machine learning rely on minimizing or maximizing a function, which we call “**objective function**”. The group of functions that are minimized are called “loss functions”. \n",
    "- A loss function is a measure of how good a prediction model does in terms of being able to predict the expected outcome. \n",
    "- A most commonly used method of finding the minimum point of function is “gradient descent”.\n",
    "- Loss functions can be broadly categorized into 2 types: <font color=blue>Classification loss and Regression Loss</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=DarkMagenta> Classification Loss Functions </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Square loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hinge loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross entropy loss\n",
    "- [Cross Entropy Tutorial](https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss/)\n",
    "- [Cross Entropy Intution Example](https://towardsdatascience.com/intuitive-explanation-of-cross-entropy-5d45fc9fd240)\n",
    "- if you have access to the underlying distribution  y , then to use the smallest number of bits on average, you should assign $\\log(\\frac{1}{y_i})$ bits to the  i-th symbol, where yi is the probability of i-th symbol.\n",
    "- By fully exploiting the known distribution  y  in this way, we achieve an optimal number of bits per transmission. \n",
    "- <font color=blue> The optimal number of bits is known as entropy. </font> Mathematically, it's just the expected number of bits under this optimal encoding:\n",
    "<h4 align=\"center\"> $ H(y) = \\sum\\limits_{i}  {(y_i \\log(\\frac{1}{y_i}))} = -\\sum\\limits_{i}  {(y_i \\log({y_i}))} $ </h4>\n",
    "\n",
    "- If we think of a distribution as the tool we use to encode symbols, then entropy measures the number of bits we'll need if we use the correct tool  y . This is optimal, in that we can't encode the symbols using fewer bits on average.\n",
    "- In contrast, <font color=blue>cross entropy is the number of bits we'll need if we encode symbols from  y  using the wrong tool  $\\hat{y}$. This consists of encoding the  i-th symbol using  $\\log(\\frac{1}{\\hat{y_i}}$)  bits instead of $\\log(\\frac{1}{y_i})$ bits. </font>\n",
    "\n",
    "- Cross entropy is always larger than entropy; encoding symbols according to the wrong distribution  y^  will always make us use more bits. The only exception is the trivial case where  y  and  y^  are equal, and in this case entropy and cross entropy are equal.\n",
    "\n",
    "---\n",
    "- **Cross Entropy Intution**\n",
    "- a coin with p probability takes log (1/p) questions to get it right. For example, when p = 1/4, log(4) = 2 questions (all logarithms are based 2 in this post). So in total, the expected number of questions for this game is as given below (which is nothing but the entropy, that is optimal number of questions),\n",
    "<h4 align=\"center\"> $ \\sum\\limits_{i}  {(p_i \\log(\\frac{1}{p_i}))} $ </h4>\n",
    "\n",
    "- Thus, cross entropy for a given strategy is simply the expected number of questions to guess the color under that strategy. \n",
    "\n",
    "---\n",
    "- **Binary Cross Entropy**\n",
    "- In a binary classification problem, the likelihood that the label y is 1 is your predicted y is $\\hat{y}$, the likelihood that y is 0 is (1 - $\\hat{y}$), since total sum of probabilities is 1. So we can write the likelihood we want to maximize in a clever way,\n",
    "<h4 align=\"center\"> $ \\hat{y}^y * (1 - \\hat{y})^{(1 - y)} $ </h4>\n",
    "\n",
    "- <font color=blue> When y is 1, the second term in the product is 1 and we want to maximize $\\hat{y}$ when y is 0, the first term in the product is 1 and we want to maximize (1 — $\\hat{y}$). This only works if y takes on value of 0 or 1 only. </font>\n",
    "\n",
    "---\n",
    "- ** Log likelihood **\n",
    "- The log of above binary likelihood can be written as below,\n",
    "<h4 align=\"center\"> $ y * \\log{\\hat{y}} + (1 - \\hat{y}) * \\log(1 - \\hat{y}) $ </h4>\n",
    "- Maximizing the log of the likelihood is equivalent of minimizing,\n",
    "<h4 align=\"center\"> $ -y * \\log{\\hat{y}} - (1 - \\hat{y}) * \\log(1 - \\hat{y}) $ </h4>\n",
    "\n",
    "- <font color=blue> this is just the expression for cross entropy. This is why cross entropy is called log loss. Minimizing cross entropy maximizes the log likelihood. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic loss\n",
    "- Logistic prediction equation: $\\hat{y}$ = Sigmoid(Wi * x + b)\n",
    "- y is actual probability\n",
    "- Then the logistic loss as define above in cross entropy section,\n",
    "<h4 align=\"center\"> $Loss (y, \\hat{y}) =  -y * \\log{\\hat{y}} - (1 - \\hat{y}) * \\log(1 - \\hat{y}) $ </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KL Divergence\n",
    "- The KL divergence from $\\hat{y}$ to y is simply the difference between cross entropy and entropy.\n",
    "- It measures the number of extra bits we'll need on average if we encode symbols from  y  according to $\\hat{y}$\n",
    "- Note that minimizing cross entropy is the same as minimizing the KL divergence from $\\hat{y}$ to y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=DarkMagenta> Regression Loss Functions </font>\n",
    "- [5 regression loss funtioncs](https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 Loss, Mean Square Error (MSE), Quadratic loss\n",
    "- Mean Square Error (MSE) is the most commonly used regression loss function. MSE is the sum of squared distances between our target variable and predicted values.\n",
    "\n",
    "<h4 align=\"center\"> $ MSE = \\sum\\limits_{i=1}^n  {(y_i - y_i^p)}^2 $ </h4>\n",
    "\n",
    "<h4 align=\"center\"> $ PSNR = 10 log \\frac{L^2}{MSE} $ </h4>\n",
    "            - L is the max pixel value, for 8-bit data L is 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 Loss, Mean Absolute Error (MAE)\n",
    "- MAE is the sum of absolute differences between our target and predicted variables. \n",
    "- So it ** measures the average magnitude of errors ** in a set of predictions, without considering their directions. \n",
    "- If we consider directions also, that would be called ** Mean Bias Error (MBE) **\n",
    "\n",
    "<h4 align=\"center\"> $ MAE = \\sum\\limits_{i=1}^n  {|y_i - y_i^p|} $ </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 vs L2 Loss\n",
    "- [L1 vs L2 Comparison](http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/)\n",
    "- <font color=blue>L1 loss is more robust to outliers than L2</font>,\n",
    "    - Since MSE squares the error (y — y_predicted = e), the value of error (e) increases a lot if e > 1. \n",
    "    - If we have an outlier in our data, the value of e will be high and e² will be >> |e|. \n",
    "    - This will make the model with **MSE loss give more weight to outliers ** than a model with MAE loss.\n",
    "    - MAE loss is useful if the training data is corrupted with outliers\n",
    "- <font color=blue>L1 loss derivative is not continuous, hence inefficient to find solution, i.e. unstable</font>,\n",
    "    - which can lead to missing minima\n",
    "    - As L2 derivative is continuous, it gives more stable solution, however it not robust in case of outliers\n",
    "- <font color=blue>Issue with L1 and L2 loss functions:</font>\n",
    "    - There can be cases where neither loss function gives desirable predictions. \n",
    "    - **For example,** if 90% of observations in our data have true target value of 150 and the remaining 10% have target value between 0–30. \n",
    "    - Then a model with MAE as loss might predict 150 for all observations, ignoring 10% of outlier cases, as it will try to go towards median value. \n",
    "    - In the same case, a model using MSE would give many predictions in the range of 0 to 30 as it will get skewed towards outliers. Both results are undesirable in many business cases.\n",
    "    - **An easy fix would be to transform the target variables. Another way is to try a different loss function. This is the motivation behind our 3rd loss function, Huber loss.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huber Loss, Smooth Mean Absolute Error\n",
    "- Huber loss is less sensitive to outliers in data than the squared error loss. It’s also **differentiable at 0**. \n",
    "- It’s basically absolute error, which becomes quadratic when error is small.\n",
    "- problem with Huber loss is that we might need to train hyper parameter delta which is an iterative process\n",
    "- ** it’s twice differentiable everywhere **\n",
    "- Many ML model implementations like XGBoost use Newton’s method to find the optimum, which is why the second derivative (Hessian) is needed. For ML frameworks like XGBoost, **twice differentiable functions are more favorable**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log-CosH Loss\n",
    "- Log-cosh is another function used in regression tasks that’s **smoother than L2**. \n",
    "- Log-cosh is the logarithm of the hyperbolic cosine of the prediction error.\n",
    "- 'logcosh' works mostly like the mean squared error, but will ** not be so strongly affected by the occasional wildly incorrect prediction **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantile Loss\n",
    "- Quantile loss functions turns out to be useful when we are interested in predicting an interval instead of only point predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSIM, MS-SSIM loss functions\n",
    "\n",
    "- **SSIM**\n",
    "- <font color=blue> For an image I with N pixels, the luminance and contrast are estimated as the mean and the standard deviation of the image intensity </font>, respectively, as follows\n",
    "\n",
    "<h3 align=\"center\"> $ mean: \\mu_I = \\frac{1}{N} \\sum\\limits_{i=1}^N  {I(i)} $ </h3>\n",
    "\n",
    "<h3 align=\"center\"> $ contrast: \\sigma_I = \\sqrt{\\frac{1}{N-1} \\sum\\limits_{i=1}^N  ({I(i) - \\mu_I})^2} $ </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combination of loss functions for perceptual quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Texture loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Variation Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior based loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triplet loss (ex: face recognition)\n",
    "- A loss is computed between anchor and a postive (similar) image, anchor and a negative image, the loss between anchor and positive has to much smaller than loss between anchor and negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=DarkOrange> Regularization Algorithms </font>\n",
    "- To avoid over optimizing/over-fitting the training set to use early termination as soon as the learning stops, other method is ** to use regularization **\n",
    "- An extension made to another method (typically regression methods) that <font color=blue>penalizes models based on their complexity</font>, favoring simpler models that are also better at generalizing.\n",
    "- other regularization technique is **dropout**\n",
    "- [Regularization techniques](https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-regularization-techniques/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 (Lasso) and L2 (Ridge) as Reguralization\n",
    "- A regularization term is added to the loss function to avoid overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 or Frobenius Regularization or Weight decay\n",
    "- The lambda term added is similar to L2 norm, but in context of neural networks its called as frobenius regularization\n",
    "- Regularization term: Loss: <h3 align=\"center\"> $ Loss (E) = \\frac{1}{m}\\sum\\limits_{i=1}^n  {(y_i - y_i^p)}^2 + \\frac{\\lambda}{2m} \\sum\\limits_{l=1}^L {||W_l||^2} $ </h3>\n",
    "- Derivative with regularization term (for backprop): <h3 align=\"center\"> $ dW = dNormal + \\frac {\\lambda}{m} W_l $ </h3>\n",
    "- Gradient decent weight updation: <h3 align=\"center\"> $ W_l = W_l - \\sigma dW $ </h3>\n",
    "- Rewriting above weight update equation: <h3 align=\"center\"> $ (1 - \\frac{\\sigma \\lambda} {m}) W_l - \\sigma dNormal $ </h3>\n",
    "- The 2nd term in above equation reduces the weights based on learning rate and lambda, that is why this regularization is ** <font color=blue> also called as weight decay </font>**\n",
    "- L2-regularization relies on the assumption that a model with small weights is simpler than a model with large weights. Thus, by **penalizing the square values of the weights in the cost function you drive all the weights to smaller values**. It becomes too costly for the cost to have large weights! This leads to a smoother model in which the output changes more slowly as the input changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop-out\n",
    "- remove few connections randomly to force the network to learn redundant representation of input, so that it doesn't overfit and depend on any particular parameter, so that all learns independently\n",
    "- The dropout rate is the fraction of the features that are zeroed out; it’s usually set between 0.2 and 0.5. \n",
    "- At test time, no units are dropped out; instead, the layer’s output values are scaled down by a factor equal to the dropout rate\n",
    "- During training time, divide each dropout layer by keep_prob to keep the same expected value for the activations. For example, if keep_prob is 0.5, then we will on average shut down half the nodes, so the output will be scaled by 0.5 since only the remaining half are contributing to the solution. Dividing by 0.5 is equivalent to multiplying by 2. Hence, the output now has the same expected value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping\n",
    "- Analyze the training and validation error plots, generally both errors will decrease with more training\n",
    "- However, after a point the training error continue to decrease but the validation error may increase, so this indicates the network starts overfit to training set and not generalizing for other data sets\n",
    "- Hence we can stop the training at that point to avoid overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Early Stopping example visualization: find validation error increase point\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8,3))\n",
    "plt.xlabel('#training iterations')\n",
    "plt.ylabel('Error')\n",
    "x_index = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
    "train_error = [100, 95, 88, 80, 60, 45, 20, 18, 15, 10, 9, 5, 3, 2, 2, 1, 0.4, 0.3]\n",
    "plt.plot(x_index, train_error, label='training_error')\n",
    "validation_error = [110, 105, 100, 95, 80, 55, 40, 38, 45, 55, 60, 60, 65, 68, 70, 70, 72, 74]\n",
    "plt.plot(x_index, validation_error, label='validation_error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation to avoid overfitting\n",
    "- More data can reduce the overfitting, but more data may not be available always so augment data by cropping, flipping, zooming, etc to create more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=DarkOrange> CNN Layer Theory </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rectified Linear Unit (ReLU)\n",
    "- [Guide to ReLU](https://medium.com/tinymind/a-practical-guide-to-relu-b83ca804f1f7)\n",
    "- ReLU\n",
    "    - defined as y = max(0, x), doesn’t “saturate,” when x gets large. Hence, doesn’t have the vanishing gradient problem suffered by other activation functions like sigmoid or tanh\n",
    "- **Dying ReLU**\n",
    "    - A ReLU neuron is “dead” if it’s stuck in the negative side and always outputs 0. \n",
    "    - Because the slope of ReLU in the negative range is also 0, once a neuron gets negative, it’s unlikely for it to recover. \n",
    "    - Such neurons are not playing any role in discriminating the input and is essentially useless.\n",
    "- Leaky ReLU\n",
    "    - Leaky ReLU has a small slope for negative values, instead of altogether zero. \n",
    "    - For example, leaky ReLU may have ** y = 0.01x ** when x < 0\n",
    "- **Parametric ReLU (PReLU)**\n",
    "    - Parametric ReLU (PReLU) is a type of leaky ReLU that, instead of having a predetermined slope like 0.01, makes it a parameter for the neural network to figure out itself: y = ax when x < 0.\n",
    "- Exponential Linear (ELU, SELU)\n",
    "    - Similar to leaky ReLU, ELU has a small slope for negative values. Instead of a straight line, it ** uses a log curve ** y = a(ex-1)\n",
    "    - sometimes called Scaled ELU (SELU) due to the constant factor a\n",
    "    - leaky ReLU while it doesn’t have the dying ReLU problem, it saturates for large negative values, allowing them to be essentially inactive\n",
    "    - It is designed to combine the good parts of ReLU and leaky ReLU\n",
    "- Concatenated ReLU (CReLU)\n",
    "    - Concatenated ReLU has two outputs, one ReLU and one negative ReLU, concatenated together. \n",
    "    - In other words, for positive x it produces [x, 0], and for negative x it produces [0, x]. \n",
    "    - Because it has two outputs, CReLU doubles the output dimension.\n",
    "- ReLU-6\n",
    "    - ReLU capped at 6, 6 is an arbitrary choice that worked well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax regression for multi-class classification\n",
    "- Softmax regression/activation is a generalization of logistic regression (binary classifier) and its used for multi-class classification used as the last layer for predictions\n",
    "- **Softmax converts the final outputs to probabilities of output classes**\n",
    "- Softmax gives probabilities for all output classes, whereas hardmax gives value 1 to highest probability class and makes remaining 0\n",
    "- Softmax is computed (as element-wise operation) and normalized on the output vector Z as below,\n",
    "<h3 align=\"center\"> $ Softmax_{prob} = \\frac {e^Z}{\\sum{e^Z}} $ </h3>\n",
    "- **Softmax example with 4 output classes,**\n",
    "    - final output Z = {5, 2, -1, 3}\n",
    "    - Softmax element-wise values (numerator) $e^Z$ = {148.4, 7.4, 0.4, 20.1}\n",
    "    - Softmax normalization factor (denominator) $\\sum{e^Z}$ = 176.3\n",
    "    - Final Softmax output probabilities = {0.842, 0.042, 0.002, 0.114}\n",
    "    - Final Hardmax output probabilities = {1, 0, 0, 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=DarkOrange> Data Pre-Processing or Feature Scaling </font>\n",
    "- Standardization\n",
    "- Mean Normalization\n",
    "- Min-Max Scaling\n",
    "- Unit Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard deviation\n",
    "- [Standard deviation tutorial](https://www.mathsisfun.com/data/standard-deviation.html#WhySquare)\n",
    "- The Standard Deviation is a measure of how spread out numbers are (compared to mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization and Standardization\n",
    "- [Standardization Vs Normalization](https://medium.com/@rrfd/standardize-or-normalize-examples-in-python-e3f174b65dfc)\n",
    "- **Normalization (Min-Max Scaling):**\n",
    "    - If the training data contain some features with higher magnitude (outliers), then the results might be dominated by them instead of all the features, hence normalize the data with largest value\n",
    "    - Normalization makes training less sensitive to the scale of features, so we can better solve for coefficients\n",
    "    - Normalizing will ensure that a convergence problem does not have a massive variance, making optimization feasible\n",
    "    - Normalization modifies the data range to [0, 1] using below formula,\n",
    "    <h4 align=\"center\"> $ norm(x) = \\frac{x - min(x)}{max(x) - min(x)} $ </h4>\n",
    "- **Standardization (Z-Score Normalization)**\n",
    "    - Data is rescaled such that mean (μ) = 0 and standard deviation (𝛔) = 1 as,\n",
    "    <h4 align=\"center\"> $ std(x) = \\frac{x - \\mu}{\\sigma} $ </h4>\n",
    "    - **standardized (z-score) or studentized (t-scores) scores to increase comparability of different features in the training data**\n",
    "    - **to arrive at (transformed) data that follows a normal distribution**\n",
    "    - But if original data does not follow normal distribution, then transformed data also may not follow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance and Correlation\n",
    "- [Covariance and Correlation Tutorial](https://medium.com/@adamzerner/covariance-and-correlation-d4c64769d4f1)\n",
    "- Covariance\n",
    "    - **Covariance is a measure of how much two random variables vary together**. It’s similar to variance, but where variance tells you how a single variable varies, covariance tells you how two variables vary together\n",
    "    - positive covariance implies a direct relationship between the variables (increasing x increases y)\n",
    "    - negative covariance implies an indirect relationship between the variables (increasing x decreases y)\n",
    "    - ** If the covariance is large, so there is a strong relationship between the numbers** or if the covariance is small, so there is a weak relationship between the numbers\n",
    "- Correlation\n",
    "    - **The covariance can tell the direction/variability between two data, can not tell the how much is the relation**, since the data expressed in different units (ex: mm, cm, meters) yields varying covariance (large/small)\n",
    "    - <font color=blue>Hence the data's has to be standardized to find the strength of covariance, the standardized covariance known as Correlation which tells the strength</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whitening"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "- [Batch Normalization Tutorial](https://www.learnopencv.com/batch-normalization-in-deep-networks/)\n",
    "- [Batch Normalization Explained](https://blog.paperspace.com/busting-the-myths-about-batch-normalization/)\n",
    "- The intermediate data in a DNN can shift due to change in distribution, so BN normalizes the intermediate data to have zero mean, unit variance similar to general input normalization.\n",
    "- BN is generally applied on layer output (Z) before applying activation, ** the mean and variance are computed from each mini-batch**\n",
    "<h4 align=\"center\"> $ Z_{norm} = \\frac{Z - \\mu}{\\sqrt{\\sigma^2+\\epsilon}} $ </h4>\n",
    "- Final BN output is scaled by $\\gamma, \\beta$\n",
    "<h4 align=\"center\"> $ Z_{N} = \\gamma * Z_{norm} + \\beta $ </h4>\n",
    "- <font color=blue> $\\gamma, \\beta$ are also updated using gradient descent $\\gamma = \\gamma - \\alpha * d\\gamma$ </font>\n",
    "- <font color=blue> To compute BN in test time, the exponential average of $\\gamma, \\beta$ computed while training on mini-batches are used </font>\n",
    "- **Covariate Shift**\n",
    "    - covariate shift is defined as a change in the distribution of data\n",
    "    - If a DNN is trained on a data set containing a class (say red roses) and similar class with different attributes (say white roses)\n",
    "    - These two classes will be present in different regions in the feature space, this difference in distribution is called the covariate shift\n",
    "    - So while training, the mini batch should have the same distribution (of both red and white roses) sampled from the entire training data set\n",
    "    - When the mini-batches have images uniformly sampled from the entire distribution, there is negligible covariate shift\n",
    "    - However, when the mini-batches are sampled from only one of the two subsets, there is a ** significant covariate shift **. This makes the training of the rose vs non-rose classifier very slow\n",
    "    - An easy way to solve this problem to normalize the inputs (mini batch) to the neural network so that the input distribution have a zero mean and a unit variance\n",
    "- **Internal Covariate Shift**\n",
    "    - However, when the networks get deeper, say, 20 or more layers, the minor fluctuations in weights over more than 20 odd layers can produce big changes in the distribution of the input being fed to deeper layers even if the input is normalized.\n",
    "    - Just as it made intuitive sense to have a uniform distribution for the input layer, it is advantageous to have the same input distribution for each hidden unit over time while training. \n",
    "    - But in a neural network, each ** hidden unit’s input distribution changes every time there is a parameter update in the previous layer **. This is called internal covariate shift. \n",
    "    - This makes training slow and requires a very small learning rate and a good parameter initialization.\n",
    "- This problem is solved by normalizing the layer’s inputs over a mini-batch and this process is therefore called Batch Normalization.\n",
    "- However, as against above discussion, batch norm actually ends up increasing internal covariate shift as compared to a network that doesn't use batch norm. They key insight from the paper is that batch norm actually makes the loss surface smoother, which is why it works so well (as pointed by a recent research paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=DarkOrange> Back Propagation </font>\n",
    "- [Back propagation step by step example](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=DarkOrange> Gradient Descent </font>\n",
    "- [Gradient Descent Tutorial](https://blog.paperspace.com/intro-to-optimization-in-deep-learning-gradient-descent/)\n",
    "- how do we go about **escaping local minima and saddle points**, while trying to converge to a global maximum. The answer is **randomness**\n",
    "- Gradient descent is a First Order Optimization Method. It only takes the first order derivatives of the loss function into account and not the higher ones. What this basically means it has no clue about the curvature of the loss function. It can tell whether the loss is declining and how fast, but cannot differentiate between whether the curve is a plane, curving upwards or curving downwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=DarkOrange> Deep Learning Training </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=DarkMagenta> Bias Variance Trade-Off </font>\n",
    "- [Bias Variance Trade-off Tutorial](https://www.listendata.com/2017/02/bias-variance-tradeoff.html)\n",
    "- [Bias Variance Trade-off Infograph](https://elitedatascience.com/bias-variance-tradeoff)\n",
    "- ** Bias **\n",
    "    - Bias is a measure of the prediction accuracy on training data\n",
    "    - <font color=blue> High bias means low prediction accuracy </font>, which means model may be too simple not able learn from training data known as underfitting\n",
    "    - For example, a linear regression model would have high bias when trying to model a non-linear relationship\n",
    "    - High Bias Techniques\n",
    "        - Linear Regression, Linear Discriminant Analysis and Logistic Regression\n",
    "    - Low Bias Techniques\n",
    "        - Decision Trees,  K-nearest neighbours and Gradient Boosting\n",
    "    - <font color=blue> Parametric algorithms which assume something about the distribution of the data points </font> suffer from High Bias. Whereas non-parametric algorithms which does not assume anything special about distribution have low bias.\n",
    "- ** Variance **\n",
    "    - Variance is a measure of the generalization of the network\n",
    "    - <font color=blue> High variance means less generalization </font>, complex models that fits well on training data but they cannot generalise the pattern well which results to overfitting\n",
    "    - Low Variance Techniques\n",
    "        - Linear Regression, Linear Discriminant Analysis and Logistic Regression\n",
    "    - High Variance Techniques\n",
    "        - Decision Trees,  K-nearest neighbours and SVM\n",
    "- ** Bias Variance Trade-off **\n",
    "    - It means there is a trade-off between predictive accuracy and generalization of pattern outside training data. Increasing the accuracy of the model will lead to less generalization of pattern outside training data. Increasing the bias will decrease the variance. Increasing the variance will decrease the bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Initialization\n",
    "- Weights initialized with zeros makes network not learn anything because of symmetry problem. as all nodes will have same weights, so they dont learn different functions/relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### He initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Clipping\n",
    "- Due to exploding gradients the weight parameters values exponentially become large values like Nan (Not a number)\n",
    "- To solve this problem simple solution is to clip the gradients above a certain threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=DarkOrange> Hyper Parameter Tuning </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-Batch Gradient Descent\n",
    "- Training set is divided in to smaller set of samples known as mini-batch\n",
    "- <font color=blue> Before splitting the training in to mini-batches, the data is randomly shuffled to ensure data is shuffled in to different mini-batches </font>\n",
    "- After a mini-batch is trained the backward propagation is applied to update weights\n",
    "- Hence, weights are updated after training on a min-batch unlike batch gradient descent where back prop & weight updation happens after processing all training samples\n",
    "- So to process the whole training set, it needs to run for train_samples/min_batch_size times **known as iterations**\n",
    "- If min-batch size = total train samples, it is batch gradient\n",
    "- If min-batch size = total train samples, it is **known as stochastic gradient descent (SGD)**\n",
    "- We may observer, the training error reduces smoothly in batch GD, where as <font color=blue>**in mini-batch ripples are observed in training error due to error computed on different mini-batches**</font>\n",
    "- SGD error will be more noisier as it operates on each sample, but its much faster\n",
    "- Batch gradient is smooth, but very slow due to large amount of data may not fit in RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x216 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Typical training error plots of batch and min-batch gradient descent\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12,3))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.xlabel('#training iterations')\n",
    "plt.ylabel('Training cost')\n",
    "x_index = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
    "train_error = [200, 180, 150, 130, 120, 100, 90, 80, 70, 60, 50, 45, 40, 35, 30, 25, 20, 15]\n",
    "plt.plot(x_index, train_error, label='Batch Gradient Descent')\n",
    "plt.legend()\n",
    "\n",
    "#sub plot\n",
    "plt.subplot(1,2,2)\n",
    "plt.xlabel('#min-batch training iterations')\n",
    "plt.ylabel('Training cost')\n",
    "x_index = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
    "validation_error = [200, 190, 175, 185, 180, 155, 165, 130, 145, 120, 140, 100, 125, 90, 95, 80, 90, 60, 78, 50, 55, 45, 40, 50, 35]\n",
    "plt.plot(x_index, validation_error, label='Mini-batch Gradient Descent')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponentially Weighted Moving Average\n",
    "- It is an optimization algorithm for local average or smoothing\n",
    "- Equation: Current_val(t) = weight * Previous_val(t-1) + (1 - weight) * current_sample\n",
    "- Generally defined as, <h3 align=\"center\"> $ Val_t = \\beta * Val_{t-1} + (1-\\beta) * currentsample $ </h3>\n",
    "- **$\\beta$ is weight factor indicates how much weightage to be given to previous values**\n",
    "- If $\\beta$ is high, then lot of weitage to previous values, then moving average adapts slowly to local changes\n",
    "- If $\\beta$ is low, then lot of weitage to current sample, then moving average adapts faster to local changes\n",
    "- $Val_t$ is $\\approx \\frac{1}{1-\\beta}$\n",
    "    - Example: if $\\beta = 0.9$ means its an average over 10 previous values\n",
    "- $\\beta$ is a hyper parameter to be tuned and typical value for $\\beta$ is 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias correction in exponential weighted average\n",
    "- Generally the initial value $V_0$ is assigned with zero, so the initial values are close to zero instead of representing the actual averages\n",
    "- To solve this we can calculate value as, $\\frac{Val_t}{1-\\beta^t}$, for initial values it works as average, when t becomes large $\\frac{1}{1-\\beta^t}$ becomes one so no difference to the value\n",
    "- **This way correcting the average values of initial samples is known as bias correction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent with momentum\n",
    "- The gradient descent error oscillates or contains ripples due to change in error surface due to current mini-batch\n",
    "- This oscillations are even bigger if we use a bigger learning rate\n",
    "- <font color=blue> **Hence for smooth convergence use the exponential moving average concept to update gradients to have fast convergence** </font>\n",
    "- Weight gradient update, <h3 align=\"center\"> $ V_{dw} = \\beta * V_{dw} + (1-\\beta) * dw $ </h3>\n",
    "<h3 align=\"center\"> $ W = W - \\alpha * V_{dw} $ </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSprop (Root Mean Square prop)\n",
    "- Similar to gradient descent with momentum, the vertical direction updates has to be minimized and horizontal direction movement has to higher for faster convergence in 2D bowl shape function\n",
    "- RMSprop also does better convergence\n",
    "- Weight gradient update, <h3 align=\"center\"> $ S_{dw} = \\beta * S_{dw} + (1-\\beta) * dw * dw $ </h3>\n",
    "<h3 align=\"center\"> $ W = W - \\alpha * \\frac {dw}{\\sqrt{1- S_{dw}}} $ </h3>\n",
    "- The intuition of RMSprop is, divide the vertical gradients with bigger number and horizontal gradients with smaller numbers for smooth and fast convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam Optimizer\n",
    "- <font color=blue> **Adam optimizer is the best optimizer, which is a combination of momentum and RMSprop** </font>\n",
    "<h3 align=\"center\"> $ V_{dw} = \\beta_1 * V_{dw} + (1-\\beta_1) * dw $ </h3>\n",
    "<h3 align=\"center\"> $ S_{dw} = \\beta_2 * S_{dw} + (1-\\beta_2) * dw * dw $ </h3>\n",
    "- Adam weight update, <h3 align=\"center\"> $ W = W - \\alpha * \\frac {V_{dw}}{\\sqrt{1- S_{dw}}} $ </h3>\n",
    "- $\\beta_1, \\beta_2$ are the 2 hyper parameters and typical values suggested by adam paper are 0.9 and 0.9999 respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate decay\n",
    "- The learning rate can higher while initial training steps to take big strides, after some epochs gradually reduce the learning rate to converge to the optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cyclical learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #layers and #hidden units hyper parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-dev set for different distribution of train, dev sets\n",
    "- The distribution of train, dev/test sets can be different, still the DNNs work well\n",
    "- But to compute bias, variance, these sets will not indicate proper analysis due to different distributions\n",
    "- Create a separate dev set from training data known as train-dev set, which is used for error analysis of bias, variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=DarkOrange> Hyper Parameter Search </font>\n",
    "- [Tutorial with examples](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/)\n",
    "- [Tutorial and examples](https://blog.nanonets.com/hyperparameter-optimization/)\n",
    "- Hyperparameter Tuning is nothing but searching for the right set of hyperparameter to achieve high precision and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "- In Grid Search, we try every combination of a preset list of values of the hyper-parameters and evaluate the model for each combination. The pattern followed here is similar to the grid, where all the values are placed in the form of a matrix.\n",
    "-  the number of evaluations required for this strategy increases exponentially with each additional parameter, due to the curse of dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Search for hyper parameter tuning\n",
    "- Random search is a technique where random combinations of the hyperparameters are used to find the best solution for the built model. It tries random combinations of a range of values.\n",
    "- The chances of finding the optimal parameter are comparatively higher in random search because of the random search pattern "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coarse to fine sampling of hyper param search space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Search\n",
    "- Grid and random search are completely uninformed by past evaluations, and as a result, often spend a significant amount of time evaluating “bad” hyperparameters.\n",
    "- <font color=blue> Bayesian approaches, in contrast to random or grid search, keep track of past evaluation results </font> which they use to form a probabilistic model mapping hyperparameters to a probability of a score on the objective function:\n",
    "- **The Algorithm:** <font color=blue> Bayesian methods attempt to build a function (more accurately, a probability distribution over possible function) that estimates how good your model might be for a certain choice of hyperparameters. By using this approximate function (called a surrogate function in literature), you don’t have to go through the set, train, evaluate loop too many time, since you can just optimize the hyperparameters to the surrogate function. </font>\n",
    "- <font color=blue> The surrogate function comes from something called a Gaussian process </font> (note: there are other ways to model the surrogate function, but I’ll use a Gaussian process).\n",
    "- The surrogate is much easier to optimize than the objective function and Bayesian methods work by finding the next set of hyperparameters to evaluate on the actual objective function by selecting hyperparameters that perform best on the surrogate function\n",
    "- <font color=blue> So it is like building a ML model to find best hyper parameters by optimizing the surrogate function with the hyperparameters\n",
    "-  In other words:\n",
    "    - 1 Build a surrogate probability model of the objective function\n",
    "    - 2 Find the hyperparameters that perform best on the surrogate\n",
    "    - 3 Apply these hyperparameters to the true objective function\n",
    "    - 4 Update the surrogate model incorporating the new results\n",
    "    - 5 Repeat steps 2–4 until max iterations or time is reached </font>\n",
    "    \n",
    "---\n",
    "- **Bayesian search with visualization**\n",
    "- The first image shows an initial estimate of the surrogate model — in black with associated **uncertainty in gray** — after two evaluations. Clearly, the surrogate model is a poor approximation of the actual objective function in red:\n",
    "- <font color=blue> The next image shows the surrogate function after 8 evaluations. Now the surrogate almost exactly matches the true function. Therefore, if the algorithm selects the hyperparameters that maximize the surrogate, they will likely yield very good results on the true evaluation function. </font>\n",
    "\n",
    "<img src=\"images/bayesian_initial.PNG\" width=\"350\" align=\"left\">\n",
    "<img src=\"images/bayesian_refined.PNG\" width=\"350\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Model Based Optimization (SMBO)\n",
    "- Sequential model-based optimization (SMBO) methods (SMBO) are a formalization of Bayesian optimization. \n",
    "- The sequential refers to running trials one after another, each time trying better hyperparameters by applying Bayesian reasoning and updating a probability model (surrogate).\n",
    "\n",
    "- There are five aspects of model-based hyperparameter optimization:\n",
    "    - 1. A domain of hyperparameters over which to search\n",
    "    - 2. An objective function which takes in hyperparameters and outputs a score that we want to minimize (or maximize)\n",
    "    - 3. The surrogate model of the objective function\n",
    "    - 4. A criteria, called a selection function, for evaluating which hyperparameters to choose next from the surrogate model\n",
    "    - 5. A history consisting of (score, hyperparameter) pairs used by the algorithm to update the surrogate model\n",
    "- <font color=blue> Gaussian process is the most common choice for surrogate function\n",
    "- Tree Parzen Estimators (TPE) while the most common choice for step 4 is Expected Improvement. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree Parzen Estimators (TPE) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Search Implementation\n",
    "- [Bayesian search with Keras](https://www.dlology.com/blog/how-to-do-hyperparameter-search-with-baysian-optimization-for-keras-model/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=DarkOrange> Ensemble Algorithms </font>\n",
    "- [Ensemble overview](https://medium.com/coinmonks/an-intro-to-ensemble-learning-in-machine-learning-5ed8792af72d)\n",
    "- [Ensemble Tutorial](https://towardsdatascience.com/ensemble-learning-in-machine-learning-getting-started-4ed85eb38e00)\n",
    "- The goal of ensemble algorithms is to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability / robustness over a single estimator\n",
    "- There are two families of ensemble methods which are usually distinguished\n",
    "    - **Averaging methods**\n",
    "        - The driving principle is to build several estimators independently and then to average their predictions. On average, the combined estimator is usually better than any of the single base estimator because its variance is reduced\n",
    "        - <font color=blue> examples: Bagging methods, Forests of randomized trees </font>\n",
    "    - **Boosting methods**\n",
    "        - Base estimators are built sequentially and one tries to reduce the bias of the combined estimator. The motivation is to combine several weak models to produce a powerful ensemble\n",
    "        - <font color=blue> examples: AdaBoost, Gradient Tree Boosting </font>\n",
    "- **The three most popular methods for combining the predictions from different models are**\n",
    "    - **Bagging or Bootstrap Aggregation**\n",
    "        - Building multiple models (typically of the same type) from different subsamples of the training dataset.\n",
    "    - **Boosting**\n",
    "        - Building multiple models (typically of the same type) each of which learns to fix the prediction errors of a prior model in the sequence of models.\n",
    "    - **Voting**\n",
    "        - Building multiple models (typically of differing types) and simple statistics (like calculating the mean) are used to combine predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting\n",
    "- You can train your model using diverse algorithms and then ensemble them to predict the final output. Say, you use a Random Forest Classifier, SVM Classifier, Linear Regression etc\n",
    "- **Hard voting** is where a model is selected from an ensemble to make the final prediction by a simple **majority** vote for accuracy\n",
    "- **Soft voting** arrives at the best result by **averaging** out the probabilities calculated by individual algorithms. Soft Voting can only be done when all your classifiers can calculate probabilities for the outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging or Bootstrap Aggregation\n",
    "- Instead of running various models on a single dataset, you can **use a single model over various random subsets of the dataset**\n",
    "- Random sampling with replacement is called Bagging, short for bootstrap aggregating. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "- An ensemble of Decision trees is a Random Forest. Random Forests performs Bagging internally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost\n",
    "- In simple terms, Run a Classifier and make predictions. Run another classifier to fit the previously misclassified instances and make predictions. Repeat until all/most of the training instances are fitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting\n",
    "- Similar to AdaBoost, Gradient Boosting also works with successive predictive models added to the ensemble.\n",
    "- Instead of updating the weights of the training instances like AdaBoost, Gradient Boosting fits the new model to the residual errors.\n",
    "- Put simply, Fit a model to the given Training set. Calculate the Residual Errors which become the new training instances. A new model is trained on these and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "- XGBoost is a recent, most preferred and powerful gradient boosting method. \n",
    "- Instead of making hard Yes and No Decision at the Leaf Nodes, XGBoost assigns positive and negative values to every decision made.\n",
    "- XGBoost can work with Trees as well as Linear Models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=DarkOrange> Recurrent Neural Networks (RNN) </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why RNNs?\n",
    "- For sequence data normal CNNs does not work because of mainly 2 reasons,\n",
    "    - The inputs does not share common features like images which is required by CNNs\n",
    "    - The input size and output size can be different (machine translation), which can not be handled by CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple RNN\n",
    "- The input 'x' is processed by a weight matrix Wx, and previous activation output by Wa and output prediction is computed by Wy matrix\n",
    "- The same weights Wa, Wx, Wy are shared among different times temporally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple RNN equations\n",
    "- The simple RNN diagram and equations are shown below (source: coursera),\n",
    "\n",
    "<img src=\"images/rnn_forward_prop_equations.PNG\" width=\"500\"/>\n",
    "\n",
    "- It can be represented as a single unit as shown below,\n",
    "<img src=\"images/rnn_diagram.PNG\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplified RNN equations notation\n",
    "- The generic version of equations are created by staking the Wa, Wx weight matrices and also correspondingly staking input 'x(t)', previous output 'a(t-1)'\n",
    "<img src=\"images/rnn_simplified_notation.PNG\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple RNN representation\n",
    "- The above equation can be represented as shown below\n",
    "\n",
    "<img src=\"images/rnn_unit.PNG\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPTT (Back Prop Through Time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Modelling\n",
    "- LM finds the probability of a sentence occurrence with a set of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "- Takes individual words in a given sentence and maps to an entry the dictionary\n",
    "- Add UKN for unknown words which are not there in your dictionary\n",
    "- Can add EOS token at the end of sentence to indicate end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word vs Character RNN\n",
    "- Character level models require lot of characters, hence states and longer dependencies and also computationally expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU (Gated Recurrent Unit)\n",
    "- To resolve the long term dependencies of input sequences, LSTM and GRU are developed\n",
    "- The GRU uses a memory cell 'c', <font color=blue> the memory cell value is updated by a gate to decide what information to retain using an update gate </font>\n",
    "- The memory cell value is dependent on current candidate cell value and previous cell output\n",
    "- The memory cell can be a vector, each element may be representing a specific characteristic of input sequence (like noun, singular/plural, subject, etc)\n",
    "\n",
    "<img src=\"images/rnn_gru.PNG\" width=\"500\"/>\n",
    "\n",
    "- The full standard GRU uses one extra operation/gate compared to above equation to find the relevance of the candidate cell output\n",
    "\n",
    "<img src=\"images/rnn_gru_standard.PNG\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM (Long Short Term Memory)\n",
    "- LSTM is a generic and complex version than GRU\n",
    "- <font color=blue> LSTM uses 3 gates rather than 2 gates as in GRU </font>\n",
    "- The main difference of LSTM compared to GRU is a(t) is not same as c(t)\n",
    "- <font color=blue> The cell state 'c' is computed with 2 gates update, forget </font>\n",
    "- Output at any time stamp is computed with 3rd gate output gate\n",
    "\n",
    "<img src=\"images/rnn_lstm.PNG\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peephole connection\n",
    "- Its a variation of LSTM, where gate values not only depend on a(t-1), x(t) but also depends on c(t-1) previous cell state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BRNN (Bi-directional RNN)\n",
    "- Bi-directional processes the input sequence from both directions to make predictions\n",
    "- The downside of BRNN is it requires entire input sequence, it is suitable for NLP tasks where entire input sequence is available, but for real-time ASR tasks entire sequence is not available where a complex BRNN can be used\n",
    "\n",
    "<img src=\"images/rnn_BRNN.PNG\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep RNN\n",
    "- To learn more complex functions the RNNs can be stacked on top of another as shown below\n",
    "\n",
    "<img src=\"images/rnn_deep.PNG\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=DarkOrange> Word Embedding </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurized representation\n",
    "- Generally the words in a dictionary are represented as one-hot encoding, this representation doesn't maintain any relation between different words\n",
    "- Hence, a simple one-hot representation requires a lot of corpus to train the model to learn all words\n",
    "- But if we can maintain a relation (like orange, apple are fruits) between words, then we need only small corpus to train model, word embedding achieves this\n",
    "- <font color=blue> Featurized representation gives each word a representation against various features, these features are used get the relation between various words </font>\n",
    "- Word embedding gives analogies/similarity of various words, it allows to train using a small corpus and get the context for unseen words based on similarity, below is an example: if Man is to Women, then what King is to ?, It will be found by vector (various features as shown below) difference between Man and Women will be similar to King and Queen.\n",
    "<img src=\"images/word_embed_analogies.PNG\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=DarkOrange> Sequence Models </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=DarkOrange> Generative Models </font>\n",
    "- The basic idea of a generative model is to take a collection of training examples and form a representation of their probability distribution.\n",
    "- In machine learning, the two main classes of models are generative and discriminative.  \n",
    "- A discriminative model is one that discriminates between two (or more) different classes of data - for example a convolutional neural network that is trained to output 1 given an image of a human face and 0 otherwise.  \n",
    "- A generative model on the other hand doesn't know anything about classes of data.  Instead, <font color=blue>generative models purpose is to generate new data which fits the distribution of the training data\n",
    "- for example, a Gaussian Mixture Model is a generative model which, after trained on a set of points, is able to generate new random points which more-or-less fit the distribution of the training data </font> (assuming a GMM is able to mimic the data well)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=DarkOrange> GAN (Generative Adversarial Network) </font>\n",
    "- [GANs as generative models](https://www.quora.com/What-are-Generative-Adversarial-Networks-GANs)\n",
    "- [Generative Models and TF Code](https://github.com/wiseodd/generative-models)\n",
    "- [Popular GANs](https://blog.usejournal.com/the-rise-of-generative-adversarial-networks-be52d424e517)\n",
    "- [How to Train GANs](https://github.com/soumith/ganhacks)\n",
    "- [SOTA GANs Keras Code](https://github.com/eriklindernoren/Keras-GAN#srgan)\n",
    "- GANs belong to the set of algorithms named generative models. These algorithms belong to the field of unsupervised learning, which does not require labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator and Descriminator\n",
    "- Generative Adversarial Networks are composed of two models: Generator, Descriminator\n",
    "- <font color=blue> The generative network keeps producing images/outputs that are closer in appearance to the real images/outputs while the discriminative network is trying to determine the differences between real and fake images. \n",
    "- The ultimate goal is to have a generative network that can produce images which are indistinguishable from the real ones. </font>\n",
    "- In the starting phase, a Generator model takes random noise signals as input and generates a random noisy image as the output, gradually with the help of the Discriminator, it starts generating images of a particular class that look real.\n",
    "- The Discriminator which will be the opponent of Generator is fed with both the generated images as well as a certain class of images at the same time, allowing it to tell the generated how the real image looks like.\n",
    "- After reaching a certain point, the Discriminator will be unable to tell if the generate image is a real or a fake image, and that is when we can see images of a certain class(class that the discriminator is trained with.) being generated by out Generator that never actually existed before.\n",
    "- the two networks can then be trained jointly (at the same time) with opposite goals :\n",
    "    - the goal of the generator is to fool the discriminator, so <font color=blue> the generative neural network is trained to maximise the final classification error (between true and generated data) </font>\n",
    "    - the goal of the discriminator is to detect fake generated data, so <font color=blue> the discriminative neural network is trained to minimise the final classification error </font>\n",
    "    \n",
    "<img src=\"images/gan_vanila.PNG\" width=\"600\">\n",
    "\n",
    "- <font color=blue> They both get stronger together until the discriminator cannot distinguish between the real and the generated images anymore. It could do nothing more than predicting real or fake with only 50% accuracy. This is no more useful than flipping a coin and guess. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative models sample target probability distribution\n",
    "- [GANs are sampling input N-dimensional distribution](https://towardsdatascience.com/understanding-generative-adversarial-networks-gans-cd6e4651a29)\n",
    "- The problem of generating a new image of dog is equivalent to the problem of generating a new vector following the “dog probability distribution” over the N dimensional vector space. So we are, in fact, facing a problem of generating a random variable with respect to a specific probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple GAN training\n",
    "- [Simple GAN Training](https://hackernoon.com/how-do-gans-intuitively-work-2dda07f247a1)\n",
    "- We train both the generator and the discriminator to make them stronger together and avoid making one network significantly stronger than the other by taking turn.\n",
    "- **Why train both networks together <font color=blue>by taking turn </font> instead of training each to be strong separately?**\n",
    "    - If one network is too strong, the other won’t be able to keep up and you will be left with two dumb networks. \n",
    "    - training GANs can be tricky. It's important that the generator and discriminator do not overpower each other (e.g., that they train at a similar rate).\n",
    "    \n",
    "---\n",
    "- **Training the discriminator**\n",
    "    - Just give it an image from the training set and another one generated image from the generator. It should output 0 if it gets fed a generated image. And it should output 1 if it gets fed a real image.\n",
    "    \n",
    "---\n",
    "- **Training the generator**\n",
    "    - The generator must try to make the discriminator outputs 1 (real) when given its generated image.\n",
    "    - Intuitively, the discriminator tells how much to tweak each pixel in order to make the image a little bit more realistic.\n",
    "    - Technically, you do that by **back-propagating the gradients of the discriminator’s output with respect to the generated image.** That way, you will have gradients tensor that have the same shape as the image.\n",
    "    \n",
    "---\n",
    "- **Loss Functions**\n",
    "- <font color=blue> Rather than just having a single loss function, we need to define three: The loss of the generator, the loss of the discriminator when using real images, and the loss of the discriminator when using fake images. The sum of the fake image and real image loss is the overall discriminator loss. </font>\n",
    "- First, we define our loss for the real images. For that, **we pass the output of the discriminator when dealing with real images and compare it with the labels, which are all 1, which means true**. We use a technique called label smoothing here to help our network getting more accurate by multiplying the 1s with 0.9.\n",
    "- Then, we define the loss for our fake images. This time we pass in the **output of the discriminator when dealing with fake images and compare it to our labels, which are all 0**, which means they are fake.\n",
    "- Finally, <font color=blue>for the generator loss, we do the same like in the last step, but instead of comparing the output with all 0s, we compare it with 1s, </font> since we want to fool the discriminator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to measure GAN performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Convolution GAN (DCGAN)\n",
    "- For the first time, convolutional neural networks were used within GANs and achieved impressive results.\n",
    "- it introduced major architectural changes to tackle problems like training instability, mode collapse, and internal covariate shift. Since then, numerous GAN architectures were introduced based on the architecture of DCGAN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## InfoGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CGAN (Conditional Generative Adversarial Networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SRGAN (Super Resolution GAN )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ESRGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CycleGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=DarkOrange> VAE (Variational Auto Encoders) </font>\n",
    "- [VAE Tutorial](https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf)\n",
    "- Variational Autoencoders (VAEs) are powerful generative models\n",
    "- When using generative models, you could simply want to generate a random, new output, that looks similar to the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto Encoder\n",
    "- An autoencoder network is actually a pair of two connected networks, an encoder and a decoder. \n",
    "- An encoder network takes in an input, and converts it into a smaller, dense representation, which the decoder network can use to convert it back to the original input.\n",
    "- The encoder learns to preserve as much of the relevant information as possible in the limited encoding, and intelligently discard irrelevant parts. The decoder learns to take the encoding and properly reconstruct it into a full image. Together, they form an autoencoder.\n",
    "\n",
    "<img src=\"images/auto_encoder.PNG\" width=\"400\"/>\n",
    "\n",
    "- The **fundamental problem with autoencoders, for generation of new or variation of input, is that the latent space they convert their inputs to and where their encoded vectors lie, may not be continuous**\n",
    "- If the space has discontinuities (eg. gaps between clusters) and you sample/generate a variation from there, the decoder will simply generate an unrealistic output, because the decoder has no idea how to deal with that region of the latent space. During training, it never saw encoded vectors coming from that region of latent space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE\n",
    "- when you’re building a generative model, you don’t want to prepare to replicate the same image you put in. You want to randomly sample from the latent space, or generate variations on an input image, from a continuous latent space.\n",
    "- Variational Autoencoders (VAEs) have one fundamentally unique property that separates them from vanilla autoencoders, and it is this property that makes them so useful for generative modeling: <font color=blue> their latent spaces are, by design, continuous, allowing easy random sampling and interpolation. </font>\n",
    "- It achieves this by making its encoder not output an encoding vector of size n, rather, <font color=blue> outputting two vectors of size n: a vector of means, μ, and another vector of standard deviations, σ. </font>\n",
    "\n",
    "<img src=\"images/vae.PNG\" width=\"300\"/>\n",
    "\n",
    "- The encoder samples the vectors stochastically, stochastic generation means, that even for the same input, while the mean and standard deviations remain the same, the actual encoding will somewhat vary on every single pass simply due to sampling.\n",
    "\n",
    "<img src=\"images/vae_vs_standard.PNG\" width=\"400\"/>\n",
    "\n",
    "- Intuitively, the mean vector controls where the encoding of an input should be centered around, while the standard deviation controls the “area”, how much from the mean the encoding can vary. \n",
    "- <font color=blue>As encodings are generated at random from anywhere inside the “circle” (the distribution), the decoder learns that not only is a single point in latent space referring to a sample of that class, but all nearby points refer to the same as well.</font> \n",
    "- This allows the decoder to not just decode single, specific encodings in the latent space (leaving the decodable latent space discontinuous), but ones that slightly vary too, as the decoder is exposed to a range of variations of the encoding of the same input during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KL divergence loss\n",
    "- What we ideally want are encodings, all of which are as close as possible to each other while still being distinct, allowing smooth interpolation, and enabling the construction of new samples.\n",
    "- In order to force this, we introduce the Kullback–Leibler divergence (KL divergence[2]) into the loss function. \n",
    "- The KL divergence between two probability distributions simply measures how much they diverge from each other. Minimizing the KL divergence here means optimizing the probability distribution parameters (μ and σ) to closely resemble that of the target distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=DarkOrange> Reinforcement Learning </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=DarkOrange> SOTA CNN Networks/Concepts </font>\n",
    "- [Consolidated list of various SOTA networks](https://towardsdatascience.com/review-srdensenet-densenet-for-sr-super-resolution-cbee599de7e8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=DarkMagenta> DenseNet </font>\n",
    "- [DenseNet Tutorial](https://towardsdatascience.com/review-densenet-image-classification-b6631a8ef803)\n",
    "- **Dense Block :** In DenseNet, <font color=blue> each layer obtains additional inputs from all preceding layers </font> and passes on its own feature-maps to all subsequent layers. Concatenation is used. Each layer is receiving a “collective knowledge” from all preceding layers.\n",
    "\n",
    "<img src=\"images/dense_block.PNG\" width=\"400\">\n",
    "\n",
    "- Since each layer receives feature maps from all preceding layers, network can be thinner and compact, i.e. number of channels can be fewer. The growth rate k is the additional number of channels for each layer. \n",
    "- <font color=blue> A bottleneck (convolution 1x1) is used to reduce the number of channels </font>\n",
    "- ** Advantages of DenseNet **\n",
    "    - <font color=blue> The error signal can be easily propagated to earlier layers more directly </font>. This is a kind of implicit deep supervision as earlier layers can get direct supervision from the final layer.\n",
    "    - It have higher computational efficiency and memory efficiency.\n",
    "    - Since each layer in DenseNet receive all preceding layers as input, more diversified features and tends to have richer patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet\n",
    "- instead of trying to learn an underlying mapping from x to H(x), learn the difference between the two, or the “residual.” Then, to calculate H(x), we can just add the residual to the input.\n",
    "- <font color=blue> Say the residual is F(x)=H(x)-x. Now, instead of trying to learn H(x) directly, resnet trying to learn F(x)+x. </font>\n",
    "\n",
    "<img src=\"images/residual_block.PNG\" width=\"300\">\n",
    "\n",
    "- <font color=blue> The intution is that, the residual block at least learn the identity mapping with skip/shortcut connection, which does not hurt the performance. </font> If the underlying residual path learns some meaningful data then performance can improve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNext\n",
    "- The model name, ResNeXt, contains Next. It means the next dimension, on top of the ResNet. This next dimension is called the “cardinality” dimension.\n",
    "\n",
    "<img src=\"images/resnext.PNG\" width=\"400\">\n",
    "\n",
    "- Authors tried below 3 variants and found results are same, so finally chose the variant (c)\n",
    "\n",
    "<img src=\"images/resnext_variants.PNG\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Highway Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Activation ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  WRNs (Wide Residual Networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MobileNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xception\n",
    "- Xception stands for “extreme inception.”, as the name suggests, it takes the principles of Inception to an extreme.\n",
    "- The modified depthwise separable convolution is the pointwise convolution followed by a depthwise convolution. This modification is motivated by the inception module in Inception-v3 that 1×1 convolution is done first before any n×n spatial convolutions.\n",
    "\n",
    "<img src=\"images/xception.PNG\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial Transformer Network (STN)\n",
    "- 2015 NIPS paper by Google"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deformable Convolutional Networks (DCN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=DarkOrange> SOTA Super Resolution Networks </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"DarkMagenta\"> Survey of SR Techniques </font>\n",
    "- [2019 SR Survey](papers/2019_2_SR_Survey.pdf)\n",
    "- [2019 SR Survey Deep Journey](papers/2019_4_SR_Survey_Deep_Journey.pdf)\n",
    "\n",
    "---\n",
    "- **SR SOTA Techniques Overview**\n",
    "<img src=\"images/sr_sota_overview.PNG\" width=\"700\">\n",
    "\n",
    "---\n",
    "- <font color=blue> **SR Network designs** </font>\n",
    "<img src=\"images/sr_network_designs.PNG\" width=\"800\">\n",
    "\n",
    "---\n",
    "- **SR Networks**\n",
    "<img src=\"images/sr_overview.PNG\" width=\"700\">\n",
    "\n",
    "---\n",
    "- **SR Networks Performance Comparison**\n",
    "<img src=\"images/sr_sota_comparison.PNG\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SR using Residual Channel Attention Networks (RCAN)\n",
    "- ECCV 2018 paper, so far the best SR paper slightly better than RDN\n",
    "- <font color=blue> It uses Residual In Residual, similar to Residual Dense Block (RDN) paper and it also uses channel-wise attention based on Squeeze and Excite (SE) paper </font>\n",
    "\n",
    "<img src=\"images/sr_rcan.PNG\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=green> New DNN Research Concepts/Papers </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blue> Attention Mechanism (Channel, Spatial) </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBAM: Convolutional Block Attention Module\n",
    "- [Attention paper CBAM 2018 ECCV](papers/2018_Attention_Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.pdf)\n",
    "- **Attention not only tells where to focus, it also improves the representation of interests. Attentions increases representation power of CNNs: focusing on important fea-tures and suppressing unnecessary ones.**\n",
    "- **Channel Attention**: As each channel of a feature map is considered as a feature detector, channel attention focuses on ‘what’ is meaningful given an input image. To compute the channel attention efficiently, we squeeze the spatial dimension of the input feature map. For aggregating spa-tial information, average-pooling has been commonly adopted so far.\n",
    "\n",
    "<img src=\"images/attention_channel_spatial.PNG\" width=\"600\">\n",
    "\n",
    "- **Spatial Attention**: attention. To compute the spatial attention, we first apply average-pooling and max-pooling operations along the channel axis and concatenate them to generate an efficient feature descriptor.\n",
    "\n",
    "--\n",
    "- **Squeeze and Excite (SE) Network**\n",
    "- SE also proposed attention similar to CBAM, but CBAM shows better results\n",
    "\n",
    "<img src=\"images/attention_resnet.PNG\" width=\"300\" align=\"left\"> \n",
    "<img src=\"images/attention_inception.PNG\" width=\"300\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Learning\n",
    "- <font color=blue> In order to achieve larger receptive field and learn higherlevel features without introducing overwhelming parameters, recursive learning, which refers to applying the same modules multiple times in a recursive manner </font>\n",
    "- In practice, recursive learning inherently brings vanishing or exploding gradient problems, consequently some techniques such as residual learning and multisupervision are often combined together with recursive learning for mitigating these problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DRCN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DRRN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transparency by Design (TbD Net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capsule Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero shot learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Federated Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=green> Interview Questions/Skills/JD </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blue> Skills required for ML/DL Engineer Jobs </font>\n",
    "- CNN architectures & algorithms, RNNs, NLP, Reinforcement learning\n",
    "- ML basic algorithms\n",
    "- Linear algebra, Probability, Statistics\n",
    "- DL frameworks (eg: Tensorflow, Keras, PyTorch)\n",
    "- Pyhon, R, jupyter notebooks\n",
    "- ARM, DSP, GPU working experience\n",
    "- Experience in building ML Applications like Image Segmentation, Object Detection, etc\n",
    "- OpenCL, CUDA GPU programming\n",
    "- Publications in CVPR/NIPS/ICML/ICLR would be an added advantage\n",
    "- Computer vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=green> Best Resources </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good resources list\n",
    "- [Over 200 best ML, NLP, Python Tutorials - 2018 Edition](https://medium.com/machine-learning-in-practice/over-200-of-the-best-machine-learning-nlp-and-python-tutorials-2018-edition-dd8cf53cb7dc)\n",
    "- [ML, DL Tutorials](https://github.com/ujjwalkarn/Machine-Learning-Tutorials)\n",
    "- [Machine Learning Introduction- Series](https://medium.com/machine-learning-for-humans/supervised-learning-740383a2feab)\n",
    "- [ML Cheet sheet](https://medium.com/machine-learning-in-practice/cheat-sheet-of-machine-learning-and-python-and-math-cheat-sheets-a4afe4e791b6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good practical resources\n",
    "- [Deep learning with python book](https://github.com/fchollet/deep-learning-with-python-notebooks)\n",
    "- [FastAI](https://github.com/fastai/courses)\n",
    "- [Deeplearning Specialization Coursera](https://github.com/Kulbear/deep-learning-coursera)\n",
    "- [Python machine learning book 2nd edition](https://github.com/rasbt/python-machine-learning-book-2nd-edition/tree/master/code/ch01)\n",
    "- [TesnorFlow examples](https://github.com/aymericdamien/TensorFlow-Examples)\n",
    "- [Data Science Jupyter notebook](https://github.com/donnemartin/data-science-ipython-notebooks#keras-tutorials)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "143px",
    "width": "223px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "227px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
