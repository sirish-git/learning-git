{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#ML-DL-Notes\" data-toc-modified-id=\"ML-DL-Notes-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>ML DL Notes</a></span></li><li><span><a href=\"#-Topics-to-learn-\" data-toc-modified-id=\"-Topics-to-learn--2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span><font color=\"red\"> Topics to learn </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#Learn-Now-(most-important)\" data-toc-modified-id=\"Learn-Now-(most-important)-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Learn Now (most important)</a></span></li><li><span><a href=\"#Learn-later\" data-toc-modified-id=\"Learn-later-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Learn later</a></span></li></ul></li><li><span><a href=\"#-Probability-and-Statistics-\" data-toc-modified-id=\"-Probability-and-Statistics--3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span><font color=\"brown\"> Probability and Statistics </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#Types-of-Probability\" data-toc-modified-id=\"Types-of-Probability-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Types of Probability</a></span></li><li><span><a href=\"#Central-Limit-Theorem-(CLT)\" data-toc-modified-id=\"Central-Limit-Theorem-(CLT)-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Central Limit Theorem (CLT)</a></span></li><li><span><a href=\"#Bernoulli-Distribution\" data-toc-modified-id=\"Bernoulli-Distribution-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Bernoulli Distribution</a></span></li><li><span><a href=\"#Binomial-Theorem\" data-toc-modified-id=\"Binomial-Theorem-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Binomial Theorem</a></span></li><li><span><a href=\"#Standard-Normal-Distribution-or-Z-distribution\" data-toc-modified-id=\"Standard-Normal-Distribution-or-Z-distribution-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Standard Normal Distribution or Z-distribution</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#-$-std(x)-=-\\frac{x---\\mu}{\\sigma}-$-\" data-toc-modified-id=\"-$-std(x)-=-\\frac{x---\\mu}{\\sigma}-$--3.5.0.1\"><span class=\"toc-item-num\">3.5.0.1&nbsp;&nbsp;</span> $ std(x) = \\frac{x - \\mu}{\\sigma} $ </a></span></li></ul></li></ul></li><li><span><a href=\"#Maximum-Likelihood-Estimation-(MLE)\" data-toc-modified-id=\"Maximum-Likelihood-Estimation-(MLE)-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>Maximum Likelihood Estimation (MLE)</a></span></li><li><span><a href=\"#Bayes-Theorem\" data-toc-modified-id=\"Bayes-Theorem-3.7\"><span class=\"toc-item-num\">3.7&nbsp;&nbsp;</span>Bayes Theorem</a></span></li><li><span><a href=\"#Bayesian-Inference\" data-toc-modified-id=\"Bayesian-Inference-3.8\"><span class=\"toc-item-num\">3.8&nbsp;&nbsp;</span>Bayesian Inference</a></span></li><li><span><a href=\"#Naive-Bayes\" data-toc-modified-id=\"Naive-Bayes-3.9\"><span class=\"toc-item-num\">3.9&nbsp;&nbsp;</span>Naive Bayes</a></span></li></ul></li><li><span><a href=\"#-ML/DL-basics-introduction-\" data-toc-modified-id=\"-ML/DL-basics-introduction--4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span><font color=\"brown\"> ML/DL basics introduction </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#-Perceptron-\" data-toc-modified-id=\"-Perceptron--4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span><font color=\"DarkMagenta\"> Perceptron </font></a></span></li><li><span><a href=\"#Parametric-Vs-Non-Parametric-algorithms\" data-toc-modified-id=\"Parametric-Vs-Non-Parametric-algorithms-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Parametric Vs Non-Parametric algorithms</a></span></li><li><span><a href=\"#-Sigmoid-Neuron-\" data-toc-modified-id=\"-Sigmoid-Neuron--4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span><font color=\"DarkMagenta\"> Sigmoid Neuron </font></a></span></li><li><span><a href=\"#Gradient-decent\" data-toc-modified-id=\"Gradient-decent-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Gradient decent</a></span></li><li><span><a href=\"#Backpropagation\" data-toc-modified-id=\"Backpropagation-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Backpropagation</a></span></li><li><span><a href=\"#Overfitting\" data-toc-modified-id=\"Overfitting-4.6\"><span class=\"toc-item-num\">4.6&nbsp;&nbsp;</span>Overfitting</a></span><ul class=\"toc-item\"><li><span><a href=\"#How-to-avoid-overfitting\" data-toc-modified-id=\"How-to-avoid-overfitting-4.6.1\"><span class=\"toc-item-num\">4.6.1&nbsp;&nbsp;</span>How to avoid overfitting</a></span></li></ul></li><li><span><a href=\"#Vanishing-Gradients\" data-toc-modified-id=\"Vanishing-Gradients-4.7\"><span class=\"toc-item-num\">4.7&nbsp;&nbsp;</span>Vanishing Gradients</a></span><ul class=\"toc-item\"><li><span><a href=\"#How-to-avoid-vanishing-gradients\" data-toc-modified-id=\"How-to-avoid-vanishing-gradients-4.7.1\"><span class=\"toc-item-num\">4.7.1&nbsp;&nbsp;</span>How to avoid vanishing gradients</a></span></li></ul></li><li><span><a href=\"#Cross-validation\" data-toc-modified-id=\"Cross-validation-4.8\"><span class=\"toc-item-num\">4.8&nbsp;&nbsp;</span>Cross validation</a></span></li><li><span><a href=\"#Types-of-data\" data-toc-modified-id=\"Types-of-data-4.9\"><span class=\"toc-item-num\">4.9&nbsp;&nbsp;</span>Types of data</a></span></li><li><span><a href=\"#Classification-and-Regression\" data-toc-modified-id=\"Classification-and-Regression-4.10\"><span class=\"toc-item-num\">4.10&nbsp;&nbsp;</span>Classification and Regression</a></span></li></ul></li><li><span><a href=\"#-Supervised-Learning-\" data-toc-modified-id=\"-Supervised-Learning--5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span><font color=\"brown\"> Supervised Learning </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#-Parametric-and-Classification-Algorithms-\" data-toc-modified-id=\"-Parametric-and-Classification-Algorithms--5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span><font color=\"brown\"> Parametric and Classification Algorithms </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#Support-Vector-Machine-(SVM)\" data-toc-modified-id=\"Support-Vector-Machine-(SVM)-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>Support Vector Machine (SVM)</a></span></li><li><span><a href=\"#Logistic-regression\" data-toc-modified-id=\"Logistic-regression-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;</span>Logistic regression</a></span></li></ul></li><li><span><a href=\"#-Parametric-and-Regression-Algorithms-\" data-toc-modified-id=\"-Parametric-and-Regression-Algorithms--5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span><font color=\"brown\"> Parametric and Regression Algorithms </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#Linear-regression\" data-toc-modified-id=\"Linear-regression-5.2.1\"><span class=\"toc-item-num\">5.2.1&nbsp;&nbsp;</span>Linear regression</a></span></li></ul></li><li><span><a href=\"#Non-Parametric-Algorithms\" data-toc-modified-id=\"Non-Parametric-Algorithms-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Non-Parametric Algorithms</a></span><ul class=\"toc-item\"><li><span><a href=\"#K-Nearest-Neighbor-(KNN)\" data-toc-modified-id=\"K-Nearest-Neighbor-(KNN)-5.3.1\"><span class=\"toc-item-num\">5.3.1&nbsp;&nbsp;</span>K-Nearest Neighbor (KNN)</a></span></li><li><span><a href=\"#Decision-Tree\" data-toc-modified-id=\"Decision-Tree-5.3.2\"><span class=\"toc-item-num\">5.3.2&nbsp;&nbsp;</span>Decision Tree</a></span></li><li><span><a href=\"#Random-Forest:--an-ensemble-of-decision-trees\" data-toc-modified-id=\"Random-Forest:--an-ensemble-of-decision-trees-5.3.3\"><span class=\"toc-item-num\">5.3.3&nbsp;&nbsp;</span>Random Forest:  an ensemble of decision trees</a></span></li></ul></li></ul></li><li><span><a href=\"#-Unsupervised-Learning-\" data-toc-modified-id=\"-Unsupervised-Learning--6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span><font color=\"brown\"> Unsupervised Learning </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#Clusttering\" data-toc-modified-id=\"Clusttering-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Clusttering</a></span><ul class=\"toc-item\"><li><span><a href=\"#K-means-clusttering\" data-toc-modified-id=\"K-means-clusttering-6.1.1\"><span class=\"toc-item-num\">6.1.1&nbsp;&nbsp;</span>K-means clusttering</a></span></li><li><span><a href=\"#Expectation–Maximization-(EM)-Clustering-using-Gaussian-Mixture-Models-(GMM)\" data-toc-modified-id=\"Expectation–Maximization-(EM)-Clustering-using-Gaussian-Mixture-Models-(GMM)-6.1.2\"><span class=\"toc-item-num\">6.1.2&nbsp;&nbsp;</span>Expectation–Maximization (EM) Clustering using Gaussian Mixture Models (GMM)</a></span></li><li><span><a href=\"#Hierarchical-Clusttering\" data-toc-modified-id=\"Hierarchical-Clusttering-6.1.3\"><span class=\"toc-item-num\">6.1.3&nbsp;&nbsp;</span>Hierarchical Clusttering</a></span></li></ul></li><li><span><a href=\"#Dimensionality-Reduction\" data-toc-modified-id=\"Dimensionality-Reduction-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Dimensionality Reduction</a></span><ul class=\"toc-item\"><li><span><a href=\"#Principal-Component-Analysis-(PCA)\" data-toc-modified-id=\"Principal-Component-Analysis-(PCA)-6.2.1\"><span class=\"toc-item-num\">6.2.1&nbsp;&nbsp;</span>Principal Component Analysis (PCA)</a></span></li><li><span><a href=\"#Singular-Value-Decomposition-(SVD)\" data-toc-modified-id=\"Singular-Value-Decomposition-(SVD)-6.2.2\"><span class=\"toc-item-num\">6.2.2&nbsp;&nbsp;</span>Singular Value Decomposition (SVD)</a></span></li></ul></li></ul></li><li><span><a href=\"#-Loss-Functions-\" data-toc-modified-id=\"-Loss-Functions--7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span><font color=\"brown\"> Loss Functions </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#-Classification-Loss-Functions-\" data-toc-modified-id=\"-Classification-Loss-Functions--7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span><font color=\"brown\"> Classification Loss Functions </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#Square-loss\" data-toc-modified-id=\"Square-loss-7.1.1\"><span class=\"toc-item-num\">7.1.1&nbsp;&nbsp;</span>Square loss</a></span></li><li><span><a href=\"#Hinge-loss\" data-toc-modified-id=\"Hinge-loss-7.1.2\"><span class=\"toc-item-num\">7.1.2&nbsp;&nbsp;</span>Hinge loss</a></span></li><li><span><a href=\"#Logistic-loss\" data-toc-modified-id=\"Logistic-loss-7.1.3\"><span class=\"toc-item-num\">7.1.3&nbsp;&nbsp;</span>Logistic loss</a></span></li><li><span><a href=\"#Cross-entropy-loss\" data-toc-modified-id=\"Cross-entropy-loss-7.1.4\"><span class=\"toc-item-num\">7.1.4&nbsp;&nbsp;</span>Cross entropy loss</a></span></li></ul></li><li><span><a href=\"#-Regression-Loss-Functions-\" data-toc-modified-id=\"-Regression-Loss-Functions--7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span><font color=\"brown\"> Regression Loss Functions </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#L2-Loss,-Mean-Square-Error-(MSE),-Quadratic-loss\" data-toc-modified-id=\"L2-Loss,-Mean-Square-Error-(MSE),-Quadratic-loss-7.2.1\"><span class=\"toc-item-num\">7.2.1&nbsp;&nbsp;</span>L2 Loss, Mean Square Error (MSE), Quadratic loss</a></span><ul class=\"toc-item\"><li><span><a href=\"#-$-MSE-=-\\sum\\limits_{i=1}^n--{(y_i---y_i^p)}^2-$-\" data-toc-modified-id=\"-$-MSE-=-\\sum\\limits_{i=1}^n--{(y_i---y_i^p)}^2-$--7.2.1.1\"><span class=\"toc-item-num\">7.2.1.1&nbsp;&nbsp;</span> $ MSE = \\sum\\limits_{i=1}^n  {(y_i - y_i^p)}^2 $ </a></span></li></ul></li><li><span><a href=\"#L1-Loss,-Mean-Absolute-Error-(MAE)\" data-toc-modified-id=\"L1-Loss,-Mean-Absolute-Error-(MAE)-7.2.2\"><span class=\"toc-item-num\">7.2.2&nbsp;&nbsp;</span>L1 Loss, Mean Absolute Error (MAE)</a></span><ul class=\"toc-item\"><li><span><a href=\"#-$-MAE-=-\\sum\\limits_{i=1}^n--{|y_i---y_i^p|}-$-\" data-toc-modified-id=\"-$-MAE-=-\\sum\\limits_{i=1}^n--{|y_i---y_i^p|}-$--7.2.2.1\"><span class=\"toc-item-num\">7.2.2.1&nbsp;&nbsp;</span> $ MAE = \\sum\\limits_{i=1}^n  {|y_i - y_i^p|} $ </a></span></li></ul></li><li><span><a href=\"#L1-vs-L2-Loss\" data-toc-modified-id=\"L1-vs-L2-Loss-7.2.3\"><span class=\"toc-item-num\">7.2.3&nbsp;&nbsp;</span>L1 vs L2 Loss</a></span></li><li><span><a href=\"#Huber-Loss,-Smooth-Mean-Absolute-Error\" data-toc-modified-id=\"Huber-Loss,-Smooth-Mean-Absolute-Error-7.2.4\"><span class=\"toc-item-num\">7.2.4&nbsp;&nbsp;</span>Huber Loss, Smooth Mean Absolute Error</a></span></li><li><span><a href=\"#Log-CosH-Loss\" data-toc-modified-id=\"Log-CosH-Loss-7.2.5\"><span class=\"toc-item-num\">7.2.5&nbsp;&nbsp;</span>Log-CosH Loss</a></span></li><li><span><a href=\"#Quantile-Loss\" data-toc-modified-id=\"Quantile-Loss-7.2.6\"><span class=\"toc-item-num\">7.2.6&nbsp;&nbsp;</span>Quantile Loss</a></span></li></ul></li><li><span><a href=\"#Triplet-loss-(ex:-face-recognition)\" data-toc-modified-id=\"Triplet-loss-(ex:-face-recognition)-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>Triplet loss (ex: face recognition)</a></span></li></ul></li><li><span><a href=\"#-Regularization-Algorithms-\" data-toc-modified-id=\"-Regularization-Algorithms--8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span><font color=\"brown\"> Regularization Algorithms </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#-L1-(Lasso)-and-L2-(Ridge)-as-Reguralization-\" data-toc-modified-id=\"-L1-(Lasso)-and-L2-(Ridge)-as-Reguralization--8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span><font color=\"DarkMagenta\"> L1 (Lasso) and L2 (Ridge) as Reguralization </font></a></span></li><li><span><a href=\"#L2-or-Frobenius-Regularization-or-Weight-decay\" data-toc-modified-id=\"L2-or-Frobenius-Regularization-or-Weight-decay-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>L2 or Frobenius Regularization or Weight decay</a></span><ul class=\"toc-item\"><li><span><a href=\"#-$-Loss-(E)-=-\\frac{1}{m}\\sum\\limits_{i=1}^n--{(y_i---y_i^p)}^2-+-\\frac{\\lambda}{2m}-\\sum\\limits_{l=1}^L-{||W_l||^2}-$-\" data-toc-modified-id=\"-$-Loss-(E)-=-\\frac{1}{m}\\sum\\limits_{i=1}^n--{(y_i---y_i^p)}^2-+-\\frac{\\lambda}{2m}-\\sum\\limits_{l=1}^L-{||W_l||^2}-$--8.2.1\"><span class=\"toc-item-num\">8.2.1&nbsp;&nbsp;</span> $ Loss (E) = \\frac{1}{m}\\sum\\limits_{i=1}^n  {(y_i - y_i^p)}^2 + \\frac{\\lambda}{2m} \\sum\\limits_{l=1}^L {||W_l||^2} $ </a></span></li><li><span><a href=\"#-$-dW-=-dNormal-+-\\frac-{\\lambda}{m}-W_l-$-\" data-toc-modified-id=\"-$-dW-=-dNormal-+-\\frac-{\\lambda}{m}-W_l-$--8.2.2\"><span class=\"toc-item-num\">8.2.2&nbsp;&nbsp;</span> $ dW = dNormal + \\frac {\\lambda}{m} W_l $ </a></span></li><li><span><a href=\"#-$-W_l-=-W_l---\\sigma-dW-$-\" data-toc-modified-id=\"-$-W_l-=-W_l---\\sigma-dW-$--8.2.3\"><span class=\"toc-item-num\">8.2.3&nbsp;&nbsp;</span> $ W_l = W_l - \\sigma dW $ </a></span></li><li><span><a href=\"#-$-(1---\\frac{\\sigma-\\lambda}-{m})-W_l---\\sigma-dNormal-$-\" data-toc-modified-id=\"-$-(1---\\frac{\\sigma-\\lambda}-{m})-W_l---\\sigma-dNormal-$--8.2.4\"><span class=\"toc-item-num\">8.2.4&nbsp;&nbsp;</span> $ (1 - \\frac{\\sigma \\lambda} {m}) W_l - \\sigma dNormal $ </a></span></li></ul></li><li><span><a href=\"#Drop-out\" data-toc-modified-id=\"Drop-out-8.3\"><span class=\"toc-item-num\">8.3&nbsp;&nbsp;</span>Drop-out</a></span></li><li><span><a href=\"#Early-Stopping\" data-toc-modified-id=\"Early-Stopping-8.4\"><span class=\"toc-item-num\">8.4&nbsp;&nbsp;</span>Early Stopping</a></span></li><li><span><a href=\"#Data-Augmentation-to-avoid-overfitting\" data-toc-modified-id=\"Data-Augmentation-to-avoid-overfitting-8.5\"><span class=\"toc-item-num\">8.5&nbsp;&nbsp;</span>Data Augmentation to avoid overfitting</a></span></li></ul></li><li><span><a href=\"#-CNN-Layer-Theory-\" data-toc-modified-id=\"-CNN-Layer-Theory--9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span><font color=\"brown\"> CNN Layer Theory </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#Rectified-Linear-Unit-(ReLU)\" data-toc-modified-id=\"Rectified-Linear-Unit-(ReLU)-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>Rectified Linear Unit (ReLU)</a></span></li><li><span><a href=\"#Softmax-regression-for-multi-class-classification\" data-toc-modified-id=\"Softmax-regression-for-multi-class-classification-9.2\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;</span>Softmax regression for multi-class classification</a></span><ul class=\"toc-item\"><li><span><a href=\"#-$-Softmax_{prob}-=-\\frac-{e^Z}{\\sum{e^Z}}-$-\" data-toc-modified-id=\"-$-Softmax_{prob}-=-\\frac-{e^Z}{\\sum{e^Z}}-$--9.2.1\"><span class=\"toc-item-num\">9.2.1&nbsp;&nbsp;</span> $ Softmax_{prob} = \\frac {e^Z}{\\sum{e^Z}} $ </a></span></li></ul></li></ul></li><li><span><a href=\"#Data-Pre-Processing-or-Feature-Scaling\" data-toc-modified-id=\"Data-Pre-Processing-or-Feature-Scaling-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Data Pre-Processing or Feature Scaling</a></span><ul class=\"toc-item\"><li><span><a href=\"#Standard-deviation\" data-toc-modified-id=\"Standard-deviation-10.1\"><span class=\"toc-item-num\">10.1&nbsp;&nbsp;</span>Standard deviation</a></span></li><li><span><a href=\"#Mean-Normalization\" data-toc-modified-id=\"Mean-Normalization-10.2\"><span class=\"toc-item-num\">10.2&nbsp;&nbsp;</span>Mean Normalization</a></span></li><li><span><a href=\"#Normalization-and-Standardization\" data-toc-modified-id=\"Normalization-and-Standardization-10.3\"><span class=\"toc-item-num\">10.3&nbsp;&nbsp;</span>Normalization and Standardization</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#-$-norm(x)-=-\\frac{x---min(x)}{max(x)---min(x)}-$-\" data-toc-modified-id=\"-$-norm(x)-=-\\frac{x---min(x)}{max(x)---min(x)}-$--10.3.0.1\"><span class=\"toc-item-num\">10.3.0.1&nbsp;&nbsp;</span> $ norm(x) = \\frac{x - min(x)}{max(x) - min(x)} $ </a></span></li><li><span><a href=\"#-$-std(x)-=-\\frac{x---\\mu}{\\sigma}-$-\" data-toc-modified-id=\"-$-std(x)-=-\\frac{x---\\mu}{\\sigma}-$--10.3.0.2\"><span class=\"toc-item-num\">10.3.0.2&nbsp;&nbsp;</span> $ std(x) = \\frac{x - \\mu}{\\sigma} $ </a></span></li></ul></li></ul></li><li><span><a href=\"#Covariance-and-Correlation\" data-toc-modified-id=\"Covariance-and-Correlation-10.4\"><span class=\"toc-item-num\">10.4&nbsp;&nbsp;</span>Covariance and Correlation</a></span></li><li><span><a href=\"#Whitening\" data-toc-modified-id=\"Whitening-10.5\"><span class=\"toc-item-num\">10.5&nbsp;&nbsp;</span>Whitening</a></span></li><li><span><a href=\"#Batch-Normalization\" data-toc-modified-id=\"Batch-Normalization-10.6\"><span class=\"toc-item-num\">10.6&nbsp;&nbsp;</span>Batch Normalization</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#-$-Z_{norm}-=-\\frac{Z---\\mu}{\\sqrt{\\sigma^2+\\epsilon}}-$-\" data-toc-modified-id=\"-$-Z_{norm}-=-\\frac{Z---\\mu}{\\sqrt{\\sigma^2+\\epsilon}}-$--10.6.0.1\"><span class=\"toc-item-num\">10.6.0.1&nbsp;&nbsp;</span> $ Z_{norm} = \\frac{Z - \\mu}{\\sqrt{\\sigma^2+\\epsilon}} $ </a></span></li><li><span><a href=\"#-$-Z_{N}-=-\\gamma-*-Z_{norm}-+-\\beta-$-\" data-toc-modified-id=\"-$-Z_{N}-=-\\gamma-*-Z_{norm}-+-\\beta-$--10.6.0.2\"><span class=\"toc-item-num\">10.6.0.2&nbsp;&nbsp;</span> $ Z_{N} = \\gamma * Z_{norm} + \\beta $ </a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Back-Propagation\" data-toc-modified-id=\"Back-Propagation-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Back Propagation</a></span></li><li><span><a href=\"#Gradient-Descent\" data-toc-modified-id=\"Gradient-Descent-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;</span>Gradient Descent</a></span></li><li><span><a href=\"#Deep-Learning-Training\" data-toc-modified-id=\"Deep-Learning-Training-13\"><span class=\"toc-item-num\">13&nbsp;&nbsp;</span>Deep Learning Training</a></span><ul class=\"toc-item\"><li><span><a href=\"#-Bias-Variance-Trade-Off-\" data-toc-modified-id=\"-Bias-Variance-Trade-Off--13.1\"><span class=\"toc-item-num\">13.1&nbsp;&nbsp;</span><font color=\"brown\"> Bias Variance Trade-Off </font></a></span></li><li><span><a href=\"#Weight-Initialization\" data-toc-modified-id=\"Weight-Initialization-13.2\"><span class=\"toc-item-num\">13.2&nbsp;&nbsp;</span>Weight Initialization</a></span><ul class=\"toc-item\"><li><span><a href=\"#He-initialization\" data-toc-modified-id=\"He-initialization-13.2.1\"><span class=\"toc-item-num\">13.2.1&nbsp;&nbsp;</span>He initialization</a></span></li></ul></li><li><span><a href=\"#Gradient-checking\" data-toc-modified-id=\"Gradient-checking-13.3\"><span class=\"toc-item-num\">13.3&nbsp;&nbsp;</span>Gradient checking</a></span></li><li><span><a href=\"#Gradient-Clipping\" data-toc-modified-id=\"Gradient-Clipping-13.4\"><span class=\"toc-item-num\">13.4&nbsp;&nbsp;</span>Gradient Clipping</a></span></li></ul></li><li><span><a href=\"#Hyper-Parameter-Tuning\" data-toc-modified-id=\"Hyper-Parameter-Tuning-14\"><span class=\"toc-item-num\">14&nbsp;&nbsp;</span>Hyper Parameter Tuning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Mini-Batch-Gradient-Descent\" data-toc-modified-id=\"Mini-Batch-Gradient-Descent-14.1\"><span class=\"toc-item-num\">14.1&nbsp;&nbsp;</span>Mini-Batch Gradient Descent</a></span></li><li><span><a href=\"#Exponentially-Weighted-Moving-Average\" data-toc-modified-id=\"Exponentially-Weighted-Moving-Average-14.2\"><span class=\"toc-item-num\">14.2&nbsp;&nbsp;</span>Exponentially Weighted Moving Average</a></span><ul class=\"toc-item\"><li><span><a href=\"#-$-Val_t-=-\\beta-*-Val_{t-1}-+-(1-\\beta)-*-currentsample-$-\" data-toc-modified-id=\"-$-Val_t-=-\\beta-*-Val_{t-1}-+-(1-\\beta)-*-currentsample-$--14.2.1\"><span class=\"toc-item-num\">14.2.1&nbsp;&nbsp;</span> $ Val_t = \\beta * Val_{t-1} + (1-\\beta) * currentsample $ </a></span></li><li><span><a href=\"#Bias-correction-in-exponential-weighted-average\" data-toc-modified-id=\"Bias-correction-in-exponential-weighted-average-14.2.2\"><span class=\"toc-item-num\">14.2.2&nbsp;&nbsp;</span>Bias correction in exponential weighted average</a></span></li></ul></li><li><span><a href=\"#Gradient-descent-with-momentum\" data-toc-modified-id=\"Gradient-descent-with-momentum-14.3\"><span class=\"toc-item-num\">14.3&nbsp;&nbsp;</span>Gradient descent with momentum</a></span><ul class=\"toc-item\"><li><span><a href=\"#-$-V_{dw}-=-\\beta-*-V_{dw}-+-(1-\\beta)-*-dw-$-\" data-toc-modified-id=\"-$-V_{dw}-=-\\beta-*-V_{dw}-+-(1-\\beta)-*-dw-$--14.3.1\"><span class=\"toc-item-num\">14.3.1&nbsp;&nbsp;</span> $ V_{dw} = \\beta * V_{dw} + (1-\\beta) * dw $ </a></span></li><li><span><a href=\"#-$-W-=-W---\\alpha-*-V_{dw}-$-\" data-toc-modified-id=\"-$-W-=-W---\\alpha-*-V_{dw}-$--14.3.2\"><span class=\"toc-item-num\">14.3.2&nbsp;&nbsp;</span> $ W = W - \\alpha * V_{dw} $ </a></span></li></ul></li><li><span><a href=\"#RMSprop-(Root-Mean-Square-prop)\" data-toc-modified-id=\"RMSprop-(Root-Mean-Square-prop)-14.4\"><span class=\"toc-item-num\">14.4&nbsp;&nbsp;</span>RMSprop (Root Mean Square prop)</a></span><ul class=\"toc-item\"><li><span><a href=\"#-$-S_{dw}-=-\\beta-*-S_{dw}-+-(1-\\beta)-*-dw-*-dw-$-\" data-toc-modified-id=\"-$-S_{dw}-=-\\beta-*-S_{dw}-+-(1-\\beta)-*-dw-*-dw-$--14.4.1\"><span class=\"toc-item-num\">14.4.1&nbsp;&nbsp;</span> $ S_{dw} = \\beta * S_{dw} + (1-\\beta) * dw * dw $ </a></span></li><li><span><a href=\"#-$-W-=-W---\\alpha-*-\\frac-{dw}{\\sqrt{1--S_{dw}}}-$-\" data-toc-modified-id=\"-$-W-=-W---\\alpha-*-\\frac-{dw}{\\sqrt{1--S_{dw}}}-$--14.4.2\"><span class=\"toc-item-num\">14.4.2&nbsp;&nbsp;</span> $ W = W - \\alpha * \\frac {dw}{\\sqrt{1- S_{dw}}} $ </a></span></li></ul></li><li><span><a href=\"#Adam-Optimizer\" data-toc-modified-id=\"Adam-Optimizer-14.5\"><span class=\"toc-item-num\">14.5&nbsp;&nbsp;</span>Adam Optimizer</a></span><ul class=\"toc-item\"><li><span><a href=\"#-$-V_{dw}-=-\\beta_1-*-V_{dw}-+-(1-\\beta_1)-*-dw-$-\" data-toc-modified-id=\"-$-V_{dw}-=-\\beta_1-*-V_{dw}-+-(1-\\beta_1)-*-dw-$--14.5.1\"><span class=\"toc-item-num\">14.5.1&nbsp;&nbsp;</span> $ V_{dw} = \\beta_1 * V_{dw} + (1-\\beta_1) * dw $ </a></span></li><li><span><a href=\"#-$-S_{dw}-=-\\beta_2-*-S_{dw}-+-(1-\\beta_2)-*-dw-*-dw-$-\" data-toc-modified-id=\"-$-S_{dw}-=-\\beta_2-*-S_{dw}-+-(1-\\beta_2)-*-dw-*-dw-$--14.5.2\"><span class=\"toc-item-num\">14.5.2&nbsp;&nbsp;</span> $ S_{dw} = \\beta_2 * S_{dw} + (1-\\beta_2) * dw * dw $ </a></span></li><li><span><a href=\"#-$-W-=-W---\\alpha-*-\\frac-{V_{dw}}{\\sqrt{1--S_{dw}}}-$-\" data-toc-modified-id=\"-$-W-=-W---\\alpha-*-\\frac-{V_{dw}}{\\sqrt{1--S_{dw}}}-$--14.5.3\"><span class=\"toc-item-num\">14.5.3&nbsp;&nbsp;</span> $ W = W - \\alpha * \\frac {V_{dw}}{\\sqrt{1- S_{dw}}} $ </a></span></li></ul></li><li><span><a href=\"#Learning-rate-decay\" data-toc-modified-id=\"Learning-rate-decay-14.6\"><span class=\"toc-item-num\">14.6&nbsp;&nbsp;</span>Learning rate decay</a></span></li><li><span><a href=\"##layers-and-#hidden-units-hyper-parameters\" data-toc-modified-id=\"#layers-and-#hidden-units-hyper-parameters-14.7\"><span class=\"toc-item-num\">14.7&nbsp;&nbsp;</span>#layers and #hidden units hyper parameters</a></span></li><li><span><a href=\"#Random-Search-for-hyper-parameter-tuning\" data-toc-modified-id=\"Random-Search-for-hyper-parameter-tuning-14.8\"><span class=\"toc-item-num\">14.8&nbsp;&nbsp;</span>Random Search for hyper parameter tuning</a></span></li><li><span><a href=\"#Coarse-to-fine-sampling-of-hyper-param-search-space\" data-toc-modified-id=\"Coarse-to-fine-sampling-of-hyper-param-search-space-14.9\"><span class=\"toc-item-num\">14.9&nbsp;&nbsp;</span>Coarse to fine sampling of hyper param search space</a></span></li><li><span><a href=\"#Train-dev-set-for-different-distribution-of-train,-dev-sets\" data-toc-modified-id=\"Train-dev-set-for-different-distribution-of-train,-dev-sets-14.10\"><span class=\"toc-item-num\">14.10&nbsp;&nbsp;</span>Train-dev set for different distribution of train, dev sets</a></span></li></ul></li><li><span><a href=\"#DNN-Performance-Tuning\" data-toc-modified-id=\"DNN-Performance-Tuning-15\"><span class=\"toc-item-num\">15&nbsp;&nbsp;</span>DNN Performance Tuning</a></span></li><li><span><a href=\"#TensorFlow\" data-toc-modified-id=\"TensorFlow-16\"><span class=\"toc-item-num\">16&nbsp;&nbsp;</span>TensorFlow</a></span><ul class=\"toc-item\"><li><span><a href=\"#TensorFlow-API-Hierarchy\" data-toc-modified-id=\"TensorFlow-API-Hierarchy-16.1\"><span class=\"toc-item-num\">16.1&nbsp;&nbsp;</span>TensorFlow API Hierarchy</a></span></li><li><span><a href=\"#Execution-modes-(Lazy,-Eager)\" data-toc-modified-id=\"Execution-modes-(Lazy,-Eager)-16.2\"><span class=\"toc-item-num\">16.2&nbsp;&nbsp;</span>Execution modes (Lazy, Eager)</a></span></li><li><span><a href=\"#Session.run-example\" data-toc-modified-id=\"Session.run-example-16.3\"><span class=\"toc-item-num\">16.3&nbsp;&nbsp;</span>Session.run example</a></span></li><li><span><a href=\"#Variables\" data-toc-modified-id=\"Variables-16.4\"><span class=\"toc-item-num\">16.4&nbsp;&nbsp;</span>Variables</a></span></li><li><span><a href=\"#tf.trainable_variables()\" data-toc-modified-id=\"tf.trainable_variables()-16.5\"><span class=\"toc-item-num\">16.5&nbsp;&nbsp;</span>tf.trainable_variables()</a></span></li><li><span><a href=\"#tf.get_default_graph()\" data-toc-modified-id=\"tf.get_default_graph()-16.6\"><span class=\"toc-item-num\">16.6&nbsp;&nbsp;</span>tf.get_default_graph()</a></span></li><li><span><a href=\"#tf.placeholder\" data-toc-modified-id=\"tf.placeholder-16.7\"><span class=\"toc-item-num\">16.7&nbsp;&nbsp;</span>tf.placeholder</a></span></li><li><span><a href=\"#Global-initialization\" data-toc-modified-id=\"Global-initialization-16.8\"><span class=\"toc-item-num\">16.8&nbsp;&nbsp;</span>Global initialization</a></span></li><li><span><a href=\"#Estimatore-API\" data-toc-modified-id=\"Estimatore-API-16.9\"><span class=\"toc-item-num\">16.9&nbsp;&nbsp;</span>Estimatore API</a></span></li><li><span><a href=\"#Dataset-API\" data-toc-modified-id=\"Dataset-API-16.10\"><span class=\"toc-item-num\">16.10&nbsp;&nbsp;</span>Dataset API</a></span></li><li><span><a href=\"#Debugging-TensorFlow-Programs\" data-toc-modified-id=\"Debugging-TensorFlow-Programs-16.11\"><span class=\"toc-item-num\">16.11&nbsp;&nbsp;</span>Debugging TensorFlow Programs</a></span><ul class=\"toc-item\"><li><span><a href=\"#Fixing-shape-issues\" data-toc-modified-id=\"Fixing-shape-issues-16.11.1\"><span class=\"toc-item-num\">16.11.1&nbsp;&nbsp;</span>Fixing shape issues</a></span></li><li><span><a href=\"#Fixing-data-type-problems\" data-toc-modified-id=\"Fixing-data-type-problems-16.11.2\"><span class=\"toc-item-num\">16.11.2&nbsp;&nbsp;</span>Fixing data type problems</a></span></li><li><span><a href=\"#Debugging-the-execution\" data-toc-modified-id=\"Debugging-the-execution-16.11.3\"><span class=\"toc-item-num\">16.11.3&nbsp;&nbsp;</span>Debugging the execution</a></span><ul class=\"toc-item\"><li><span><a href=\"#Change-logging-level\" data-toc-modified-id=\"Change-logging-level-16.11.3.1\"><span class=\"toc-item-num\">16.11.3.1&nbsp;&nbsp;</span>Change logging level</a></span></li><li><span><a href=\"#tf.Print()\" data-toc-modified-id=\"tf.Print()-16.11.3.2\"><span class=\"toc-item-num\">16.11.3.2&nbsp;&nbsp;</span>tf.Print()</a></span></li><li><span><a href=\"#tf.debug()\" data-toc-modified-id=\"tf.debug()-16.11.3.3\"><span class=\"toc-item-num\">16.11.3.3&nbsp;&nbsp;</span>tf.debug()</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Ensemble-Algorithms\" data-toc-modified-id=\"Ensemble-Algorithms-17\"><span class=\"toc-item-num\">17&nbsp;&nbsp;</span>Ensemble Algorithms</a></span><ul class=\"toc-item\"><li><span><a href=\"#Voting\" data-toc-modified-id=\"Voting-17.1\"><span class=\"toc-item-num\">17.1&nbsp;&nbsp;</span>Voting</a></span></li><li><span><a href=\"#Bagging-or-Bootstrap-Aggregation\" data-toc-modified-id=\"Bagging-or-Bootstrap-Aggregation-17.2\"><span class=\"toc-item-num\">17.2&nbsp;&nbsp;</span>Bagging or Bootstrap Aggregation</a></span></li><li><span><a href=\"#Random-Forests\" data-toc-modified-id=\"Random-Forests-17.3\"><span class=\"toc-item-num\">17.3&nbsp;&nbsp;</span>Random Forests</a></span></li><li><span><a href=\"#AdaBoost\" data-toc-modified-id=\"AdaBoost-17.4\"><span class=\"toc-item-num\">17.4&nbsp;&nbsp;</span>AdaBoost</a></span></li><li><span><a href=\"#Gradient-Boosting\" data-toc-modified-id=\"Gradient-Boosting-17.5\"><span class=\"toc-item-num\">17.5&nbsp;&nbsp;</span>Gradient Boosting</a></span></li><li><span><a href=\"#XGBoost\" data-toc-modified-id=\"XGBoost-17.6\"><span class=\"toc-item-num\">17.6&nbsp;&nbsp;</span>XGBoost</a></span></li></ul></li><li><span><a href=\"#Recurrent-Neural-Networks-(RNN)\" data-toc-modified-id=\"Recurrent-Neural-Networks-(RNN)-18\"><span class=\"toc-item-num\">18&nbsp;&nbsp;</span>Recurrent Neural Networks (RNN)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Why-RNNs?\" data-toc-modified-id=\"Why-RNNs?-18.1\"><span class=\"toc-item-num\">18.1&nbsp;&nbsp;</span>Why RNNs?</a></span></li><li><span><a href=\"#Simple-RNN\" data-toc-modified-id=\"Simple-RNN-18.2\"><span class=\"toc-item-num\">18.2&nbsp;&nbsp;</span>Simple RNN</a></span><ul class=\"toc-item\"><li><span><a href=\"#Simple-RNN-equations\" data-toc-modified-id=\"Simple-RNN-equations-18.2.1\"><span class=\"toc-item-num\">18.2.1&nbsp;&nbsp;</span>Simple RNN equations</a></span></li><li><span><a href=\"#Simplified-RNN-equations-notation\" data-toc-modified-id=\"Simplified-RNN-equations-notation-18.2.2\"><span class=\"toc-item-num\">18.2.2&nbsp;&nbsp;</span>Simplified RNN equations notation</a></span></li><li><span><a href=\"#Simple-RNN-representation\" data-toc-modified-id=\"Simple-RNN-representation-18.2.3\"><span class=\"toc-item-num\">18.2.3&nbsp;&nbsp;</span>Simple RNN representation</a></span></li></ul></li><li><span><a href=\"#BPTT-(Back-Prop-Through-Time)\" data-toc-modified-id=\"BPTT-(Back-Prop-Through-Time)-18.3\"><span class=\"toc-item-num\">18.3&nbsp;&nbsp;</span>BPTT (Back Prop Through Time)</a></span></li><li><span><a href=\"#Name-Entity-Recognition-(NER)\" data-toc-modified-id=\"Name-Entity-Recognition-(NER)-18.4\"><span class=\"toc-item-num\">18.4&nbsp;&nbsp;</span>Name Entity Recognition (NER)</a></span></li><li><span><a href=\"#Language-Modelling\" data-toc-modified-id=\"Language-Modelling-18.5\"><span class=\"toc-item-num\">18.5&nbsp;&nbsp;</span>Language Modelling</a></span><ul class=\"toc-item\"><li><span><a href=\"#Tokenization\" data-toc-modified-id=\"Tokenization-18.5.1\"><span class=\"toc-item-num\">18.5.1&nbsp;&nbsp;</span>Tokenization</a></span></li><li><span><a href=\"#Word-vs-Character-RNN\" data-toc-modified-id=\"Word-vs-Character-RNN-18.5.2\"><span class=\"toc-item-num\">18.5.2&nbsp;&nbsp;</span>Word vs Character RNN</a></span></li></ul></li><li><span><a href=\"#GRU-(Gated-Recurrent-Unit)\" data-toc-modified-id=\"GRU-(Gated-Recurrent-Unit)-18.6\"><span class=\"toc-item-num\">18.6&nbsp;&nbsp;</span>GRU (Gated Recurrent Unit)</a></span></li><li><span><a href=\"#LSTM-(Long-Short-Term-Memory)\" data-toc-modified-id=\"LSTM-(Long-Short-Term-Memory)-18.7\"><span class=\"toc-item-num\">18.7&nbsp;&nbsp;</span>LSTM (Long Short Term Memory)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Peephole-connection\" data-toc-modified-id=\"Peephole-connection-18.7.1\"><span class=\"toc-item-num\">18.7.1&nbsp;&nbsp;</span>Peephole connection</a></span></li></ul></li><li><span><a href=\"#BRNN-(Bi-directional-RNN)\" data-toc-modified-id=\"BRNN-(Bi-directional-RNN)-18.8\"><span class=\"toc-item-num\">18.8&nbsp;&nbsp;</span>BRNN (Bi-directional RNN)</a></span></li><li><span><a href=\"#Deep-RNN\" data-toc-modified-id=\"Deep-RNN-18.9\"><span class=\"toc-item-num\">18.9&nbsp;&nbsp;</span>Deep RNN</a></span></li></ul></li><li><span><a href=\"#Reinforcement-Learning\" data-toc-modified-id=\"Reinforcement-Learning-19\"><span class=\"toc-item-num\">19&nbsp;&nbsp;</span>Reinforcement Learning</a></span></li><li><span><a href=\"#-New-DNN-Research-Concepts\" data-toc-modified-id=\"-New-DNN-Research-Concepts-20\"><span class=\"toc-item-num\">20&nbsp;&nbsp;</span><font color=\"green\"> New DNN Research Concepts</font></a></span><ul class=\"toc-item\"><li><span><a href=\"#Transparency-by-Design-(TbD-Net)\" data-toc-modified-id=\"Transparency-by-Design-(TbD-Net)-20.1\"><span class=\"toc-item-num\">20.1&nbsp;&nbsp;</span>Transparency by Design (TbD Net)</a></span></li><li><span><a href=\"#Capsule-Net\" data-toc-modified-id=\"Capsule-Net-20.2\"><span class=\"toc-item-num\">20.2&nbsp;&nbsp;</span>Capsule Net</a></span></li><li><span><a href=\"#Zero-shot-learning\" data-toc-modified-id=\"Zero-shot-learning-20.3\"><span class=\"toc-item-num\">20.3&nbsp;&nbsp;</span>Zero shot learning</a></span></li></ul></li><li><span><a href=\"#-Interview-Questions-\" data-toc-modified-id=\"-Interview-Questions--21\"><span class=\"toc-item-num\">21&nbsp;&nbsp;</span><font color=\"orange\"> Interview Questions </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#Skills-required-for-ML/DL-Engineer-Jobs\" data-toc-modified-id=\"Skills-required-for-ML/DL-Engineer-Jobs-21.1\"><span class=\"toc-item-num\">21.1&nbsp;&nbsp;</span>Skills required for ML/DL Engineer Jobs</a></span></li></ul></li><li><span><a href=\"#-Best-Resources-\" data-toc-modified-id=\"-Best-Resources--22\"><span class=\"toc-item-num\">22&nbsp;&nbsp;</span><font color=\"brown\"> Best Resources </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#Good-resources-list\" data-toc-modified-id=\"Good-resources-list-22.1\"><span class=\"toc-item-num\">22.1&nbsp;&nbsp;</span>Good resources list</a></span></li><li><span><a href=\"#Good-practical-resources\" data-toc-modified-id=\"Good-practical-resources-22.2\"><span class=\"toc-item-num\">22.2&nbsp;&nbsp;</span>Good practical resources</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# ML DL Notes\n",
    "- ** Use images/diagrams for clarity, update below sections with images**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red> Topics to learn </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn Now (most important)\n",
    "- [** ML Cheat sheet **](https://medium.com/machine-learning-in-practice/cheat-sheet-of-machine-learning-and-python-and-math-cheat-sheets-a4afe4e791b6)\n",
    "- Probability\n",
    "    - Bayes theorem, Naive bayes\n",
    "    - Prior and posterior\n",
    "    - MAP\n",
    "    - sampling methods\n",
    "- CNN layers theory\n",
    "    - Softmax\n",
    "    - Convolution types\n",
    "        - Depthwise separable convolution\n",
    "        - Shufflenet\n",
    "        - transposed convolution\n",
    "        - subpixel convolution\n",
    "        - dilation\n",
    "        - bottleneck layer\n",
    "- Network architectures\n",
    "- Gradient descent, back propagation\n",
    "- Hyper parameters and tuning\n",
    "    - learning rate\n",
    "    - batch size\n",
    "    - momentum (addresses convergence speed and local minima)\n",
    "    - learning rate decay, weight decay\n",
    "    - grid search, random search\n",
    "    - successive halving\n",
    "- Optimizers\n",
    "    - Adam\n",
    "    - Adagard\n",
    "- DNN performance tuning\n",
    "    - [tutorial](https://medium.com/@jonathan_hui/improve-deep-learning-models-performance-network-tuning-part-6-29bf90df6d2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn later\n",
    "- log odd ratio\n",
    "- Multiclass SVM\n",
    "- Kernel PCA\n",
    "- Weight normalization\n",
    "- instance normalization, layer normalization, group normalization\n",
    "- Regularization\n",
    "    - Elastic net\n",
    "- classification loss functions\n",
    "    - cross entropy loss function\n",
    "- generative and discriminative model\n",
    "- VAE\n",
    "- GANs\n",
    "- Gaussian Mixture Model (GMM)\n",
    "- Hidden Markov Models (HMM)    \n",
    "- confusion matrix\n",
    "- Precision and recall\n",
    "- ROI pooling\n",
    "- Deep learning studio\n",
    "- Teacher student training (knowledge distilation)\n",
    "- Study later\n",
    "    - Clusttering techniques\n",
    "    - svd\n",
    "    - lda\n",
    "    - binning, xgboost\n",
    "    - deep metric learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "The raw code for this IPython notebook is by default hidden for easier reading.\n",
       "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "The raw code for this IPython notebook is by default hidden for easier reading.\n",
    "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>.''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=brown> Probability and Statistics </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Probability\n",
    "- [Tutorial](https://towardsdatascience.com/probability-concepts-explained-introduction-a7c0316de465)\n",
    "- **Marginal Probability**\n",
    "    - If A is an event, then the marginal probability is the probability of that event occurring, P(A)\n",
    "    - an example of a marginal probability would be the probability that a card drawn from a pack is red: P(red) = 0.5\n",
    "- **Joint Probability**\n",
    "    - The probability of the intersection of two or more events.\n",
    "    - If A and B are two events then the joint probability of the two events is written as P(A ∩ B)\n",
    "    - Example: the probability that a card drawn from a pack is red and has the value 4 is P(red and 4) = 2/52 = 1/26\n",
    "- **Conditional Probability**\n",
    "    - The conditional probability is the probability that some event(s) occur given that we know other events have already occurred\n",
    "    - If A and B are two events then the conditional probability of A occurring given that B has occurred is written as P(A|B)\n",
    "    - Example: the probability that a card is a four given that we have drawn a red card is P(4|red) = 2/26 = 1/13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Central Limit Theorem (CLT)\n",
    "- [CLT Simple Explanation](http://blog.minitab.com/blog/understanding-statistics/how-the-central-limit-theorem-works)\n",
    "- Central limit theorem says that if you have a sufficient number of randomly selected, independent samples (or observations), the <font color=blue> means of those samples will follow a normal distribution </font> -- even if the population you're sampling from does not!\n",
    "- As shown in below figure, the samples of rolling die distribution is almost uniform (as each number of die has same probability 1/6 follows uniform distribution)\n",
    "- ** The means of consecutive samples (5 in the example) follows gaussian distribution, with more number of samples it comes close to gaussian **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x216 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x216 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CLT visualization code\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# seed the random generator\n",
    "np.random.seed(1)\n",
    "# generate roll dies\n",
    "num_rolls = [150, 30000]\n",
    "#plt.figure(figsize=(7,7))\n",
    "for i, num_samples in enumerate(num_rolls):\n",
    "    # generate random samples\n",
    "    rolls = np.random.randint(1, 7, num_samples)\n",
    "    \n",
    "    # find means of consecutive random samples\n",
    "    #print(rolls)\n",
    "    step = 5\n",
    "    means = []\n",
    "    for j in range(0, num_samples, step):\n",
    "        means.append(np.mean(rolls[j:j+step]))\n",
    "        #print(rolls[j:j+step])    \n",
    "    #print(means)\n",
    "    \n",
    "    # plot samples histogram\n",
    "    plt.figure(figsize=(6,3))    \n",
    "    plt.subplot(1,2,1)\n",
    "    plt.hist(rolls)\n",
    "    if i == 0:\n",
    "        plt.title('samples distribution')\n",
    "    # plot samples mean histogram\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.hist(means)         \n",
    "    if i == 0:\n",
    "        plt.title('means of samples distribution')\n",
    "    plt.text(7,0,'number of samples: %d'%num_samples, ha='left', va='bottom')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bernoulli Distribution\n",
    "- The Bernoulli distribution is a model for an experiment that has only two possible outcomes. The corresponding experiment, which has only two possible outcomes is said to be a Bernoulli trial.\n",
    "- ** Bernoulli checks for specific outcome though the experiment has more outcomes.** Suppose a random experiment **has two outcomes**, namely Success and Failure.\n",
    "- example:\n",
    "    - In a single throw of a dice, the outcome \"5\" is called a success and any other outcome (not rolling a 5) is called a failure, then the successive throws of a dice will contain Bernoulli trials.The probability of success = 1/6 and the probability of failure = 5/6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binomial Theorem\n",
    "- [Binomial tutorial](https://www.analyticsvidhya.com/blog/2017/02/basic-probability-data-science-with-examples/)\n",
    "- ** binomial distribution is the discrete probability distribution of the number of success in a sequence of n independent Bernoulli trials (having only yes/no or true/false outcomes). **\n",
    "- so, the Probability for getting k successes in n Bernoulli trails is given by:\n",
    "    - P(X=k) = n<sub>C<sub>k</sub></sub> p<sup>k</sup> q<sup>(n-k)</sup>,  [here p is the probability of success and q is the probability of failure]\n",
    "- If probability of each trail is equal, then the binomial probability distribution is also  equal as shown in graph.2\n",
    "- If we increase the number of trails, there will be many combinations and bars get thinner and thinner as shown in graph.3\n",
    "- What if we play infinite number of trails, ** the bars get infinitely small and the probability distribution looks something like a continuous set of bars which are very close **, almost continuous as shown in graph.4. This now becomes <font color=blue> a probability density function </font>\n",
    "- An observation is, **probability is highest at mean value** and decrease as we move away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x216 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Binomial example visualization: find the probability of winning series in 5 games\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import binom\n",
    "\n",
    "plt.figure(figsize=(13,3))\n",
    "trails = [5, 5, 20, 1000]\n",
    "win_prob = [0.75, 0.5, 0.5, 0.5]\n",
    "for i, prob in enumerate(win_prob):\n",
    "    num_games = trails[i]\n",
    "    binom_prob = binom(num_games, prob)\n",
    "\n",
    "    x = [i for i in range(0, num_games+1, 1)]\n",
    "    # plot    \n",
    "    plt.subplot(1,4,i+1)\n",
    "    plt.bar(x, binom_prob.pmf(x))  \n",
    "    plt.xlabel('number of trails')\n",
    "    if i == 0:\n",
    "        plt.ylabel('probability')\n",
    "        plt.title('UnEqual probability')\n",
    "    elif i == 1:\n",
    "        plt.title('equal probability')\n",
    "    else:\n",
    "        plt.title('equal prob. #more trails')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Normal Distribution or Z-distribution\n",
    "- [Tutorial](https://www.analyticsvidhya.com/blog/2017/02/basic-probability-data-science-with-examples/)\n",
    "- In order to make the proper comparisons among different normal distributions which are centered around their own averages, **the distributions has to standardized to same scale to have a fair/correct comparison**\n",
    "- The standard normal distribution, commonly referred to the Z-distribution, is a special case of a normal distribution with the following properties:\n",
    "    - It has a mean of zero.\n",
    "    - It has a standard deviation of one.\n",
    "- **Z-scores:**\n",
    "    - **The distance in terms of number of standard deviations, the observed value is away from the mean, is the standard score or the Z score.**\n",
    "    - A score on the standard normal distribution is called a Z-Score. It should be interpreted as the number of standard deviations a data point is above or below the mean. A positive Z-Score indicates that a point is above the average, and a negative Z-Score indicates a score below the average.\n",
    "    - Data is rescaled such that mean (μ) = 0 and standard deviation (𝛔) = 1 as,\n",
    "    <h4 align=\"center\"> $ std(x) = \\frac{x - \\mu}{\\sigma} $ </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardization visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimation (MLE)\n",
    "- [Tutorial intro](https://www.analyticsvidhya.com/blog/2018/07/introductory-guide-maximum-likelihood-estimation-case-study-r/)\n",
    "- [Tutorial calculation](https://towardsdatascience.com/probability-concepts-explained-maximum-likelihood-estimation-c7b4342fdbb1)\n",
    "- While studying stats and probability, you must have come across problems like – What is the probability of x > 100, given that x follows a normal distribution with mean 50 and standard deviation (sd) 10. In such problems, we already know the distribution (normal in this case) and its parameters (mean and sd).\n",
    "- but in real life problems these quantities are unknown and must be estimated from the data. <font color=blue> MLE is the technique which helps us in determining the parameters of the distribution that best describe the given data </font>.\n",
    "- ** MLE can be defined as a method for estimating population parameters (such as the mean and variance for Normal, rate (lambda) for Poisson, etc.) from sample data such that the probability (likelihood) of obtaining the observed data is maximized. **\n",
    "- we want to calculate is the total probability of observing all of the data, i.e. the joint probability distribution of all observed data points. To do this we would need to calculate some conditional probabilities, which can get very difficult. So it is here that we’ll make our first assumption. The **assumption is that each data point is generated independently of the others.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Theorem\n",
    "- [Bayes Intro](https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/)\n",
    "- [Bayes Example: Rich people given happy](https://www.quora.com/What-is-an-intuitive-explanation-of-Bayes-Rule)\n",
    "- [Bayesian inference](https://towardsdatascience.com/probability-concepts-explained-bayesian-inference-for-parameter-estimation-90e8930e5348)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=brown> ML/DL basics introduction </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=DarkMagenta> Perceptron </font>\n",
    "- Takes several 'binary' inputs to produce a 'binary' output by comparing the weighted sum of inputs against a threshold, \n",
    "- The threshold is treated as bias and moved to left side of equation to compare weighted sum against zero\n",
    "- If a small change in a weight or bias causes only a small change in output, it is possible for a network to learn. \n",
    "- But, this doesn't happen with perceptrons sometimes as <font color=blue>small change in weights can entirely flip the output</font> from say 1 to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametric Vs Non-Parametric algorithms\n",
    "- Parametric\n",
    "    - Parametric methods makes an assumption about the form of the function relating X and Y\n",
    "    - Linear regression is a parametric method\n",
    "- Non-Parametric\n",
    "    - non-parametric learners do not have a model structure specified a priori. \n",
    "    - We don’t speculate about the form of the function f that we are trying to learn before training the model, as we did previously with linear regression. \n",
    "    - Instead, the model structure is purely determined from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=DarkMagenta> Sigmoid Neuron </font>\n",
    "- Sigmoid neurons are similar to perceptrons (shape is a smoothed out version of a step function), <br> but modified so that **small changes in their weights and bias cause only a small change in their output**\n",
    "- instead of being just 0 or 1, these inputs can also take on any values between 0 and 1\n",
    "- output is not 0 or 1. Instead, it's σ(w⋅x+b), where σ is called the sigmoid function\n",
    "- Somewhat confusingly, and for historical reasons, such multiple layer networks are sometimes called multilayer perceptrons or MLPs, despite being made up of sigmoid neurons, not perceptrons. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient decent\n",
    "- To quantify how well we're achieving this goal we define a cost function\n",
    "- to find a set of weights and biases which make the cost as small as possible. We'll do that using an algorithm known as gradient descent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "- One of the problems that occur during neural network training is called overfitting. \n",
    "- The error on the training set is driven to a very small value, but when new data is presented to the network the error is large. The network has <font color=blue>memorized the training examples, but it has not learned to generalize to new situations</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to avoid overfitting\n",
    "- Go for simpler models over more complicated models. Generally, the **fewer parameters** that you have to tune the better. \n",
    "- Use **more data** to train the model. \n",
    "- Some sort of <font color= blue>regularization</font> can help penalize certain sources of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing Gradients\n",
    "- if a change in the parameter's value causes very small change in the network's output - the network just can't learn the parameter effectively, which is a problem.\n",
    "-  For example, **sigmoid maps the real number line onto a \"small\" range** of [0, 1]. As a result, there are large regions of the input space which are mapped to an extremely small range. In these regions of the input space, even a large change in the input will produce a small change in the output - hence the gradient is small.\n",
    "- This **becomes much worse when we stack multiple layers** of such non-linearities on top of each other. <br> For instance, first layer will map a large input region to a smaller output region, which will be mapped to an even smaller region by the second layer, which will be mapped to an even smaller region by the third layer and so on. ** As a result, even a large change in the parameters of the first layer doesn't change the output much **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to avoid vanishing gradients\n",
    "- We can avoid this problem by using activation functions which don't have this property of 'squashing' the input space into a small region. \n",
    "- A popular <font color=blue>choice is Rectified Linear Unit</font> which maps x to max(0,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "- **Cross validation is a method for estimating the prediction accuracy of a model.**\n",
    "- One way to evaluate a model is to see how well it predicts the data used to fit the model. But this is too optimistic -- a model tailored to a particular data set will make better predictions on that data set than on new data. \n",
    "- Another way is to hold out some data and fit the model using the rest. Then you can test your accuracy on the holdout data.  But the held out data is \"wasted\" from the point of view of building the model. If you have huge amounts of data, so holding some data out won't make the model much worse\n",
    "- Cross validation does something like this but tries to <font color=blue>make more efficient use of the data</font>: you divide the data into (say) 10 equal parts. Then **successively hold out each part and fit the model using the rest**. This gives you 10 estimates of prediction accuracy which can be combined into an overall measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of data\n",
    "- ** Categorical**: Categorical variables take on values that are names or labels. The colour of a ball (e.g., red, green, blue) or the breed of a dog (e.g., collie, shepherd, terrier) would be examples of categorical variables.\n",
    "- ** Quantitative **: Quantitative variables are numerical. They represent a measurable quantity. For example, when we speak of the population of a city, we are talking about the number of people in the city — a measurable attribute of the city. Therefore population would be a quantitative variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification and Regression\n",
    "- **So in very simple terms, classification is about predicting a label and regression is about predicting a quantity**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=brown> Supervised Learning </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=brown> Parametric and Classification Algorithms </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM)\n",
    "- [SVM Overview](https://towardsdatascience.com/support-vector-machines-a-brief-overview-37e018ae310f)\n",
    "- [SVM kernel trick example](https://medium.com/machine-learning-for-humans/supervised-learning-2-5c1c23f3560d)\n",
    "- SVM is also a <font color=blue> binary classifier </font> (classifies 2 classes) like logistic regression\n",
    "- Support vector machines attempt to pass a <font color=blue> linearly separable hyperplane through a dataset in order to classify the data into two groups </font>\n",
    "- This hyperplane is a linear separator for any dimension; it could be a line (2D), plane (3D), and hyperplane (4D+)\n",
    "- the <font color=blue> best hyperplane is the one that maximizes the margin </font>. The margin is the distance between the hyperplane and a few close points. These <font color=blue> close points are the support vectors because they control the hyperplane. </font>\n",
    "- The classes have to be linearly separable to be classified using SVM, a variant of SVM is proposed to classify the data's which are not perfectly separable, it is known as a <font color=blue> Soft Margin Classifier or a Support Vector Classifier </font>, which allows slight mis-classification. SVM classifier contains a tuning parameter in order to control how much misclassification it will allow\n",
    "- **Kernel Trick**:\n",
    "    - The non-linear lower feature space from lower dimension is transformed to higher dimension to classify non-linear, which is known as kernel trick\n",
    "    - these kernels transform our data in order to pass a linear hyperplane and thus classify our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "- The idea of <font color=blue>Logistic Regression is to find a relationship between features and probability of particular outcome.</font>\n",
    "-  Logistic regression works largely the same way linear regression works: it multiplies each input by a coefficient, sums them up, and adds a constant. <font color=blue> In logistic regression, however, the output is actually the log of the odds ratio. </font>\n",
    "- This type of a problem is referred to as **Binomial Logistic Regression**, where the response variable has two values 0 and 1 or pass and fail or true and false. **Multinomial Logistic Regression deals** with situations where the response variable can have three or more possible values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=brown> Parametric and Regression Algorithms </font>\n",
    "- **Regression is a statistical way to establish a relationship between a dependent variable and a set of independent variable(s)**\n",
    "- Regression is concerned with modeling the relationship between variables that is iteratively refined using a measure of error in the predictions made by the model.\n",
    "- Regression methods are a workhorse of statistics and have been co-opted into statistical machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression\n",
    "- While doing linear regression our objective is to **fit a line through the distribution which is nearest to most of the points**. Hence reducing the distance (error term) of data points from the fitted line. \n",
    "- It is conventional to use squares, as Regression line minimizes the sum of “Square of Residuals”. \n",
    "- That’s why the method of Linear Regression is <font color=blue> known as “Ordinary Least Square (OLS)”</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Parametric Algorithms\n",
    "- [Non-Parametric Supervised Learning Tutorial](https://medium.com/machine-learning-for-humans/supervised-learning-3-b1551b9c4930)\n",
    "- non-parametric learners do not have a model structure specified a priori. We don’t speculate about the form of the function f that we’re trying to learn before training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbor (KNN)\n",
    "- You look at the k closest data points and take the average of their values if variables are continuous (like housing prices), or the mode if they’re categorical (like cat vs. dog)\n",
    "- **Choosing k:** tuning hyperparameters with cross-validation\n",
    "    - To decide which value of k to use, you can **test different k-NN models** using different values of k with cross-validation\n",
    "    - Pick whichever yields the lowest error, on average, across all iterations\n",
    "- Higher values of k help address overfitting, but if the value of k is too high your model will be very biased and inflexible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "- Making a good decision tree is like playing a game of “20 questions”\n",
    "- There are ways to quantify information gain so that you can essentially evaluate every possible split of the training data and maximize information gain for every split\n",
    "- **Choosing splits in a decision tree**\n",
    "    - **Entropy is the amount of disorder in a set** (measured by Gini index or cross-entropy)\n",
    "    - If the values are really mixed, there’s lots of entropy; if you can cleanly split values, there’s no entropy.\n",
    "    - **For every split at a parent node, you want the child nodes to be as pure as possible (minimize entropy.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest:  an ensemble of decision trees\n",
    "- A model comprised of many models is called an **ensemble model**, and this is usually a winning strategy.\n",
    "- A single decision tree can make a lot of wrong calls because it has very black-and-white judgments. \n",
    "- A random forest is a meta-estimator that aggregates many decision trees, with some helpful modifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=brown> Unsupervised Learning </font>\n",
    "- [Unsupervised Tutorial](https://medium.com/machine-learning-for-humans/unsupervised-learning-f45587588294)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clusttering\n",
    "- [Top5 Clustterings](https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68)\n",
    "- The goal of clustering is to create groups of data points such that points in different clusters are dissimilar while points within a cluster are similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means clusttering\n",
    "- [K-means tutorial](https://towardsdatascience.com/clustering-using-k-means-algorithm-81da00f156f6)\n",
    "- k-means clustering works on unlabeled data to cluster our data points into k groups. \n",
    "- A larger k creates smaller groups with more granularity, a lower k means larger groups and less granularity.\n",
    "- The output of the algorithm would be a set of “labels” assigning each data point to one of the k groups.\n",
    "- In k-means clustering, the way these **groups are defined is by creating a centroid** for each group. The centroids are like the heart of the cluster, they “capture” the points closest to them and add them to the cluster.\n",
    "- **K-means algorithm steps**\n",
    "    1. **Define the k centroids.**\n",
    "        - Initialize these at random (there are also fancier algorithms for initializing the centroids that end up converging more effectively).\n",
    "    2. **Find the closest centroid & update cluster assignments.**\n",
    "        - Assign each data point to one of the k clusters. Each data point is assigned to the nearest centroid’s cluster. Here, the measure of “nearness” is a hyperparameter — often **Euclidean distance**.\n",
    "        - If we’re using the Euclidean distance between data points and every centroid, a straight line is drawn between two centroids, then a perpendicular bisector (boundary line) divides this line into two clusters\n",
    "    3. **Move the centroids to the center of their clusters.**\n",
    "        - The new position of each centroid is calculated as the average position of all the points in its cluster.\n",
    "    - Keep repeating steps 2 and 3 until the centroid stop moving a lot at each iteration (i.e., until the algorithm converges)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expectation–Maximization (EM) Clustering using Gaussian Mixture Models (GMM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Clusttering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "- Dimensionality reduction looks a lot like compression. This is about trying to reduce the complexity of the data while keeping as much of the relevant structure as possible.\n",
    "- reducing the dimension of the feature space is called “dimensionality reduction.” There are many ways to achieve dimensionality reduction, but most of these techniques fall into one of two classes:\n",
    "    - Feature Elimination\n",
    "    - Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)\n",
    "- [PCA Tutorial with example](https://georgemdallas.wordpress.com/2013/10/30/principal-component-analysis-4-dummies-eigenvectors-eigenvalues-and-dimension-reduction/)\n",
    "- [PCA Tutorial](https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c)\n",
    "- The principal components are the underlying structure in the data. They represent the directions in which the data has maximum variance and also the directions in which the data is most spread out.\n",
    "- Principal component analysis is a technique for feature extraction — so it combines our input variables in a specific way, then we can drop the “least important” variables while still retaining the most valuable parts of all of the variables!\n",
    "- As an added benefit, each of the “new” variables **after PCA are all independent of one another**. This is a benefit because the assumptions of a linear model require our independent variables to be independent of one another\n",
    "- **Eigenvector and Eigenvalue**\n",
    "    - eigenvector was the direction of the line drawn to find the maximum variance\n",
    "    - eigenvalue was a number that tells us how the data set is spread out on the line which is an eigenvector.\n",
    "    - **amount of eigenvectors that exist equals the number of dimensions the data set has** are perpendicular/orthogonal to each other\n",
    "    - <font color=blue> The big eigen vector (with highest variance) is the principal component </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition (SVD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=brown> Loss Functions </font>\n",
    "- All the algorithms in machine learning rely on minimizing or maximizing a function, which we call “**objective function**”. The group of functions that are minimized are called “loss functions”. \n",
    "- A loss function is a measure of how good a prediction model does in terms of being able to predict the expected outcome. \n",
    "- A most commonly used method of finding the minimum point of function is “gradient descent”.\n",
    "- Loss functions can be broadly categorized into 2 types: <font color=blue>Classification loss and Regression Loss</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=brown> Classification Loss Functions </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Square loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hinge loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic loss\n",
    "- Logistic prediction equation: Yp = Sigmoid(Wi * x + b)\n",
    "- Loss (Yp, Yl) = - (Yl logYp + (1 - Yl) log (1- Yp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=brown> Regression Loss Functions </font>\n",
    "- [** 5 regression loss funtioncs **](https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 Loss, Mean Square Error (MSE), Quadratic loss\n",
    "- Mean Square Error (MSE) is the most commonly used regression loss function. MSE is the sum of squared distances between our target variable and predicted values.\n",
    "\n",
    "<h4 align=\"center\"> $ MSE = \\sum\\limits_{i=1}^n  {(y_i - y_i^p)}^2 $ </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 Loss, Mean Absolute Error (MAE)\n",
    "- MAE is the sum of absolute differences between our target and predicted variables. \n",
    "- So it ** measures the average magnitude of errors ** in a set of predictions, without considering their directions. \n",
    "- If we consider directions also, that would be called ** Mean Bias Error (MBE) **\n",
    "\n",
    "<h4 align=\"center\"> $ MAE = \\sum\\limits_{i=1}^n  {|y_i - y_i^p|} $ </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 vs L2 Loss\n",
    "- [L1 vs L2 Comparison](http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/)\n",
    "- <font color=blue>L1 loss is more robust to outliers than L2</font>,\n",
    "    - Since MSE squares the error (y — y_predicted = e), the value of error (e) increases a lot if e > 1. \n",
    "    - If we have an outlier in our data, the value of e will be high and e² will be >> |e|. \n",
    "    - This will make the model with **MSE loss give more weight to outliers ** than a model with MAE loss.\n",
    "    - MAE loss is useful if the training data is corrupted with outliers\n",
    "- <font color=blue>L1 loss derivative is not continuous, hence inefficient to find solution, i.e. unstable</font>,\n",
    "    - which can lead to missing minima\n",
    "    - As L2 derivative is continuous, it gives more stable solution, however it not robust in case of outliers\n",
    "- <font color=blue>Issue with L1 and L2 loss functions:</font>\n",
    "    - There can be cases where neither loss function gives desirable predictions. \n",
    "    - **For example,** if 90% of observations in our data have true target value of 150 and the remaining 10% have target value between 0–30. \n",
    "    - Then a model with MAE as loss might predict 150 for all observations, ignoring 10% of outlier cases, as it will try to go towards median value. \n",
    "    - In the same case, a model using MSE would give many predictions in the range of 0 to 30 as it will get skewed towards outliers. Both results are undesirable in many business cases.\n",
    "    - **An easy fix would be to transform the target variables. Another way is to try a different loss function. This is the motivation behind our 3rd loss function, Huber loss.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huber Loss, Smooth Mean Absolute Error\n",
    "- Huber loss is less sensitive to outliers in data than the squared error loss. It’s also **differentiable at 0**. \n",
    "- It’s basically absolute error, which becomes quadratic when error is small.\n",
    "- problem with Huber loss is that we might need to train hyper parameter delta which is an iterative process\n",
    "- ** it’s twice differentiable everywhere **\n",
    "- Many ML model implementations like XGBoost use Newton’s method to find the optimum, which is why the second derivative (Hessian) is needed. For ML frameworks like XGBoost, **twice differentiable functions are more favorable**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log-CosH Loss\n",
    "- Log-cosh is another function used in regression tasks that’s **smoother than L2**. \n",
    "- Log-cosh is the logarithm of the hyperbolic cosine of the prediction error.\n",
    "- 'logcosh' works mostly like the mean squared error, but will ** not be so strongly affected by the occasional wildly incorrect prediction **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantile Loss\n",
    "- Quantile loss functions turns out to be useful when we are interested in predicting an interval instead of only point predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triplet loss (ex: face recognition)\n",
    "- A loss is computed between anchor and a postive (similar) image, anchor and a negative image, the loss between anchor and positive has to much smaller than loss between anchor and negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=brown> Regularization Algorithms </font>\n",
    "- To avoid over optimizing/over-fitting the training set to use early termination as soon as the learning stops, other method is ** to use regularization **\n",
    "- An extension made to another method (typically regression methods) that <font color=blue>penalizes models based on their complexity</font>, favoring simpler models that are also better at generalizing.\n",
    "- other regularization technique is **dropout**\n",
    "- [Regularization techniques](https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-regularization-techniques/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=DarkMagenta> L1 (Lasso) and L2 (Ridge) as Reguralization </font>\n",
    "- A regularization term is added to the loss function to avoid overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 or Frobenius Regularization or Weight decay\n",
    "- The lambda term added is similar to L2 norm, but in context of neural networks its called as frobenius regularization\n",
    "- Regularization term: Loss: <h3 align=\"center\"> $ Loss (E) = \\frac{1}{m}\\sum\\limits_{i=1}^n  {(y_i - y_i^p)}^2 + \\frac{\\lambda}{2m} \\sum\\limits_{l=1}^L {||W_l||^2} $ </h3>\n",
    "- Derivative with regularization term (for backprop): <h3 align=\"center\"> $ dW = dNormal + \\frac {\\lambda}{m} W_l $ </h3>\n",
    "- Gradient decent weight updation: <h3 align=\"center\"> $ W_l = W_l - \\sigma dW $ </h3>\n",
    "- Rewriting above weight update equation: <h3 align=\"center\"> $ (1 - \\frac{\\sigma \\lambda} {m}) W_l - \\sigma dNormal $ </h3>\n",
    "- The 2nd term in above equation reduces the weights based on learning rate and lambda, that is why this regularization is ** <font color=blue> also called as weight decay </font>**\n",
    "- L2-regularization relies on the assumption that a model with small weights is simpler than a model with large weights. Thus, by **penalizing the square values of the weights in the cost function you drive all the weights to smaller values**. It becomes too costly for the cost to have large weights! This leads to a smoother model in which the output changes more slowly as the input changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop-out\n",
    "- remove few connections randomly to force the network to learn redundant representation of input, so that it doesn't overfit and depend on any particular parameter, so that all learns independently\n",
    "- The dropout rate is the fraction of the features that are zeroed out; it’s usually set between 0.2 and 0.5. \n",
    "- At test time, no units are dropped out; instead, the layer’s output values are scaled down by a factor equal to the dropout rate\n",
    "- During training time, divide each dropout layer by keep_prob to keep the same expected value for the activations. For example, if keep_prob is 0.5, then we will on average shut down half the nodes, so the output will be scaled by 0.5 since only the remaining half are contributing to the solution. Dividing by 0.5 is equivalent to multiplying by 2. Hence, the output now has the same expected value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping\n",
    "- Analyze the training and validation error plots, generally both errors will decrease with more training\n",
    "- However, after a point the training error continue to decrease but the validation error may increase, so this indicates the network starts overfit to training set and not generalizing for other data sets\n",
    "- Hence we can stop the training at that point to avoid overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Early Stopping example visualization: find validation error increase point\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8,3))\n",
    "plt.xlabel('#training iterations')\n",
    "plt.ylabel('Error')\n",
    "x_index = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
    "train_error = [100, 95, 88, 80, 60, 45, 20, 18, 15, 10, 9, 5, 3, 2, 2, 1, 0.4, 0.3]\n",
    "plt.plot(x_index, train_error, label='training_error')\n",
    "validation_error = [110, 105, 100, 95, 80, 55, 40, 38, 45, 55, 60, 60, 65, 68, 70, 70, 72, 74]\n",
    "plt.plot(x_index, validation_error, label='validation_error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation to avoid overfitting\n",
    "- More data can reduce the overfitting, but more data may not be available always so augment data by cropping, flipping, zooming, etc to create more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=brown> CNN Layer Theory </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rectified Linear Unit (ReLU)\n",
    "- [Guide to ReLU](https://medium.com/tinymind/a-practical-guide-to-relu-b83ca804f1f7)\n",
    "- ReLU\n",
    "    - defined as y = max(0, x), doesn’t “saturate,” when x gets large. Hence, doesn’t have the vanishing gradient problem suffered by other activation functions like sigmoid or tanh\n",
    "- **Dying ReLU**\n",
    "    - A ReLU neuron is “dead” if it’s stuck in the negative side and always outputs 0. \n",
    "    - Because the slope of ReLU in the negative range is also 0, once a neuron gets negative, it’s unlikely for it to recover. \n",
    "    - Such neurons are not playing any role in discriminating the input and is essentially useless.\n",
    "- Leaky ReLU\n",
    "    - Leaky ReLU has a small slope for negative values, instead of altogether zero. \n",
    "    - For example, leaky ReLU may have ** y = 0.01x ** when x < 0\n",
    "- **Parametric ReLU (PReLU)**\n",
    "    - Parametric ReLU (PReLU) is a type of leaky ReLU that, instead of having a predetermined slope like 0.01, makes it a parameter for the neural network to figure out itself: y = ax when x < 0.\n",
    "- Exponential Linear (ELU, SELU)\n",
    "    - Similar to leaky ReLU, ELU has a small slope for negative values. Instead of a straight line, it ** uses a log curve ** y = a(ex-1)\n",
    "    - sometimes called Scaled ELU (SELU) due to the constant factor a\n",
    "    - leaky ReLU while it doesn’t have the dying ReLU problem, it saturates for large negative values, allowing them to be essentially inactive\n",
    "    - It is designed to combine the good parts of ReLU and leaky ReLU\n",
    "- Concatenated ReLU (CReLU)\n",
    "    - Concatenated ReLU has two outputs, one ReLU and one negative ReLU, concatenated together. \n",
    "    - In other words, for positive x it produces [x, 0], and for negative x it produces [0, x]. \n",
    "    - Because it has two outputs, CReLU doubles the output dimension.\n",
    "- ReLU-6\n",
    "    - ReLU capped at 6, 6 is an arbitrary choice that worked well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax regression for multi-class classification\n",
    "- Softmax regression/activation is a generalization of logistic regression (binary classifier) and its used for multi-class classification used as the last layer for predictions\n",
    "- **Softmax converts the final outputs to probabilities of output classes**\n",
    "- Softmax gives probabilities for all output classes, whereas hardmax gives value 1 to highest probability class and makes remaining 0\n",
    "- Softmax is computed (as element-wise operation) and normalized on the output vector Z as below,\n",
    "<h3 align=\"center\"> $ Softmax_{prob} = \\frac {e^Z}{\\sum{e^Z}} $ </h3>\n",
    "- **Softmax example with 4 output classes,**\n",
    "    - final output Z = {5, 2, -1, 3}\n",
    "    - Softmax element-wise values (numerator) $e^Z$ = {148.4, 7.4, 0.4, 20.1}\n",
    "    - Softmax normalization factor (denominator) $\\sum{e^Z}$ = 176.3\n",
    "    - Final Softmax output probabilities = {0.842, 0.042, 0.002, 0.114}\n",
    "    - Final Hardmax output probabilities = {1, 0, 0, 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing or Feature Scaling\n",
    "- Standardization\n",
    "- Mean Normalization\n",
    "- Min-Max Scaling\n",
    "- Unit Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard deviation\n",
    "- [Standard deviation tutorial](https://www.mathsisfun.com/data/standard-deviation.html#WhySquare)\n",
    "- The Standard Deviation is a measure of how spread out numbers are (compared to mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization and Standardization\n",
    "- [Standardization Vs Normalization](https://medium.com/@rrfd/standardize-or-normalize-examples-in-python-e3f174b65dfc)\n",
    "- **Normalization (Min-Max Scaling):**\n",
    "    - If the training data contain some features with higher magnitude (outliers), then the results might be dominated by them instead of all the features, hence normalize the data with largest value\n",
    "    - Normalization makes training less sensitive to the scale of features, so we can better solve for coefficients\n",
    "    - Normalizing will ensure that a convergence problem does not have a massive variance, making optimization feasible\n",
    "    - Normalization modifies the data range to [0, 1] using below formula,\n",
    "    <h4 align=\"center\"> $ norm(x) = \\frac{x - min(x)}{max(x) - min(x)} $ </h4>\n",
    "- **Standardization (Z-Score Normalization)**\n",
    "    - Data is rescaled such that mean (μ) = 0 and standard deviation (𝛔) = 1 as,\n",
    "    <h4 align=\"center\"> $ std(x) = \\frac{x - \\mu}{\\sigma} $ </h4>\n",
    "    - **standardized (z-score) or studentized (t-scores) scores to increase comparability of different features in the training data**\n",
    "    - **to arrive at (transformed) data that follows a normal distribution**\n",
    "    - But if original data does not follow normal distribution, then transformed data also may not follow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance and Correlation\n",
    "- [Covariance and Correlation Tutorial](https://medium.com/@adamzerner/covariance-and-correlation-d4c64769d4f1)\n",
    "- Covariance\n",
    "    - **Covariance is a measure of how much two random variables vary together**. It’s similar to variance, but where variance tells you how a single variable varies, covariance tells you how two variables vary together\n",
    "    - positive covariance implies a direct relationship between the variables (increasing x increases y)\n",
    "    - negative covariance implies an indirect relationship between the variables (increasing x decreases y)\n",
    "    - ** If the covariance is large, so there is a strong relationship between the numbers** or if the covariance is small, so there is a weak relationship between the numbers\n",
    "- Correlation\n",
    "    - **The covariance can tell the direction/variability between two data, can not tell the how much is the relation**, since the data expressed in different units (ex: mm, cm, meters) yields varying covariance (large/small)\n",
    "    - <font color=blue>Hence the data's has to be standardized to find the strength of covariance, the standardized covariance known as Correlation which tells the strength</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whitening"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "- [Batch Normalization Tutorial](https://www.learnopencv.com/batch-normalization-in-deep-networks/)\n",
    "- [Batch Normalization Explained](https://blog.paperspace.com/busting-the-myths-about-batch-normalization/)\n",
    "- The intermediate data in a DNN can shift due to change in distribution, so BN normalizes the intermediate data to have zero mean, unit variance similar to general input normalization.\n",
    "- BN is generally applied on layer output (Z) before applying activation, ** the mean and variance are computed from each mini-batch**\n",
    "<h4 align=\"center\"> $ Z_{norm} = \\frac{Z - \\mu}{\\sqrt{\\sigma^2+\\epsilon}} $ </h4>\n",
    "- Final BN output is scaled by $\\gamma, \\beta$\n",
    "<h4 align=\"center\"> $ Z_{N} = \\gamma * Z_{norm} + \\beta $ </h4>\n",
    "- <font color=blue> $\\gamma, \\beta$ are also updated using gradient descent $\\gamma = \\gamma - \\alpha * d\\gamma$ </font>\n",
    "- <font color=blue> To compute BN in test time, the exponential average of $\\gamma, \\beta$ computed while training on mini-batches are used </font>\n",
    "- **Covariate Shift**\n",
    "    - covariate shift is defined as a change in the distribution of data\n",
    "    - If a DNN is trained on a data set containing a class (say red roses) and similar class with different attributes (say white roses)\n",
    "    - These two classes will be present in different regions in the feature space, this difference in distribution is called the covariate shift\n",
    "    - So while training, the mini batch should have the same distribution (of both red and white roses) sampled from the entire training data set\n",
    "    - When the mini-batches have images uniformly sampled from the entire distribution, there is negligible covariate shift\n",
    "    - However, when the mini-batches are sampled from only one of the two subsets, there is a ** significant covariate shift **. This makes the training of the rose vs non-rose classifier very slow\n",
    "    - An easy way to solve this problem to normalize the inputs (mini batch) to the neural network so that the input distribution have a zero mean and a unit variance\n",
    "- **Internal Covariate Shift**\n",
    "    - However, when the networks get deeper, say, 20 or more layers, the minor fluctuations in weights over more than 20 odd layers can produce big changes in the distribution of the input being fed to deeper layers even if the input is normalized.\n",
    "    - Just as it made intuitive sense to have a uniform distribution for the input layer, it is advantageous to have the same input distribution for each hidden unit over time while training. \n",
    "    - But in a neural network, each ** hidden unit’s input distribution changes every time there is a parameter update in the previous layer **. This is called internal covariate shift. \n",
    "    - This makes training slow and requires a very small learning rate and a good parameter initialization.\n",
    "- This problem is solved by normalizing the layer’s inputs over a mini-batch and this process is therefore called Batch Normalization.\n",
    "- However, as against above discussion, batch norm actually ends up increasing internal covariate shift as compared to a network that doesn't use batch norm. They key insight from the paper is that batch norm actually makes the loss surface smoother, which is why it works so well (as pointed by a recent research paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back Propagation\n",
    "- [Back propagation step by step example](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "- [Gradient Descent Tutorial](https://blog.paperspace.com/intro-to-optimization-in-deep-learning-gradient-descent/)\n",
    "- how do we go about **escaping local minima and saddle points**, while trying to converge to a global maximum. The answer is **randomness**\n",
    "- Gradient descent is a First Order Optimization Method. It only takes the first order derivatives of the loss function into account and not the higher ones. What this basically means it has no clue about the curvature of the loss function. It can tell whether the loss is declining and how fast, but cannot differentiate between whether the curve is a plane, curving upwards or curving downwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=brown> Bias Variance Trade-Off </font>\n",
    "- [Bias Variance Trade-off Tutorial](https://www.listendata.com/2017/02/bias-variance-tradeoff.html)\n",
    "- [Bias Variance Trade-off Infograph](https://elitedatascience.com/bias-variance-tradeoff)\n",
    "- ** Bias **\n",
    "    - Bias is a measure of the prediction accuracy on training data\n",
    "    - <font color=blue> High bias means low prediction accuracy </font>, which means model may be too simple not able learn from training data known as underfitting\n",
    "    - For example, a linear regression model would have high bias when trying to model a non-linear relationship\n",
    "    - High Bias Techniques\n",
    "        - Linear Regression, Linear Discriminant Analysis and Logistic Regression\n",
    "    - Low Bias Techniques\n",
    "        - Decision Trees,  K-nearest neighbours and Gradient Boosting\n",
    "    - <font color=blue> Parametric algorithms which assume something about the distribution of the data points </font> suffer from High Bias. Whereas non-parametric algorithms which does not assume anything special about distribution have low bias.\n",
    "- ** Variance **\n",
    "    - Variance is a measure of the generalization of the network\n",
    "    - <font color=blue> High variance means less generalization </font>, complex models that fits well on training data but they cannot generalise the pattern well which results to overfitting\n",
    "    - Low Variance Techniques\n",
    "        - Linear Regression, Linear Discriminant Analysis and Logistic Regression\n",
    "    - High Variance Techniques\n",
    "        - Decision Trees,  K-nearest neighbours and SVM\n",
    "- ** Bias Variance Trade-off **\n",
    "    - It means there is a trade-off between predictive accuracy and generalization of pattern outside training data. Increasing the accuracy of the model will lead to less generalization of pattern outside training data. Increasing the bias will decrease the variance. Increasing the variance will decrease the bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Initialization\n",
    "- Weights initialized with zeros makes network not learn anything because of symmetry problem. as all nodes will have same weights, so they dont learn different functions/relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### He initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Clipping\n",
    "- Due to exploding gradients the weight parameters values exponentially become large values like Nan (Not a number)\n",
    "- To solve this problem simple solution is to clip the gradients above a certain threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-Batch Gradient Descent\n",
    "- Training set is divided in to smaller set of samples known as mini-batch\n",
    "- <font color=blue> Before splitting the training in to mini-batches, the data is randomly shuffled to ensure data is shuffled in to different mini-batches </font>\n",
    "- After a mini-batch is trained the backward propagation is applied to update weights\n",
    "- Hence, weights are updated after training on a min-batch unlike batch gradient descent where back prop & weight updation happens after processing all training samples\n",
    "- So to process the whole training set, it needs to run for train_samples/min_batch_size times **known as iterations**\n",
    "- If min-batch size = total train samples, it is batch gradient\n",
    "- If min-batch size = total train samples, it is **known as stochastic gradient descent (SGD)**\n",
    "- We may observer, the training error reduces smoothly in batch GD, where as <font color=blue>**in mini-batch ripples are observed in training error due to error computed on different mini-batches**</font>\n",
    "- SGD error will be more noisier as it operates on each sample, but its much faster\n",
    "- Batch gradient is smooth, but very slow due to large amount of data may not fit in RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x216 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Typical training error plots of batch and min-batch gradient descent\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12,3))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.xlabel('#training iterations')\n",
    "plt.ylabel('Training cost')\n",
    "x_index = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
    "train_error = [200, 180, 150, 130, 120, 100, 90, 80, 70, 60, 50, 45, 40, 35, 30, 25, 20, 15]\n",
    "plt.plot(x_index, train_error, label='Batch Gradient Descent')\n",
    "plt.legend()\n",
    "\n",
    "#sub plot\n",
    "plt.subplot(1,2,2)\n",
    "plt.xlabel('#min-batch training iterations')\n",
    "plt.ylabel('Training cost')\n",
    "x_index = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
    "validation_error = [200, 190, 175, 185, 180, 155, 165, 130, 145, 120, 140, 100, 125, 90, 95, 80, 90, 60, 78, 50, 55, 45, 40, 50, 35]\n",
    "plt.plot(x_index, validation_error, label='Mini-batch Gradient Descent')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponentially Weighted Moving Average\n",
    "- It is an optimization algorithm for local average or smoothing\n",
    "- Equation: Current_val(t) = weight * Previous_val(t-1) + (1 - weight) * current_sample\n",
    "- Generally defined as, <h3 align=\"center\"> $ Val_t = \\beta * Val_{t-1} + (1-\\beta) * currentsample $ </h3>\n",
    "- **$\\beta$ is weight factor indicates how much weightage to be given to previous values**\n",
    "- If $\\beta$ is high, then lot of weitage to previous values, then moving average adapts slowly to local changes\n",
    "- If $\\beta$ is low, then lot of weitage to current sample, then moving average adapts faster to local changes\n",
    "- $Val_t$ is $\\approx \\frac{1}{1-\\beta}$\n",
    "    - Example: if $\\beta = 0.9$ means its an average over 10 previous values\n",
    "- $\\beta$ is a hyper parameter to be tuned and typical value for $\\beta$ is 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias correction in exponential weighted average\n",
    "- Generally the initial value $V_0$ is assigned with zero, so the initial values are close to zero instead of representing the actual averages\n",
    "- To solve this we can calculate value as, $\\frac{Val_t}{1-\\beta^t}$, for initial values it works as average, when t becomes large $\\frac{1}{1-\\beta^t}$ becomes one so no difference to the value\n",
    "- **This way correcting the average values of initial samples is known as bias correction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent with momentum\n",
    "- The gradient descent error oscillates or contains ripples due to change in error surface due to current mini-batch\n",
    "- This oscillations are even bigger if we use a bigger learning rate\n",
    "- <font color=blue> **Hence for smooth convergence use the exponential moving average concept to update gradients to have fast convergence** </font>\n",
    "- Weight gradient update, <h3 align=\"center\"> $ V_{dw} = \\beta * V_{dw} + (1-\\beta) * dw $ </h3>\n",
    "<h3 align=\"center\"> $ W = W - \\alpha * V_{dw} $ </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSprop (Root Mean Square prop)\n",
    "- Similar to gradient descent with momentum, the vertical direction updates has to be minimized and horizontal direction movement has to higher for faster convergence in 2D bowl shape function\n",
    "- RMSprop also does better convergence\n",
    "- Weight gradient update, <h3 align=\"center\"> $ S_{dw} = \\beta * S_{dw} + (1-\\beta) * dw * dw $ </h3>\n",
    "<h3 align=\"center\"> $ W = W - \\alpha * \\frac {dw}{\\sqrt{1- S_{dw}}} $ </h3>\n",
    "- The intuition of RMSprop is, divide the vertical gradients with bigger number and horizontal gradients with smaller numbers for smooth and fast convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam Optimizer\n",
    "- <font color=blue> **Adam optimizer is the best optimizer, which is a combination of momentum and RMSprop** </font>\n",
    "<h3 align=\"center\"> $ V_{dw} = \\beta_1 * V_{dw} + (1-\\beta_1) * dw $ </h3>\n",
    "<h3 align=\"center\"> $ S_{dw} = \\beta_2 * S_{dw} + (1-\\beta_2) * dw * dw $ </h3>\n",
    "- Adam weight update, <h3 align=\"center\"> $ W = W - \\alpha * \\frac {V_{dw}}{\\sqrt{1- S_{dw}}} $ </h3>\n",
    "- $\\beta_1, \\beta_2$ are the 2 hyper parameters and typical values suggested by adam paper are 0.9 and 0.9999 respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate decay\n",
    "- The learning rate can higher while initial training steps to take big strides, after some epochs gradually reduce the learning rate to converge to the optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #layers and #hidden units hyper parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Search for hyper parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coarse to fine sampling of hyper param search space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-dev set for different distribution of train, dev sets\n",
    "- The distribution of train, dev/test sets can be different, still the DNNs work well\n",
    "- But to compute bias, variance, these sets will not indicate proper analysis due to different distributions\n",
    "- Create a separate dev set from training data known as train-dev set, which is used for error analysis of bias, variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN Performance Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow API Hierarchy\n",
    "- TF has number of abstraction layers\n",
    "- Lowest level is HW specific, above it high level C++ API's\n",
    "- Python API levels are available provides core numeric processing (ex: add, mul, etc)\n",
    "- Next is high level python modules (layers, losses)\n",
    "- Highest level is estimator API does everything in terms of training, evaluate, create & save checkpoints etc\n",
    "\n",
    "<img src=\"images/TensorFlow_Hierarcy.PNG\" alt=\"TensorFlow Hierarchy\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution modes (Lazy, Eager)\n",
    "- Lazy evaluation mode:\n",
    "    - In Lazy evaluation mode, first the network graph (DAG) is build and the executed under session.run \n",
    "    - **Lazy evaluation is default mode which helps in distributed computing, performance/memory optimization**\n",
    "    - Used in production codes\n",
    "- Eager mode: \n",
    "    - In this the values are immediately computed, session.run is not required unlike lazy mode\n",
    "    - **Eager mode can be executed by importing eager module and enabling \"enable_eager_execution\" at the start of the code only once**\n",
    "    - <font color=blue> Eager mode may be useful to debug the code while developing </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session.run example\n",
    "- Session.run() needs to be executed to compute the operation and get results, it can be executed on any value\n",
    "- **sess.run() can take multiple inputs and produces corresponding outputs**\n",
    "- <font color=blue>.eval is similar to sess.run(), which needs to be used with the value/operation </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output without sess.run()\n",
      " Tensor(\"Add_3:0\", shape=(3,), dtype=int32)\n",
      "sess.run(z1) result\n",
      " [ 4  7 10]\n",
      "z1.eval() result\n",
      " [ 4  7 10]\n",
      "o1 result\n",
      " [ 4  7 10]\n",
      "o2 result\n",
      " [ 3 10 21]\n",
      "o3 results\n",
      " [2 3 4]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# create constants\n",
    "x = tf.constant([3, 5, 7])\n",
    "y = tf.constant([1, 2, 3])\n",
    "\n",
    "# create operations\n",
    "z1 = tf.add(x, y)\n",
    "z2 = x * y           #can create using normal operations\n",
    "z3 = x - y\n",
    "\n",
    "# printing the values without sess.run outputs only shape\n",
    "print('output without sess.run()\\n', z1)\n",
    "\n",
    "# run the graphs\n",
    "with tf.Session() as sess:\n",
    "    print(\"sess.run(z1) result\\n\", sess.run(z1))\n",
    "    # can also rung using .eval with operation\n",
    "    print(\"z1.eval() result\\n\", z1.eval())\n",
    "    # can pass multiple arguments to sess.run and gets corresponding outputs\n",
    "    o1, o2, o3 = sess.run([z1, z2, z3])\n",
    "    print('o1 result\\n', o1)\n",
    "    print('o2 result\\n', o2)\n",
    "    print('o3 results\\n', o3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables\n",
    "- Use tf.get_variable() instead of using tf.variable(), as tf.get_variable() can be reused, it gets an existing variable with these parameters or create a new one.\n",
    "- ex: a = tf.get_variable(name=\"var_1\", **initializer**=tf.constant(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.trainable_variables()\n",
    "- Returns all variables created with trainable=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.get_default_graph()\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.placeholder\n",
    "- Use a placeholder to feed in the data for dependent values\n",
    "- Use None to shape values to allow to use variable size (ex: batchsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global initialization\n",
    "- add an Op to initialize global variables\n",
    "- init_op = **tf.global_variables_initializer()**\n",
    "\n",
    "- with tf.Session() as sess:\n",
    "    - sess.run(init_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimatore API\n",
    "- Estimators, which represent a complete model. The Estimator API provides methods to train the model, to judge the model’s accuracy, and to generate predictions.\n",
    "- **tf.estimator.Estimator(model_fn=model_fn, config=config, params=params,)**\n",
    "- **model_fn is the function initializing the model.** This function is represented and fully defined by an EstimatorSpec object which knows how to generate all outputs, how to train, and how to evaluate the model.\n",
    "- config is a RunConfig object specifying how to run the estimator.\n",
    "- params is an object holding the hyper-parameters of the model.\n",
    "- After creating the Estimator we can train it using the train_and_evaluate function\n",
    "    - **tf.estimator.train_and_evaluate(model_estimator, train_spec, eval_spec)**\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging TensorFlow Programs\n",
    "- The general problems occur with shape of tensors, data type\n",
    "- There can be errors related to specific input values or conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing shape issues\n",
    "- tf.expand, tf.squeeze, tf.reshape to change dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing data type problems \n",
    "- tf.cast()\n",
    "    - To typecast tensorflow variable (ex: a = tf.cast(a, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging the execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change logging level\n",
    "- Default TF log messages level is Warn, we can set other levels to more or less debug info using below api\n",
    "    - tf.logging.set_verbosity(tf.logging.INFO)\n",
    "- Decreasing order of message levels: DEBUG, INFO, WARN, ERROR, FATAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.Print()\n",
    "- Prints values in to a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.debug()\n",
    "- An interactive debugger, can be called with --debug option\n",
    "- We can step through the code and use break points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Algorithms\n",
    "- [Ensemble overview](https://medium.com/coinmonks/an-intro-to-ensemble-learning-in-machine-learning-5ed8792af72d)\n",
    "- [Ensemble Tutorial](https://towardsdatascience.com/ensemble-learning-in-machine-learning-getting-started-4ed85eb38e00)\n",
    "- The goal of ensemble algorithms is to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability / robustness over a single estimator\n",
    "- There are two families of ensemble methods which are usually distinguished\n",
    "    - **Averaging methods**\n",
    "        - The driving principle is to build several estimators independently and then to average their predictions. On average, the combined estimator is usually better than any of the single base estimator because its variance is reduced\n",
    "        - <font color=blue> examples: Bagging methods, Forests of randomized trees </font>\n",
    "    - **Boosting methods**\n",
    "        - Base estimators are built sequentially and one tries to reduce the bias of the combined estimator. The motivation is to combine several weak models to produce a powerful ensemble\n",
    "        - <font color=blue> examples: AdaBoost, Gradient Tree Boosting </font>\n",
    "- **The three most popular methods for combining the predictions from different models are**\n",
    "    - **Bagging or Bootstrap Aggregation**\n",
    "        - Building multiple models (typically of the same type) from different subsamples of the training dataset.\n",
    "    - **Boosting**\n",
    "        - Building multiple models (typically of the same type) each of which learns to fix the prediction errors of a prior model in the sequence of models.\n",
    "    - **Voting**\n",
    "        - Building multiple models (typically of differing types) and simple statistics (like calculating the mean) are used to combine predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting\n",
    "- You can train your model using diverse algorithms and then ensemble them to predict the final output. Say, you use a Random Forest Classifier, SVM Classifier, Linear Regression etc\n",
    "- **Hard voting** is where a model is selected from an ensemble to make the final prediction by a simple **majority** vote for accuracy\n",
    "- **Soft voting** arrives at the best result by **averaging** out the probabilities calculated by individual algorithms. Soft Voting can only be done when all your classifiers can calculate probabilities for the outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging or Bootstrap Aggregation\n",
    "- Instead of running various models on a single dataset, you can **use a single model over various random subsets of the dataset**\n",
    "- Random sampling with replacement is called Bagging, short for bootstrap aggregating. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "- An ensemble of Decision trees is a Random Forest. Random Forests performs Bagging internally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost\n",
    "- In simple terms, Run a Classifier and make predictions. Run another classifier to fit the previously misclassified instances and make predictions. Repeat until all/most of the training instances are fitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting\n",
    "- Similar to AdaBoost, Gradient Boosting also works with successive predictive models added to the ensemble.\n",
    "- Instead of updating the weights of the training instances like AdaBoost, Gradient Boosting fits the new model to the residual errors.\n",
    "- Put simply, Fit a model to the given Training set. Calculate the Residual Errors which become the new training instances. A new model is trained on these and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "- XGBoost is a recent, most preferred and powerful gradient boosting method. \n",
    "- Instead of making hard Yes and No Decision at the Leaf Nodes, XGBoost assigns positive and negative values to every decision made.\n",
    "- XGBoost can work with Trees as well as Linear Models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why RNNs?\n",
    "- For sequence data normal CNNs does not work because of mainly 2 reasons,\n",
    "    - The inputs does not share common features like images which is required by CNNs\n",
    "    - The input size and output size can be different (machine translation), which can not be handled by CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple RNN\n",
    "- The input 'x' is processed by a weight matrix Wx, and previous activation output by Wa and output prediction is computed by Wy matrix\n",
    "- The same weights Wa, Wx, Wy are shared among different times temporally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple RNN equations\n",
    "- The simple RNN diagram and equations are shown below (source: coursera),\n",
    "\n",
    "<img src=\"images/rnn_forward_prop_equations.PNG\" width=\"500\"/>\n",
    "\n",
    "- It can be represented as a single unit as shown below,\n",
    "<img src=\"images/rnn_diagram.PNG\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplified RNN equations notation\n",
    "- The generic version of equations are created by staking the Wa, Wx weight matrices and also correspondingly staking input 'x(t)', previous output 'a(t-1)'\n",
    "<img src=\"images/rnn_simplified_notation.PNG\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple RNN representation\n",
    "- The above equation can be represented as shown below\n",
    "\n",
    "<img src=\"images/rnn_unit.PNG\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPTT (Back Prop Through Time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Modelling\n",
    "- LM finds the probability of a sentence occurrence with a set of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "- Takes individual words in a given sentence and maps to an entry the dictionary\n",
    "- Add UKN for unknown words which are not there in your dictionary\n",
    "- Can add EOS token at the end of sentence to indicate end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word vs Character RNN\n",
    "- Character level models require lot of characters, hence states and longer dependencies and also computationally expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU (Gated Recurrent Unit)\n",
    "- To resolve the long term dependencies of input sequences, LSTM and GRU are developed\n",
    "- The GRU uses a memory cell 'c', <font color=blue> the memory cell value is updated by a gate to decide what information to retain using an update gate </font>\n",
    "- The memory cell value is dependent on current candidate cell value and previous cell output\n",
    "- The memory cell can be a vector, each element may be representing a specific characteristic of input sequence (like noun, singular/plural, subject, etc)\n",
    "\n",
    "<img src=\"images/rnn_gru.PNG\" width=\"500\"/>\n",
    "\n",
    "- The full standard GRU uses one extra operation/gate compared to above equation to find the relevance of the candidate cell output\n",
    "\n",
    "<img src=\"images/rnn_gru_standard.PNG\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM (Long Short Term Memory)\n",
    "- LSTM is a generic and complex version than GRU\n",
    "- <font color=blue> LSTM uses 3 gates rather than 2 gates as in GRU </font>\n",
    "- The main difference of LSTM compared to GRU is a(t) is not same as c(t)\n",
    "- <font color=blue> The cell state 'c' is computed with 2 gates update, forget </font>\n",
    "- Output at any time stamp is computed with 3rd gate output gate\n",
    "\n",
    "<img src=\"images/rnn_lstm.PNG\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peephole connection\n",
    "- Its a variation of LSTM, where gate values not only depend on a(t-1), x(t) but also depends on c(t-1) previous cell state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BRNN (Bi-directional RNN)\n",
    "- Bi-directional processes the input sequence from both directions to make predictions\n",
    "- The downside of BRNN is it requires entire input sequence, it is suitable for NLP tasks where entire input sequence is available, but for real-time ASR tasks entire sequence is not available where a complex BRNN can be used\n",
    "\n",
    "<img src=\"images/rnn_BRNN.PNG\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep RNN\n",
    "- To learn more complex functions the RNNs can be stacked on top of another as shown below\n",
    "\n",
    "<img src=\"images\\rnn_deep.PNG\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=green> New DNN Research Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transparency by Design (TbD Net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capsule Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero shot learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=orange> Interview Questions </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skills required for ML/DL Engineer Jobs\n",
    "- CNN architectures & algorithms, RNNs, NLP, Reinforcement learning\n",
    "- ML basic algorithms\n",
    "- Linear algebra, Probability, Statistics\n",
    "- DL frameworks (eg: Tensorflow, Keras, PyTorch)\n",
    "- Pyhon, R, jupyter notebooks\n",
    "- ARM, DSP, GPU working experience\n",
    "- Experience in building ML Applications like Image Segmentation, Object Detection, etc\n",
    "- OpenCL, CUDA GPU programming\n",
    "- Publications in CVPR/NIPS/ICML/ICLR would be an added advantage\n",
    "- Computer vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=brown> Best Resources </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good resources list\n",
    "- [Over 200 best ML, NLP, Python Tutorials - 2018 Edition](https://medium.com/machine-learning-in-practice/over-200-of-the-best-machine-learning-nlp-and-python-tutorials-2018-edition-dd8cf53cb7dc)\n",
    "- [ML, DL Tutorials](https://github.com/ujjwalkarn/Machine-Learning-Tutorials)\n",
    "- [Machine Learning Introduction- Series](https://medium.com/machine-learning-for-humans/supervised-learning-740383a2feab)\n",
    "- [ML Cheet sheet](https://medium.com/machine-learning-in-practice/cheat-sheet-of-machine-learning-and-python-and-math-cheat-sheets-a4afe4e791b6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good practical resources\n",
    "- [Deep learning with python book](https://github.com/fchollet/deep-learning-with-python-notebooks)\n",
    "- [FastAI](https://github.com/fastai/courses)\n",
    "- [Deeplearning Specialization Coursera](https://github.com/Kulbear/deep-learning-coursera)\n",
    "- [Python machine learning book 2nd edition](https://github.com/rasbt/python-machine-learning-book-2nd-edition/tree/master/code/ch01)\n",
    "- [TesnorFlow examples](https://github.com/aymericdamien/TensorFlow-Examples)\n",
    "- [Data Science Jupyter notebook](https://github.com/donnemartin/data-science-ipython-notebooks#keras-tutorials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "143px",
    "width": "223px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "241px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
